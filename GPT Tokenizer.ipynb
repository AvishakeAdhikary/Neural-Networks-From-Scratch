{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b539e1-2b33-4562-b556-576309d3682c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Welcome to GPT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48581a13-1731-400c-a762-18b5502aa0d5",
   "metadata": {},
   "source": [
    "üòî\n",
    "\n",
    "We have a sad face because, `Tokenizers` are the least favourite part of **Large Language Models (LLMs)** that I need to work with. But unfortunately it is completely necessary to understand in detail to work with them... And a lot of oddness with LLMs traces back the these `Tokenization`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6137f68-6892-4958-bf41-7d6edae3cf1a",
   "metadata": {},
   "source": [
    "So...\n",
    "\n",
    "What is `Tokenization`?\n",
    "\n",
    "Now in our last notebook (<a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch</a>) we already implemented tokenization but it was done in a completely na√Øve and simple way..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f5fce-e244-4f45-bfa3-02954b4d16d1",
   "metadata": {},
   "source": [
    "In the previous notebook, the question we encountered was, \"how do we *plug-in* text into the GPT?\" and we ended up with a vocabulary of `92` characters, from which we created two look-up tables `stoi` and `itos` for mapping characters to indeces and vice-versa, which could be used as a `token` table for encode and decode functions, where `encode` function returned the encoded token integers and `decode` function returned the decoded message from the encoded tokens..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beddc98-35c3-4aa4-bdb9-27c153390bfe",
   "metadata": {},
   "source": [
    "And later we saw that the way we *plug* these `tokens` into the model is by using a `tokenEmbeddingTable` where this table represented a row of `92` possible characters with their respective `embeddings`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abecad87-85ca-4aa5-a823-201e58738d6b",
   "metadata": {},
   "source": [
    "But,\n",
    "\n",
    "In practice, people use a lot more complecated schemes to encode and decode these `tokens`...\n",
    "\n",
    "And we deal with **chunk level** texts. And these **chunk level** texts are constructed using algorithms like `Byte-Pair` algorithms which we will be covering in a bit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b396b4-0857-458c-b300-a21791d2a706",
   "metadata": {},
   "source": [
    "I'd also like discuss the paper that introduced this concept of `byte` level tokenization encoding as a mechanism in the context of LLMs...\n",
    "\n",
    "Which is this paper : <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">Language Models are Unsupervised Multitask Learners\r",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e307d-3140-4b2e-a8ec-e02d1921804e",
   "metadata": {},
   "source": [
    "And if we scroll to the point **2.2 Input Representation** within the paper we see that they conclude with the line: \"The vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens an a larger batchsize of 512 is used.\"\n",
    "\n",
    "Which means that the `vocabularySize` they used is about `50,257` and in the `Transformer` architecture's attention layer, every single token is *attending* to previous tokens in a sequence, and it is able to see upto `1024 tokens` in a sequence..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc000b40-06bd-4070-b41c-5441f46dca17",
   "metadata": {},
   "source": [
    "So **`Tokens` are the fundamental atomic units of a LLMs.** and everything related to it...\n",
    "\n",
    "And **`Tokenization` is the process of translating `strings` or text into sequences of `tokens` and vice versa.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b62eef-693a-4471-94c5-cff8d8dbb39e",
   "metadata": {},
   "source": [
    "And we can also look into the <a href=\"https://arxiv.org/pdf/2307.09288\">Llama 2: Open Foundation and Fine-Tuned Chat Models</a> paper by Meta and we see that, in their paper in section **2.1** they mentioned that they trained their model with 2 trillion tokens of data...\n",
    "\n",
    "And luckily the `Byte-Pair` algorithm is fairly simple and we can implement it ourselves and we can build our own `tokenizer`.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6d7f3-3628-45a2-b8ae-746922f2eddb",
   "metadata": {},
   "source": [
    "# Tiktokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184e298-c0e6-428c-ba63-154fc2c1a4e1",
   "metadata": {},
   "source": [
    "Before we dive into the code we can go to this nice website that has been created for us <a href=\"https://tiktokenizer.vercel.app/\">Tiktokenizer</a> and familize ourselves with the tokenization types that are used ...\n",
    "\n",
    "And what's great about this website is, tokenization is runs live on our browsers with the help of JavaScript... So we can plug in our own text and test the different tokenization techniques and their respective outputs and color coded tokens..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ad2da-baaf-4f0b-9c27-233383d1794f",
   "metadata": {},
   "source": [
    "Now what I'd like you to do is plug this sample text into the website input:\n",
    "\n",
    "```text\n",
    "Today we will be learning tokenization. It is not very fun, but definitely informative.\r\n",
    "\r\n",
    "69 + 420 = 489\r\n",
    "6969 + 420 = 7389\r\n",
    "\r\n",
    "Hat.\r\n",
    "hat.\r\n",
    "HAT.\r\n",
    "I have a hat.\r\n",
    "\r\n",
    "ÁßÅ„ÅØ„Ç≥„É≥„Éî„É•„Éº„Çø„Éº„ÅåÂ§ßÂ•Ω„Åç„Åß„Åô„ÄÇÁßÅ„ÅØÊ©üÊ¢∞Â≠¶Áøí„Ç®„É≥„Ç∏„Éã„Ç¢„Åß„Åô„ÄÇ \r\n",
    "\r\n",
    "for i in range(1, 101):\r\n",
    "    if i % 3 == 0 and i % 5 == 0:\r\n",
    "        print(\"FizzBuzz\")\r\n",
    "    elif i % 3 == 0:\r\n",
    "        print(\"Fizz\")\r\n",
    "    elif i % 5 == 0:\r\n",
    "        print(\"Buzz\")\r\n",
    "    else:\r\n",
    "        print(i)\n",
    "```\n",
    "\n",
    "And set the tokenizer to `GPT-2` because that's what we discussed earlier to relate what's happening currently..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a7eee-2cad-4675-aef2-238796d22ff5",
   "metadata": {},
   "source": [
    "And we immediately see that we get this kind of output:\n",
    "![Tokenizer_GPT2Test](ExplanationMedia/Images/Tokenizer_GPT2Test.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0606b07-ab3c-44cf-bdf3-7830e9090bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1918a-e531-48d9-891a-efebf23cc8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7db461-bc3d-4dce-ac89-4c3f0b4fa720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce400dec-18dd-48c5-8ada-9454765744d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405668dc-7312-481b-97b8-f090411967d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8caf93-ba7a-4e27-b3c1-2b11abd1070f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899cb14-f86e-4dde-99bc-078e07297e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01464f04-2100-4f52-a0ac-98e1d88fa6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
