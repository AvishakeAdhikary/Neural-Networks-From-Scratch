{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b539e1-2b33-4562-b556-576309d3682c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Welcome to GPT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48581a13-1731-400c-a762-18b5502aa0d5",
   "metadata": {},
   "source": [
    "üòî\n",
    "\n",
    "We have a sad face because, `Tokenizers` are the least favourite part of **Large Language Models (LLMs)** that I need to work with. But unfortunately it is completely necessary to understand in detail to work with them... And a lot of oddness with LLMs traces back the these `Tokenization`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6137f68-6892-4958-bf41-7d6edae3cf1a",
   "metadata": {},
   "source": [
    "So...\n",
    "\n",
    "What is `Tokenization`?\n",
    "\n",
    "Now in our last notebook (<a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch</a>) we already implemented tokenization but it was done in a completely na√Øve and simple way..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f5fce-e244-4f45-bfa3-02954b4d16d1",
   "metadata": {},
   "source": [
    "In the previous notebook, the question we encountered was, \"how do we *plug-in* text into the GPT?\" and we ended up with a vocabulary of `92` characters, from which we created two look-up tables `stoi` and `itos` for mapping characters to indeces and vice-versa, which could be used as a `token` table for encode and decode functions, where `encode` function returned the encoded token integers and `decode` function returned the decoded message from the encoded tokens..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beddc98-35c3-4aa4-bdb9-27c153390bfe",
   "metadata": {},
   "source": [
    "And later we saw that the way we *plug* these `tokens` into the model is by using a `tokenEmbeddingTable` where this table represented a row of `92` possible characters with their respective `embeddings`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abecad87-85ca-4aa5-a823-201e58738d6b",
   "metadata": {},
   "source": [
    "But,\n",
    "\n",
    "In practice, people use a lot more complecated schemes to encode and decode these `tokens`...\n",
    "\n",
    "And we deal with **chunk level** texts. And these **chunk level** texts are constructed using algorithms like `Byte-Pair` algorithms which we will be covering in a bit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b396b4-0857-458c-b300-a21791d2a706",
   "metadata": {},
   "source": [
    "I'd also like discuss the paper that introduced this concept of `byte` level tokenization encoding as a mechanism in the context of LLMs...\n",
    "\n",
    "Which is this paper : <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">Language Models are Unsupervised Multitask Learners\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e307d-3140-4b2e-a8ec-e02d1921804e",
   "metadata": {},
   "source": [
    "And if we scroll to the point **2.2 Input Representation** within the paper we see that they conclude with the line: \"The vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens an a larger batchsize of 512 is used.\"\n",
    "\n",
    "Which means that the `vocabularySize` they used is about `50,257` and in the `Transformer` architecture's attention layer, every single token is *attending* to previous tokens in a sequence, and it is able to see upto `1024 tokens` in a sequence..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc000b40-06bd-4070-b41c-5441f46dca17",
   "metadata": {},
   "source": [
    "So **`Tokens` are the fundamental atomic units of a LLMs.** and everything related to it...\n",
    "\n",
    "And **`Tokenization` is the process of translating `strings` or text into sequences of `tokens` and vice versa.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b62eef-693a-4471-94c5-cff8d8dbb39e",
   "metadata": {},
   "source": [
    "And we can also look into the <a href=\"https://arxiv.org/pdf/2307.09288\">Llama 2: Open Foundation and Fine-Tuned Chat Models</a> paper by Meta and we see that, in their paper in section **2.1** they mentioned that they trained their model with 2 trillion tokens of data...\n",
    "\n",
    "And luckily the `Byte-Pair` algorithm is fairly simple and we can implement it ourselves and we can build our own `tokenizer`.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6d7f3-3628-45a2-b8ae-746922f2eddb",
   "metadata": {},
   "source": [
    "# Tiktokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184e298-c0e6-428c-ba63-154fc2c1a4e1",
   "metadata": {},
   "source": [
    "Before we dive into the code we can go to this nice website that has been created for us <a href=\"https://tiktokenizer.vercel.app/\">Tiktokenizer</a> and familize ourselves with the tokenization types that are used ...\n",
    "\n",
    "And what's great about this website is, tokenization is runs live on our browsers with the help of JavaScript... So we can plug in our own text and test the different tokenization techniques and their respective outputs and color coded tokens..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ad2da-baaf-4f0b-9c27-233383d1794f",
   "metadata": {},
   "source": [
    "Now what I'd like you to do is plug this sample text into the website input:\n",
    "\n",
    "```text\n",
    "Today we will be learning tokenization. It is not very fun, but definitely informative.\n",
    "\n",
    "69 + 420 = 489\n",
    "6969 + 420 = 7389\n",
    "\n",
    "Hat.\n",
    "hat.\n",
    "HAT.\n",
    "I have a hat.\n",
    "\n",
    "ÁßÅ„ÅØ„Ç≥„É≥„Éî„É•„Éº„Çø„Éº„ÅåÂ§ßÂ•Ω„Åç„Åß„Åô„ÄÇÁßÅ„ÅØÊ©üÊ¢∞Â≠¶Áøí„Ç®„É≥„Ç∏„Éã„Ç¢„Åß„Åô„ÄÇ \n",
    "\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "```\n",
    "\n",
    "And set the tokenizer to `GPT-2` because that's what we discussed earlier to relate what's happening currently..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a7eee-2cad-4675-aef2-238796d22ff5",
   "metadata": {},
   "source": [
    "And we immediately see that we get this kind of output:\n",
    "![Tokenizer_GPT2Test](ExplanationMedia/Images/Tokenizer_GPT2Test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7b007",
   "metadata": {},
   "source": [
    "Now, I have divided our sample text that we want to tokenize into 5 parts:\n",
    "\n",
    "1. A sample line\n",
    "2. Arithmetic\n",
    "3. Case Sensitivity\n",
    "4. Foreign Language\n",
    "5. Python code\n",
    "\n",
    "And before starting, the discussion on the above points, I'd also like to mention that, we also need to understand that our texts contains white-spaces, and new line characters and so on, but we can hide them for more clarity..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5dbadd",
   "metadata": {},
   "source": [
    "1. Now let's start by discussing the sample line first: \\\n",
    "We see that the sample line has been *chunked* into nice little pieces and we have the `tokenized` text with us. But immediately we see that the text \"tokenization\" has been chunked into two little pieces, and we see that in the middle of a sentence 'spaces' are a part of them, we will see why that is in a bit...\n",
    "2. Next up we have the arithmetic code: \\\n",
    "We see that the token `420` is a single token but the token `489` is split up into two single tokens. And the LLM has to take a count of it and process it correctly in it's neural network as well...\n",
    "3. Next up we have our case sensitivity and punctuation: \\\n",
    "We see that we considered a token \"hat\" and how differently they appear with each case. And how having a leading space on top of the token makes it a completely different token that the ones without it. But the most interesting part here is that the LLM has to learn from the raw data that all of these \"hat\"s have the exact same concept and have to group them into the parameters of the neural network and understand that these are almost similar, but not exactly similar all by itself.\n",
    "4. Next up we will discuss the foreign languages: \\\n",
    "I have put this because, non english languages work, slightly worse in LLMs, and that is because the training dataset for these LLMs are generally very small compared to the English language, which is not just true for the LLM itself but for also for the `tokenizer`. So when we will train the tokenizer, we will see that there is a lot more English than Non-English text. And what ends up happening is, we get a lot more tokens for English tokens than Non-English tokens. In other words, if we try to see an English text and Japanese text as an example for comparison, we see that the number of tokens for Japanese used is much larger than compared to English and that is because the *chunks* are a lot more broken up and we end up using a lot more tokens for the exact same thing. And intuitively what this does is, it bloats up the sequence length of all the documents that we train on, and we very fast run out of context in the `Transformer`'s attention part.\n",
    "5. And lastly we will discuss the tokenization of the coding part, for us, I have taken the example of a Python code: \\\n",
    "We immediately see that all the individual spaces in the example are all separate tokens (specifically token `220`), which means that when the `Transformer` attends this text, it has to attend all the spaces individually, which is basically another way of saying that it is being extremely wasteful in the part of tokenization. (GPT-2 is extremely un-optimized for coding)\\\n",
    "Now we can try to change the tokenizer to `cl100k_base`(which is the GPT-4 tokenizer) for now and check the results, and we get something like this:\n",
    "![Tokenizer_CL100KExample](ExplanationMedia/Images/Tokenizer_CL100KExample.png)\n",
    "We immediately see that the token count has decresed and that is because the number of tokens in the GPT-4 tokenizer is roughly double than that of the GPT-2 tokenizer. Which means now we are now feeding a lot denser inputs to the transformer, which means that the `Transformer` is now able to see more in the previous context than before. But increasing this length infinitely is not good as well, because our Embedding table and the Softmax in the transformer ends up increasing in size and we end up doing a lot more computation than before. But there is a *sweet spot* that we can come to which makes us end up with nice vocabulary at the end. And I'd also like you to note that the **whitespace** handling of the GPT-4 has improved a lot, and they get grouped and we end up having a lot more efficent in Python tokenization, and this was a deliberate choice made by OpenAI and that is because this densifies the information in the tokenization for each of those tokens and the `Transformer` ends up looking further back into the context of previous code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc663d5",
   "metadata": {},
   "source": [
    "# Coding By Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a4ca2",
   "metadata": {},
   "source": [
    "Let's now start writing some code...\n",
    "\n",
    "Let's understand what we are trying to do:\n",
    "1. We want to take `strings` and we want to feed them into language models\n",
    "2. For that we need so somehow `tokenize` strings and into some `integers` mapped into a fixed vocabulary \n",
    "3. We will use those integers to make a *look-up* into a `lookupTable` of embedding vectors and feed those vectors into the transformer as an input\n",
    "\n",
    "But the reason this gets tricky is because:\n",
    "1. We want to support different kinds of languages\n",
    "2. We also want to support different kinds of special characters that we might find on the internet (for example emojis such as üëã)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5cda5",
   "metadata": {},
   "source": [
    "Let's take a *toy-example* first:\n",
    "```text\n",
    "Hello in japanese. üëã Êó•Êú¨Ë™û„Åß„Åì„Çì„Å´„Å°„ÅØ„ÄÇ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076cb500",
   "metadata": {},
   "source": [
    "So the question we now arrive is, how do we feed this text into a `Transformer`?\n",
    "\n",
    "Let's first dive into the definitions of `strings` in the <a href=\"https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str\">python documentation</a> we will find a text where it says \"Strings are immutable sequences of Unicode code points.\"\n",
    "\n",
    "So, what are Unicode code points?\n",
    "\n",
    "So now if we try to look up the <a href=\"https://en.wikipedia.org/wiki/Unicode\">Unicode page</a> from Wikipedia, we understand that Unicode is a text encoding standard maintained by the Unicode Consortium as a part of The Unicode Standard.\n",
    "What this essentially is, is that it is roughly a definition of 149,813 characters and 161 scripts (it is about what they look like and what integers represent those characters) **as of right now**.\n",
    "\n",
    "And I say **as of right now** because, we can see that the standard is very much alive and keeps on changing.\n",
    "\n",
    "And the way we can **access the Unicode code point of a character** is by using a function in python called `ord()`, which takes a single character as an input at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5da12",
   "metadata": {},
   "source": [
    "So we can now experiment with codes:\n",
    "\n",
    "For example, if we do:\n",
    "```python\n",
    "print(ord(\"H\"))\n",
    "print(ord(\"üëã\"))\n",
    "print(ord(\"Êó•\"))\n",
    "```\n",
    "For which we get:\n",
    "```python\n",
    "72\n",
    "128075\n",
    "26085\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380988b3",
   "metadata": {},
   "source": [
    "So using the same ideology we can look up all the characters in out *toy-example* string using a for loop to take out each character and passing the `ord()` function's output of those characters into a list, like this:\n",
    "```python\n",
    "print([ord(character) for character in \"Hello in japanese. üëã Êó•Êú¨Ë™û„Åß„Åì„Çì„Å´„Å°„ÅØ„ÄÇ\"])\n",
    "```\n",
    "For which we get:\n",
    "```python\n",
    "[72, 101, 108, 108, 111, 32, 105, 110, 32, 106, 97, 112, 97, 110, 101, 115, 101, 46, 32, 128075, 32, 26085, 26412, 35486, 12391, 12371, 12435, 12395, 12385, 12399, 12290]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad750fc4",
   "metadata": {},
   "source": [
    "Now see that we have already turned this raw text into integers, now we might arrive at the question that \"why can't we use these integers and not have any tokenization at all?\".\n",
    "\n",
    "One reason for this is that the vocabulary is quite long. But more dangerous reason is that because the Unicode standard is very much alive, it keeps changing, which means that it is not a stable representation of something that we might want to use directly into our models...\n",
    "\n",
    "So we need somethins a bit better at this point..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0c80c",
   "metadata": {},
   "source": [
    "Now, we tend towards the idea of `encodings`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98752c84",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
