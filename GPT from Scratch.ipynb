{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9b417b-c07f-4ff7-bf54-dd89f05626ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Welcome to GPT from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35e52d-ea1b-4e9a-89ee-55b892e9d40f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Things to discuss before starting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b496593-73b0-4847-9abc-6a5d19e67d9e",
   "metadata": {},
   "source": [
    "This notebook is a continuation of my previous notebooks in order:\n",
    "1. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/Neural%20Network%20with%20Derivatives.ipynb\">Neural Networks with Derivatives</a>\n",
    "2. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a>\n",
    "3. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Multi%20Layer%20Perceptron.ipynb\">NameWeave - Multi Layer Perceptron</a>\n",
    "4. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a>\n",
    "5. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Manual%20Back%20Propagation.ipynb\">NameWeave - Manual Back Propagation</a>\n",
    "6. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20WaveNet.ipynb\">NameWeave - WaveNet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b91d8-8d4b-498f-8932-8c8b8506087e",
   "metadata": {},
   "source": [
    "Which means, I will be using a lot of the terminologies, explanations, and code from the previous notebooks that I have created in the series...\n",
    "\n",
    "And we will gradually build a complete **GPT** from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff5339-c626-48e0-9135-cb81cfa60d57",
   "metadata": {},
   "source": [
    "This notebook will be a little bit different from the previous notebooks that I have created previously and I will discuss the changes in a bit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a5bb2-04a6-4442-ba16-9f36a2b2eafc",
   "metadata": {},
   "source": [
    "We will use a completely new dataset `Harry_Potter_Books.txt` which is a combined raw text of all the Harry Potter books by J. K. Rowling combined into a single `text` file, instead of `Indian_Names.txt` which we have used in the previous notebooks that contained Indian Firstnames crawled from a website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a01b1-4c47-4a51-8e74-33fb867f41f2",
   "metadata": {},
   "source": [
    "We will also keep softwares like <a href=\"https://chat.openai.com/\">ChatGPT</a>, <a href=\"https://gemini.google.com/app\">Google Gemini</a>, and other Large Language Models (LLM's) in mind and create our own little **Generative Pre-Trained Transformer (GPT)**...\n",
    "\n",
    "And you would probably know by now what these models are and what they do...\n",
    "\n",
    "Our **GPT** is going to be a *character level language model*, instead of a *sub-word-tokenized model* which softwares like ChatGPT and Google Gemini use in their models, and we will discuss everything in a bit...\n",
    "\n",
    "And we will not write all the `TORCH.NN` modules from scratch now, because we have already covered the most important ones already in our previous notebooks, instead we will implement the networks based on `TORCH.NN` library from PyTorch...\n",
    "\n",
    "And most importantly we will start from the simplest model (Bigram Model) and modify the same model within the same notebook and make our way upto the entire `Transformer` architecture...\n",
    "\n",
    "I have created an entire folder as `GPT Scripts` in the root of this repository to save each script for you to run them without even having to use jupyter notebooks. Rather you can simple use the `<filename>.py` to run each model to see how they perform on the same dataset as we move up, using:\n",
    "```bash\n",
    "python <filename>.py\n",
    "```\n",
    "\n",
    "One more thing to discuss is we remember from our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20WaveNet.ipynb\">NameWeave - WaveNet</a> notebook that we refered to multiple dimensions in a tensor as our own made up names, but the thing to know is that in real world these multiple dimensions have names, specifically **batch**, **time** and **channel** dimensions like this in order:\n",
    "\n",
    "$$\n",
    "(B, T, C)\\rightarrow(Batch, Time, Channel)\n",
    "$$\n",
    "\n",
    "And we will refer to these dimensions using the actual names used in real world this time...\n",
    "\n",
    "So, it's going to me a long journey and it if going to be legen...wait-for-it...dary. Legendary!!!\\\n",
    "![Barney Stinson Wink](https://media.tenor.com/nJ3EeUPhVKkAAAAM/barny-stinson.gif)\n",
    "\n",
    "So let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b5a45-f476-4f09-a2dd-ecef880a9205",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Understanding GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d72da0-a8c5-4180-bdf4-3bedef669acd",
   "metadata": {},
   "source": [
    "So what is a **GPT**?\n",
    "\n",
    "Well, **GPT** expands for the terminology as **Generative Pre-Trained Transformer**.\n",
    "\n",
    "You see how it consists of three words?\n",
    "\n",
    "Let's look at it in context of each word:\n",
    "1. Generative → Generates New Content\n",
    "2. Pre-Trained → Pre-trained on a dataset\n",
    "3. Transformer → Transformer architecture is being followed to make up the model (Don't worry we will discuss this later)\n",
    "\n",
    "That was easy..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1d95f-acec-487c-84f0-07262f7b7274",
   "metadata": {},
   "source": [
    "Let's understand what **GPT** can do currently...\n",
    "\n",
    "Let's take `ChatGPT` as an example as of now and let's see it's capabilities...\n",
    "\n",
    "![ChatGPT Current Capabilities](https://miro.medium.com/v2/resize:fit:679/1*_3AM0Yhc7qgCvZ_X1L8mhw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40702e51-6551-4716-8635-83d3a6b6ab12",
   "metadata": {},
   "source": [
    "We see that `GPT` goes from left to right and generates text sequentially..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86d794-f0e4-4a1a-831c-a2ff1e6657de",
   "metadata": {},
   "source": [
    "I wanted to show another thing:\n",
    "![ChatGPT Different Responses](ExplanationMedia/Images/ChatGPT_Different_Response.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d7c99-fc6d-4bbc-b687-b2737aa06840",
   "metadata": {},
   "source": [
    "See how we get a different response each time?\n",
    "\n",
    "Which hints us that it is more like a probabilistic system, which is for any one `prompt` it can give us multiple answers..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307442b-c05f-4618-8094-6201513200c6",
   "metadata": {},
   "source": [
    "Now, this is just one example of a `prompt`, and people have come up with billions of different prompts as of now, and in fact there are many websites that index the interactions with `ChatGPT` as well.\n",
    "\n",
    "You can look at this <a href=\"https://writesonic.com/blog/best-chatgpt-examples\">website</a> as an example.\n",
    "\n",
    "We see that it is a very remarkable system, and it is what we call a **\"Language Model\"**.\n",
    "\n",
    "Or in other words, it models the sequence of words or characters (or \"tokens\" more generally) and it knows how words follow each other in English language (even other languages)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2eefa5-76b9-452d-93c5-3817d91de573",
   "metadata": {},
   "source": [
    "Let's understand what **GPT** does from it's perspective...\n",
    "\n",
    "Well it is trying to complete the sequence...\n",
    "\n",
    "In other words, the `inputs` or `task` that we give to the GPT model, it treats it as a *start of a sequence* and it tries to complete the sequence as a whole. Which makes it a language model in this sense...\n",
    "\n",
    "You would think that it is utterly ridiculous and that we cannot just model an entire architecture and make it act like a helpful assistant.\n",
    "\n",
    "Well that is the beauty of it. And we will discuss all the under-the-hood components of what makes a software like `ChatGPT` work.\n",
    "\n",
    "So, What is the neural network architecture under-the-hood that models this sequence of words/characters/tokens?\n",
    "\n",
    "That comes from this paper from Google: <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> from 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bfc2f-df64-4d23-9859-3fb78cce4dcd",
   "metadata": {},
   "source": [
    "This was a landmark paper in Artificial Intelligence that proposed the `Transformer` architecture. But if you start reading this paper, it may seem like a pretty random *machine-translation* paper. And that's because, I think the authors did not fully anticipate the impact it would create in this domain in the years to come...\n",
    "\n",
    "Let's look at the original `Transformer` architecture as of now:\n",
    "![Transformer Archtecture](ExplanationMedia/Images/Transformer_Model_Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffa820-1b0c-4290-b557-1f1719754dfe",
   "metadata": {},
   "source": [
    "And this `Transformer` architecture was copy pasted in huge amount of applications in most recent years...\n",
    "\n",
    "And what we'd like to do now is create something like `ChatGPT`. But we would not be able to completely clone `ChatGPT` because it is a way more serious *production-grade* system which currently requires *thousands* of GPUs and *millions* of dollars to train the network, and also it is trained on a very good *chunk* of internet data. And there are a lot of **pre-training** and **fine-tuning** stages to it.\n",
    "\n",
    "Rather we would like to create a transformer-based language model, and in our case it is going to be a character level language model. And we also don't want to train on a *chunk* of internet, rather we need a smaller dataset (I proposed we work with `Harry_Potter_Books.txt` which is roughly a `7MB` file). And we would try to model how these characters in this dataset, follow each other.\n",
    "\n",
    "Let's take this paragraph for example:\n",
    "```python\n",
    "\"\"\"\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
    "were proud to say that they were perfectly normal, \n",
    "thank you very much.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Given a chunk of these characters in the past:\n",
    "```python\n",
    "\"\"\"\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
    "were proud to say that the\n",
    "\"\"\"\n",
    "```\n",
    "The `Transformer` model will look at these characters as a context in the past, and it is going to predict that the letter `'y'` is likely to come next in the sequence. And it is going to produce (generate) character sequences that look like Harry Potter. And in that process it is going to model all the patterns inside this data.\n",
    "\n",
    "And once we have trained the model, our model will be able to generate *infinite `Harry Potter`*\n",
    "\n",
    "![Harry Potter Woo](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExcjBmNzJ5N2EzMDQzeTB3cXV4ODN5ZGJkdWlldHhleGw3d3hpMGRhMyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/TJO5x5QQM72Q0weWXN/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843798f4-fca5-420d-8caf-4bfea1eb2245",
   "metadata": {},
   "source": [
    "So let's install the required dependencies and load our dataset up and look into the data and what it looks like first..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715def1-2630-4f41-a531-73ce25bbf959",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e06f1d-1362-491a-b516-46f9488e02a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fb2c8-d5ea-4a57-9e1b-a6789e187c66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2270b6-af58-43e5-bbd4-56f13d2f5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74bbb7-345b-4257-852a-0a25b32fda0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030a829-eb7a-4432-a021-738bbfb1f758",
   "metadata": {},
   "source": [
    "This time, I will divide the dataset loading part into two forms:\n",
    "1. If you're trying to use `Google Colab` to run the code\n",
    "2. If you're trying to use `Jupyter Notebook locally` to run the code\n",
    "And you can choose between either one of those with our desired mode..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750cf030-de56-4833-b069-70bb13d5d206",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## For Google Colab users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b9527-67ba-4a64-a77a-55bb42734c98",
   "metadata": {},
   "source": [
    "This will download the `Harry_Potter_Books.txt` into your current folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a81b0b-d3f5-4595-9b8d-d8da1ec4b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/AvishakeAdhikary/Neural-Networks-From-Scratch/main/Datasets/Harry_Potter_Books.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dde7bc-c5c2-4f8f-8f38-3dc419f2c75c",
   "metadata": {},
   "source": [
    "Now you can load up the dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53794f1-26c2-48a7-b282-bff93d6297f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Harry_Potter_Books.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1825e0-4180-4279-a8c0-06b8329efa32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## For local Jupyter Notebook users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623e165-f912-4676-9107-41ea3479c2b0",
   "metadata": {},
   "source": [
    "You don't have to download the dataset if you have the entire repository cloned.\n",
    "\n",
    "The dataset `Harry_Potter_Books.txt` is already located inside the `Datasets` directory...\n",
    "\n",
    "So we can simply open the file and look at its content by specifying the relative path..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70974e14-fe05-4cfb-b794-e6773301ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/Harry_Potter_Books.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431416c3-581d-45e6-a660-228adea1ae5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c635c2-e349-458a-b406-b42edb18c275",
   "metadata": {},
   "source": [
    "We can look at the length of the entire dataset and it's number of characters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20aa4c0-dd36-4178-ae4d-6c287dce74d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset in Characters:  6765190\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Dataset in Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004d6ac-34e8-4d8a-8258-35f93fe646a3",
   "metadata": {},
   "source": [
    "We see that it is roughly `6-million` characters...\n",
    "\n",
    "And if you want to look at the first `1000` characters we can do:\n",
    "```python\n",
    "print(text[:1000])\n",
    "```\n",
    "Which prints the output:\n",
    "```python\n",
    "\"\"\"\n",
    "/ \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "THE BOY WHO LIVED \r\n",
    "\r\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \r\n",
    "were proud to say that they were perfectly normal, \r\n",
    "thank you very much. They were the last people you’d \r\n",
    "expect to be involved in anything strange or \r\n",
    "mysterious, because they just didn’t hold with such \r\n",
    "nonsense. \r\n",
    "\r\n",
    "Mr. Dursley was the director of a firm called \r\n",
    "Grunnings, which made drills. He was a big, beefy \r\n",
    "man with hardly any neck, although he did have a \r\n",
    "very large mustache. Mrs. Dursley was thin and \r\n",
    "blonde and had nearly twice the usual amount of \r\n",
    "neck, which came in very useful as she spent so \r\n",
    "much of her time craning over garden fences, spying \r\n",
    "on the neighbors. The Dursley s had a small son \r\n",
    "called Dudley and in their opinion there was no finer \r\n",
    "boy anywhere. \r\n",
    "\r\n",
    "The Dursleys had everything they wanted, but they \r\n",
    "also had a secret, and their greatest fear was that \r\n",
    "somebody would discover it. They didn’t think they \r\n",
    "could bear it if anyone found out about the Potters. \r\n",
    "Mrs. Potter was Mrs. Dursl\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8478f344-d22f-4895-93d1-08b79c709e2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee4734-5dbc-4f5a-95d1-e25e7f74e268",
   "metadata": {},
   "source": [
    "We can now start building our vocabulary, just like we did in our previous notebooks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff89869c-5e80-489e-a3c5-7c6a7ef58ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '—', '‘', '’', '“', '”', '•', '■', '□']\n",
      "Vocabulary Size: 92\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text))) # Gives us all the characters in the english alphabet, hopefully our dataset has all of them\n",
    "vocabularySize = len(characters) # We define a common vocabulary size\n",
    "print(\"Characters:\", characters)\n",
    "print(\"Vocabulary Size:\", vocabularySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4882e5-a2d1-4d15-9939-372895dc2b39",
   "metadata": {},
   "source": [
    "So we have a possible `vocabulary` of `92` characters that our model will be able to see or emit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efcb11-c293-4b73-8b73-09b979a3c0cf",
   "metadata": {},
   "source": [
    "Now we would like to develop a strategy to <strong><i>tokenize</i></strong> our input `text`.\n",
    "\n",
    "And when we say **tokenize** we generally mean to convert raw text as a string to some sequence of integers according to some vocabulary of possible elements...\n",
    "\n",
    "For us, because we are developing a character level language model, so we are simply going to be translating individual `characters` into `integers`.\n",
    "\n",
    "And we will build `4` things here:\n",
    "1. String to Index Vocabulary → `stoi` → A map of `characters` to `integers`\n",
    "2. Index to String Vocabulary → `itos` → A map of `integers` to `characters`\n",
    "3. Token Encoder → That will encode sequence of characters into indeces\n",
    "4. Token Decoder → That will encode sequence of encoded indeces into characters\n",
    "\n",
    "And you will be able to recognize the first two from our previous notebooks...\n",
    "\n",
    "Before we dive in, let's understand the python concept of `lambda` functions, which some you might have forgotten...\n",
    "\n",
    "So what are `lambda` functions?\n",
    "\n",
    "Python Lambda Functions are *anonymous functions* means that the function is without a name. As we already know the `def` keyword is used to define a normal function in Python. Similarly, the `lambda` keyword is used to define an anonymous function in Python.\n",
    "\n",
    "Syntax: `lambda arguments : expression`\n",
    "\n",
    "For example:\n",
    "```python\n",
    "output = lambda input: input+1\n",
    "print(output(input=1))\n",
    "```\n",
    "We would print `2`.\n",
    "\n",
    "Now, let me first run it, then I will explain it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b1dcd86-01ee-4ff2-9ecd-7366263bced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOI: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '>': 26, '?': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, 'Z': 53, '\\\\': 54, ']': 55, 'a': 56, 'b': 57, 'c': 58, 'd': 59, 'e': 60, 'f': 61, 'g': 62, 'h': 63, 'i': 64, 'j': 65, 'k': 66, 'l': 67, 'm': 68, 'n': 69, 'o': 70, 'p': 71, 'q': 72, 'r': 73, 's': 74, 't': 75, 'u': 76, 'v': 77, 'w': 78, 'x': 79, 'y': 80, 'z': 81, '|': 82, '~': 83, '—': 84, '‘': 85, '’': 86, '“': 87, '”': 88, '•': 89, '■': 90, '□': 91}\n",
      "ITOS: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '%', 5: '&', 6: \"'\", 7: '(', 8: ')', 9: '*', 10: ',', 11: '-', 12: '.', 13: '/', 14: '0', 15: '1', 16: '2', 17: '3', 18: '4', 19: '5', 20: '6', 21: '7', 22: '8', 23: '9', 24: ':', 25: ';', 26: '>', 27: '?', 28: 'A', 29: 'B', 30: 'C', 31: 'D', 32: 'E', 33: 'F', 34: 'G', 35: 'H', 36: 'I', 37: 'J', 38: 'K', 39: 'L', 40: 'M', 41: 'N', 42: 'O', 43: 'P', 44: 'Q', 45: 'R', 46: 'S', 47: 'T', 48: 'U', 49: 'V', 50: 'W', 51: 'X', 52: 'Y', 53: 'Z', 54: '\\\\', 55: ']', 56: 'a', 57: 'b', 58: 'c', 59: 'd', 60: 'e', 61: 'f', 62: 'g', 63: 'h', 64: 'i', 65: 'j', 66: 'k', 67: 'l', 68: 'm', 69: 'n', 70: 'o', 71: 'p', 72: 'q', 73: 'r', 74: 's', 75: 't', 76: 'u', 77: 'v', 78: 'w', 79: 'x', 80: 'y', 81: 'z', 82: '|', 83: '~', 84: '—', 85: '‘', 86: '’', 87: '“', 88: '”', 89: '•', 90: '■', 91: '□'}\n",
      "Encoded Text:  [39, 60, 62, 60, 69, 59, 56, 73, 80]\n",
      "Decoded Text:  Legendary\n"
     ]
    }
   ],
   "source": [
    "stoi = {character:index for index, character in enumerate(characters)}\n",
    "itos = {index:character for index, character in enumerate(characters)}\n",
    "encode = lambda string: [stoi[character] for character in string] # Token Encoder that takes in a string as an input, and outputs a list of integers\n",
    "decode = lambda list: ''.join([itos[index] for index in list]) # Token Decoder that takes in the encoded list of integers and outputs the decoded string\n",
    "\n",
    "print(\"STOI:\", stoi)\n",
    "print(\"ITOS:\", itos)\n",
    "print(\"Encoded Text: \", encode(\"Legendary\"))\n",
    "print(\"Decoded Text: \", decode(encode(\"Legendary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8b2f0-a80c-49e9-97a9-5965c01139b6",
   "metadata": {},
   "source": [
    "The `Token Encoder` here takes in a `string` or a `sequence of characters` and encodes it into a `list of integers` based on `stoi` mapping. And the `Token Decoder` takes in the encoded `list of integers` and decodes it based on `itos` mapping to get back the exact same string...\n",
    "\n",
    "In other words, it is more like a translation of `characters` into `integers` and `integers` into `characters`, because our model is going to be a character level language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3be15a-b6d6-4607-9bfa-eab54fa8dee6",
   "metadata": {},
   "source": [
    "Now this is only one of many possible `encodings` or `tokenizers` that are out there in the world right now...\n",
    "\n",
    "And people have come up with many such `tokenizers`, for example, Google uses <a href=\"https://github.com/google/sentencepiece\">`sentencepiece`</a>, OpenAI uses <a href=\"https://github.com/openai/tiktoken\">`tiktoken`</a>...\n",
    "\n",
    "And these `tokenizers` which are out there are more like `sub-word` tokenizers, which are **not** encoding `entire words` and also **not** encoding `individual characters`, and more like a `sub-word` unit level `tokenizers` which is usually what's adapted in practice...\n",
    "\n",
    "As an example let's take `tiktoken` vocabulary which uses `Byte-Pair Encoding (BPE)` to encode these `tokens`:\\\n",
    "![Tiktoken Vocabulary](ExplanationMedia/Images/Tiktoken_Vocabulary.png)\n",
    "\n",
    "We see that `tiktoken` has a vocabulary of roughly `50257` which for us is just `92`.\n",
    "\n",
    "And when we try to encode a sample string in `tiktoken`, we get:\\\n",
    "![Tiktoken Example](ExplanationMedia/Images/TikToken_Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b5404-df32-49ae-a752-ff97a9aeb981",
   "metadata": {},
   "source": [
    "We see that we only get `3` outputs for and entire string of `9` characters...\n",
    "\n",
    "Which means that we can *trade-off* `sequences of integers` and `vocabularies`...\n",
    "\n",
    "In other words, we can have a very long `sequences of integers` and very short `vocabularies` or we can have very short `sequences of integers` and very long `vocabularies`...\n",
    "\n",
    "But for now I'd like to keep our `tokenizer` extremely simple using our own character-level tokenizer (meaning we have very small `vocabulary`) and very simple `encode` and `decode` functions, but we do get very long `sequences of integers` as a result...\n",
    "\n",
    "Don't worry, if you'd like I will build a `tokenizer` in the future...\n",
    "\n",
    "So let's now move forward..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27569cc5-193d-4d54-b2e4-798f630390db",
   "metadata": {},
   "source": [
    "Now that we have a `token encoder` and a `token decoder` or effectively a `tokenizer` we can move forward and encode our entire `Harry Potter` dataset...\n",
    "\n",
    "And we will use <a href=\"https://pytorch.org/\">PyTorch</a> library for that:\n",
    "![PyTorch Logo](ExplanationMedia/Images/PyTorchLogo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0b5d2-4800-437b-a6c7-754ce0492e28",
   "metadata": {},
   "source": [
    "So we can now wrap our `text` data after `encoding` it into a `tensor` of datatype `long` because we want *floating-point numbers* to do mathematical transformations on this data later like this:\n",
    "```python\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "783af38b-46c0-4477-ab9f-927fe13527ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff76a2-4be5-4eda-ae6b-5688a1104c94",
   "metadata": {},
   "source": [
    "And then we can check the `shape` and `type` of this data and print out the first `100` characters, just like we did before (without decoding it) like this:\n",
    "```python\n",
    "print(data.shape, data.dtype)\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "torch.Size([6765190]) torch.int64\n",
    "```\n",
    "And:\n",
    "```python\n",
    "print(data[:100])\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "tensor([13,  1,  0,  0,  0,  0,  0, 47, 35, 32,  1, 29, 42, 52,  1, 50, 35, 42,\r\n",
    "         1, 39, 36, 49, 32, 31,  1,  0,  0, 40, 73, 12,  1, 56, 69, 59,  1, 40,\r\n",
    "        73, 74, 12,  1, 31, 76, 73, 74, 67, 60, 80, 10,  1, 70, 61,  1, 69, 76,\r\n",
    "        68, 57, 60, 73,  1, 61, 70, 76, 73, 10,  1, 43, 73, 64, 77, 60, 75,  1,\r\n",
    "        31, 73, 64, 77, 60, 10,  1,  0, 78, 60, 73, 60,  1, 71, 73, 70, 76, 59,\r\n",
    "         1, 75, 70,  1, 74, 56, 80,  1, 75, 63])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bda6f2-cf98-47ee-a401-875c08ad876a",
   "metadata": {},
   "source": [
    "And we see that we have a massive list of integers and is an identical translation of the first `100` characters exactly in the `text` file...\n",
    "\n",
    "And the entire dataset is just stretched out into a very large `sequence of integers`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fbe11-71fd-4baf-8fcf-a06d55d37a0d",
   "metadata": {},
   "source": [
    "Now before we move on with our progress, we would like to do one more thing that is we'd like to split our dataset into a `Train` and `Validation` split...\n",
    "\n",
    "So let's do that..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83671c-4c1f-426e-8a38-6bac731d79c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Splitting the Dataset into `Training` and `Validation` splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59615b21-02d3-4231-9872-8ecf8c038948",
   "metadata": {},
   "source": [
    "Now I'd like to split our data into a split of:\n",
    "1. First 90% into `Training` Split\n",
    "2. Last 10% into `Validation` Split\n",
    "\n",
    "And we are doing this to understand, to what extent our model is `overfitting`...\n",
    "\n",
    "Because we don't want our model to copy and create the exact book of `Harry Potter` instead, we want a model that will create `Harry Potter` like text...\n",
    "\n",
    "And here's how I do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40503ff9-d386-4899-ad3b-3de99801faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintyPercentOfDatasetLength = int((0.9 * len(data)))\n",
    "trainingData = data[:nintyPercentOfDatasetLength] # Data up till 90% of the length\n",
    "validationData = data[nintyPercentOfDatasetLength:] # Data from 90% of the length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5531f0-f95f-4e0e-86dc-447955cfd57f",
   "metadata": {},
   "source": [
    "We can now move on to the next part..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e41fc-c973-498c-a30d-cfb9f8c047e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Data Into Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6399b-5ed0-4b4e-b27e-69c535c78b06",
   "metadata": {},
   "source": [
    "We would like to now proceed to feed these integer sequences into the neural network so that it can train and learn those patterns...\n",
    "\n",
    "**BUT**\n",
    "\n",
    "We need to realise that we are not going to feed in the entire dataset into the neural network because that is going to be extremely computationally heavy, and rather we would load the data into small batches or *chunks* of data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b9b6c-fba0-49f5-9317-5ea8afe9e6c3",
   "metadata": {},
   "source": [
    "Now I typically use the term `block size` and specify a length to it, but this *chunk* of data can be recognized as different terminologies as well, for example `context length`.\n",
    "\n",
    "Let's start with a `blockSize` of just `8`...\n",
    "```python\n",
    "blockSize = 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f5140-2513-4d95-b747-399e00482dfd",
   "metadata": {},
   "source": [
    "Now let's look at the first *chunk* of training data (`blockSize` of `8` + `1`)...\n",
    "\n",
    "I'll explain why this `+1` is there in a bit...\n",
    "\n",
    "So we have:\n",
    "```python\n",
    "trainingData[:blockSize + 1]\n",
    "```\n",
    "For which we get the sequence:\n",
    "```python\n",
    "tensor([13,  1,  0,  0,  0,  0,  0, 47, 35])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e865ff-8b5d-4f85-8fc0-49fce59df9c1",
   "metadata": {},
   "source": [
    "Which is the first `9` characters in a `sequence` in the `training-set`...\n",
    "\n",
    "Now, I'd like to point out that when we take out a *chunk* of data like this, we actually have **multiple examples** packed within it, because all of these characters **follow** each other...\n",
    "\n",
    "And we are going to simultaneously train it at every one of these positions...\n",
    "\n",
    "And in a chunk of `9` characters, there's actually `8` individual examples packed in there...\n",
    "\n",
    "How so?\n",
    "\n",
    "Let's look at it this way...\n",
    "\n",
    "For our example:\n",
    "```python\n",
    "[13,  1,  0,  0,  0,  0,  0, 47, 35]\n",
    "```\n",
    "1. In the context of `[13]` → `1` is likely to come next,\n",
    "2. In the context of `[13,  1]` → `0` is likely to come next,\n",
    "3. In the context of `[13,  1,  0]` → `0` is likely to come next,\n",
    "4. In the context of `[13,  1,  0,  0]` → `0` is likely to come next,\n",
    "5. In the context of `[13,  1,  0,  0,  0]` → `0` is likely to come next,\n",
    "6. In the context of `[13,  1,  0,  0,  0,  0]` → `0` is likely to come next,\n",
    "7. In the context of `[13,  1,  0,  0,  0,  0,  0]` → `47` is likely to come next,\n",
    "8. In the context of `[13,  1,  0,  0,  0,  0,  0, 47]` → `35` is likely to come next.\n",
    "\n",
    "Summing upto `8` individual examples in our case, which is the `blockSize` and we take the `+1` to get the desired `label` for training..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844f6f1-c31e-4b74-bc7d-430536c6de0b",
   "metadata": {},
   "source": [
    "Let's see how we can achieve the same output in a code snippet:\n",
    "```python\n",
    "inputs = trainingData[:blockSize] # First Chuck of Characters\n",
    "outputs = trainingData[1:blockSize + 1] # First Chunk of Characters offset by 1\n",
    "for i in range(blockSize):\n",
    "    context = inputs[:i+1] # Context is inputs upto the offset\n",
    "    label = outputs[i] # Label is the offset\n",
    "    print(f\"When input example is {context}, then the label is: {label}\")\n",
    "```\n",
    "\n",
    "For which we get:\n",
    "```python\r\n",
    "When input example is tensor([13]), then the label is: 1\r\n",
    "When input example is tensor([13,  1]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0,  0,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0,  0,  0,  0]), then the label is: 47\r\n",
    "When input example is tensor([13,  1,  0,  0,  0,  0,  0, 47]), then the label is: 35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7ff83-47a3-4c18-8904-3cbab831730e",
   "metadata": {},
   "source": [
    "One more thing to mention is that we not only train these examples all the way to the context of `blockSize` just for efficiency.\n",
    "\n",
    "**We also want our network to get *\"used to\"* seeing these examples for context of as small as `1` all the way upto the `blockSize` and everything in between.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65986360-6e76-4764-9fc1-2828740745ec",
   "metadata": {},
   "source": [
    "So, during `inference` we can start sampling from as little as `1` character of context. And once it starts sampling it can go all the way upto `blockSize` and after the context of `blockSize` we can start `truncating`, because the neural network will receive more than `blockSize` inputs when its trying to predict the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56596f7-828c-4e0c-8967-ed9b7c059206",
   "metadata": {},
   "source": [
    "And these input examples that we just looked are nothing but the `Time` dimension...\n",
    "\n",
    "But we need to care about the `Batch` dimension now... And that is because, everytime we feed these *chunks* of texts into a `Transformer`, we are going to have **mini-batches** of multiple chunks of texts that are all **stacked-up** in a single tensor (this is done for efficiency, such that we could keep the `GPU`'s busy, because they are very good at parallel processing of data, and we want to process multiple *chunks* of text all at the same time, but they are processed completely independently and they don't \"talk\" to each other)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f12c1-a099-407d-9329-35ba0c3b766b",
   "metadata": {},
   "source": [
    "We will also set up a `seed` so that whatever numbers I see here in my system, you are going to see the same numbers in your system later as well...\n",
    "\n",
    "Let's now generalize our discussion into code and I will discuss what is happening in the code one by one..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5871cb-da5c-4a93-b818-7cea3baa75ae",
   "metadata": {},
   "source": [
    "More specifically let's define a `getBatch()` method try to pick out batches of `batchSize`...\n",
    "\n",
    "Now don't get confused between `blockSize` and `batchSize`...\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "\\begin{align}\n",
    "blockSize \\rightarrow \\text{The number of independent sequences of characters we want to process in parallel} \\\\\n",
    "batchSize \\rightarrow \\text{The maximum context length of predictions}\n",
    "\\end{align}\n",
    "}\n",
    "$$\n",
    "\n",
    "We will now use our older *example* code get batches of examples, but now in context of `batchSize` now, and we will pick random indeces from the entire dataset, which will then be used to process all the possible examples in sequence and their corresponding labels in a batch using `torch.stack` which essentially concatenates a sequence of tensors along a new dimension. Which makes the `inputBatch` and `outputBatch` a `(4, 8)` tensor, where each row in an `inputBatch` is a *chunk* of the training set, and `outputBatch` will be used all the way at the end during **loss-function**...\n",
    "\n",
    "So, we can spell each of these `examples in a sequence` can be spelled out just like we did before to get their corresponding `labels` for each of these examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3535142b-a9e1-4d18-98c8-16ae1e486be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: torch.Size([4, 8])  Values: tensor([[70, 70, 67,  1, 70, 61,  1, 75],\n",
      "        [59,  1,  0, 56, 75,  1, 75, 63],\n",
      "        [59,  1,  0, 71, 67, 60, 69, 75],\n",
      "        [75, 63, 60,  1, 68, 56, 81, 60]])\n",
      "Outputs: torch.Size([4, 8])  Values: tensor([[70, 67,  1, 70, 61,  1, 75, 63],\n",
      "        [ 1,  0, 56, 75,  1, 75, 63, 60],\n",
      "        [ 1,  0, 71, 67, 60, 69, 75, 80],\n",
      "        [63, 60,  1, 68, 56, 81, 60, 10]])\n",
      "---------------------------------------------\n",
      "When input example is [70], then the label is: 70\n",
      "When input example is [70, 70], then the label is: 67\n",
      "When input example is [70, 70, 67], then the label is: 1\n",
      "When input example is [70, 70, 67, 1], then the label is: 70\n",
      "When input example is [70, 70, 67, 1, 70], then the label is: 61\n",
      "When input example is [70, 70, 67, 1, 70, 61], then the label is: 1\n",
      "When input example is [70, 70, 67, 1, 70, 61, 1], then the label is: 75\n",
      "When input example is [70, 70, 67, 1, 70, 61, 1, 75], then the label is: 63\n",
      "When input example is [59], then the label is: 1\n",
      "When input example is [59, 1], then the label is: 0\n",
      "When input example is [59, 1, 0], then the label is: 56\n",
      "When input example is [59, 1, 0, 56], then the label is: 75\n",
      "When input example is [59, 1, 0, 56, 75], then the label is: 1\n",
      "When input example is [59, 1, 0, 56, 75, 1], then the label is: 75\n",
      "When input example is [59, 1, 0, 56, 75, 1, 75], then the label is: 63\n",
      "When input example is [59, 1, 0, 56, 75, 1, 75, 63], then the label is: 60\n",
      "When input example is [59], then the label is: 1\n",
      "When input example is [59, 1], then the label is: 0\n",
      "When input example is [59, 1, 0], then the label is: 71\n",
      "When input example is [59, 1, 0, 71], then the label is: 67\n",
      "When input example is [59, 1, 0, 71, 67], then the label is: 60\n",
      "When input example is [59, 1, 0, 71, 67, 60], then the label is: 69\n",
      "When input example is [59, 1, 0, 71, 67, 60, 69], then the label is: 75\n",
      "When input example is [59, 1, 0, 71, 67, 60, 69, 75], then the label is: 80\n",
      "When input example is [75], then the label is: 63\n",
      "When input example is [75, 63], then the label is: 60\n",
      "When input example is [75, 63, 60], then the label is: 1\n",
      "When input example is [75, 63, 60, 1], then the label is: 68\n",
      "When input example is [75, 63, 60, 1, 68], then the label is: 56\n",
      "When input example is [75, 63, 60, 1, 68, 56], then the label is: 81\n",
      "When input example is [75, 63, 60, 1, 68, 56, 81], then the label is: 60\n",
      "When input example is [75, 63, 60, 1, 68, 56, 81, 60], then the label is: 10\n"
     ]
    }
   ],
   "source": [
    "# We define a manual seed such that you see the same numbers I see in my machine\n",
    "torch.manual_seed(69420)\n",
    "batchSize = 4 # Number of independent sequences of characters we want to process in parallel\n",
    "blockSize = 8 # Maximum context length of predictions\n",
    "\n",
    "def getBatch(split):\n",
    "    # Take the trainingData if the split is 'train' otherwise take the validationData\n",
    "    data = trainingData if split=='train' else validationData\n",
    "    # Generates random integers of batchSize between 0 and len(data) - blockSize\n",
    "    indexes = torch.randint(high=len(data) - blockSize, size=(batchSize,))\n",
    "    # Takes the inputs and outputs after stacking them up in a single tensor\n",
    "    inputs = torch.stack([data[i:i+blockSize] for i in indexes])\n",
    "    outputs = torch.stack([data[i+1:i+blockSize+1] for i in indexes])\n",
    "    return inputs, outputs\n",
    "\n",
    "# We call the method to initialize inputBatch and outputBatch\n",
    "inputBatch, outputBatch = getBatch('train')\n",
    "\n",
    "print(\"Inputs:\", inputBatch.shape, \" Values:\" , inputBatch)\n",
    "print(\"Outputs:\", outputBatch.shape, \" Values:\" , outputBatch)\n",
    "\n",
    "print('---------------------------------------------')\n",
    "\n",
    "for batchIndex in range(batchSize):\n",
    "    for blockIndex in range(blockSize):\n",
    "        context = inputBatch[batchIndex, : blockIndex+1]\n",
    "        label = outputBatch[batchIndex, blockIndex]\n",
    "        print(f\"When input example is {context.tolist()}, then the label is: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93ef1e-66a2-4e1c-a482-05699ec6e9fe",
   "metadata": {},
   "source": [
    "Now that we have our `inputBatch` and `outputBatch` we can start feeding these batches into a neural network and start getting predictions....\n",
    "\n",
    "Now we are going to start off with the simplest possible neural network, which in my opinion is a `Bigram Language Model` which was already covered in our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a> notebook in a lot of depth, and we will rather go faster and implement a `PyTorch Module` directly that implements the `Bigram Language Model`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc651c6-296a-45c2-8b59-4de0387293f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb728173-527c-42c4-879c-035a9adf9861",
   "metadata": {},
   "source": [
    "Before we implement the bigram model, I'd like to discuss this syntax in python:\n",
    "```python\n",
    "class A:\n",
    "    x = 10\n",
    "\n",
    "class B(A):\n",
    "    def show(self):\n",
    "        print(self.x)\n",
    "\n",
    "y = B()\n",
    "y.show()\n",
    "```\n",
    "For which we get the output:\n",
    "```python\n",
    "10\n",
    "```\n",
    "Which essentially means that this syntax (`B(A)`) is used for `inheritence`, which is much more helpful now to implement `Torch Modules` in our own implementations. And that is because `torch.nn.Module` contains many such methods already implemented like `forward(*input)` which let's us define a forward pass and internally manages the `__call__()` method and we can return our calculated `logits` within the `Module` that we are going to define and later call `backward()` on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4fac9-4f30-453e-9970-e6147886dec9",
   "metadata": {},
   "source": [
    "So, we understand that we need an `Embedding Look-Up Table` for each of our characters in the vocabulary. From our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Multi%20Layer%20Perceptron.ipynb\">NameWeave - Multi Layer Perceptron</a> notebook we understand that when we index into such an embedding table of two dimensions using **batches of input** of two dimensions, we get an indexed output of a three dimensional tensor of `(B, T, C)` which we can now refer as the `Batch`, `Time` and `Channel` dimensions of that tensor, which essentially is nothing but for each input of a batch, it picks out a row of the embedding table. In our case, `Batch` is `4` which is the `batchSize`, `Time` is `8` which is the `blockSize` and `Channel` is the embedding dimension we will specify...\n",
    "\n",
    "These indexed embeddings, for now can be interpreted as the `logits` or the scores for the next character in a sequence. Or in other words, we are predicting what comes next based on just the individual identity of a single token (which means they are not *talking* to each other). For example, if a token is say `69`, the token itself will be able to make pretty decent predictions of what comes next by knowing that the token is in fact `69`...\n",
    "\n",
    "So now, let's write our first `model` out, and test it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18c46b-935b-4256-9f4e-e22ec2fa7983",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Writing out BigramModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4555314d-c991-4230-b50a-1e5f5a601ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 92])\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    # Constructor for the model\n",
    "    def __init__(self, vocabularySize):\n",
    "        # Initializing the embedding table\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, vocabularySize)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, indeces):\n",
    "        # Index into embeddings to get the logits\n",
    "        logits = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "        return logits\n",
    "\n",
    "# Testing the model\n",
    "model = BigramLanguageModel(vocabularySize)\n",
    "outputs = model(inputBatch)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1334a1-719a-4365-9a06-21b3d7f1369b",
   "metadata": {},
   "source": [
    "Looks like we do get the scores for every one of those `(4, 8)` positions..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda938d9-60bc-4b51-949f-e58210b358f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding `cross_entropy` loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7e90c-a42e-430d-9000-1738310a4331",
   "metadata": {},
   "source": [
    "Now that we have the predictions of *'what comes next'* we'd like to evaluate the `loss function`. And in the `NameWeave` series we saw that a good way to measure the `loss` or the quality of the predictions is to use the **Negetive Log Likelihood** loss which is also implemented in PyTorch under the name of `cross_entropy`. \n",
    "\n",
    "And remember how I said the `outputBatch` is required when we calculate the `loss function`? \n",
    "\n",
    "This is exactly when we would require the `outputBatch` to calculate the difference on the predictions or the `logits` and their corresponding `labels`. Or in other words, we have the identity of the next character, but how well are we predicting the next character based on the `logits`...\n",
    "\n",
    "And we'd like to call the `cross_entropy` in it's **functional** form, which means we don't have to create a `Module` for it. But when we look at the documentation of <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\">CrossEntropyLoss</a> of PyTorch, we understand that we have **multi-dimensional inputs** (because we have a `(B, T, C) tensor`), but PyTorch `CrossEntropyLoss` expects $(minibatch,C,d_1,d_2,\\ldots,d_K)$ or a `(B, C, T) tensor`.\n",
    "\n",
    "So what to do now with our `(B, T, C) tensor` that we already have?\n",
    "\n",
    "Well we will try to **reshape** our tensor now, in order to fit those `logits` as well as the `labels` which is a `(B, T) tensor`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf711fb-8cdc-454d-94eb-d11876eec7ad",
   "metadata": {},
   "source": [
    "Now because we are only interested in manipulating the `embeddings` of the `logits` (which for our case is the `channel` dimension), we can start treating everything else as a **batch-dimension**.\n",
    "\n",
    "This is good because PyTorch `CrossEntropyLoss` expects a $(minibatch,C)$ tensor...\n",
    "\n",
    "So we can stretch our 3-dimensional tensor into 2-dimensional tensor by combining all the other batch dimensions into a single dimension by multiplying the dimension values into one using `view()` method of PyTorch...\n",
    "\n",
    "And it looks something like this:\n",
    "![CrossEntropyGPTExplanation](ExplanationMedia/Images/CrossEntropyGPTExplanation.png)\n",
    "\n",
    "And then we can print out the `loss` to check where we are as well...\n",
    "\n",
    "So let's do this now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2df3c-f7cc-4ff6-a9ce-eedd3e794a6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Writing out Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c020eea-698f-4c20-adea-517ea24cd2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of outputs: torch.Size([32, 92])\n",
      "Loss: tensor(5.0409, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    # Constructor for the model\n",
    "    def __init__(self, vocabularySize):\n",
    "        # Initializing the embedding table\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, vocabularySize)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, indeces, labels):\n",
    "        # Index into embeddings to get the logits\n",
    "        logits = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "        # Pop out the shape dimensions\n",
    "        batch, time, channel = logits.shape\n",
    "        # Stretch out the logits and labels\n",
    "        logits = logits.view(batch*time, channel)\n",
    "        labels = labels.view(batch*time)\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "# Testing the model\n",
    "model = BigramLanguageModel(vocabularySize)\n",
    "outputs, loss = model(inputBatch, outputBatch)\n",
    "print(\"Shape of outputs:\", outputs.shape)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cead918-6d47-4fbc-b806-c5b51654d1d6",
   "metadata": {},
   "source": [
    "Now because we have `92` possible characters, we can actually guess what our loss should be... And we have already covered this in our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a> notebook in more details...\n",
    "\n",
    "But we are expecting our loss to be around:\n",
    "$$\n",
    "-\\ln{(P_x = \\frac{1}{92})} \\approx 4.5217\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb963f8-8605-48d4-b942-569613c7d201",
   "metadata": {},
   "source": [
    "But right now we are getting around `5.3098`, which tells us that our initial predictions are not super diffuse, and have got a little bit of entropy and so we are guessing wrong, but ultimately we are able to evaluate the loss..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943970fd-0df3-441f-bf73-8edae07d7ac7",
   "metadata": {},
   "source": [
    "Now that we can evaluate the quality of the model, we'd like to also be able to `generate` from the model... And once again I'll go a little bit faster, because I covered a lot of these already in my previous notebooks of the `NameWeave` series..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b38d03-1a7b-4722-859b-423407e9214b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding `generate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0d56a-6c51-439e-828b-eef00f4bf9a7",
   "metadata": {},
   "source": [
    "Now because we train our neural network with transformations in the **forward pass**, we can **forward** a single character index (say the new line character `'\\n'` which is also the `0-th character` according to `itos` look-up table and is a fairly good example to forward) and get the predictions (`logits`) from the neural network. \n",
    "\n",
    "Now because these `logits` that come out are a *chunk* of a batch of examples, we need to focus on the **last** character in a chunk, because we are trying to predict the next character in a block (which is none other than the `Time` dimension), so we will pop out the `-1` element of that *chunk* (which will make `(B, T, C) tensor` → `(B, C) tensor`) to get the `logits` properly in a batch.\n",
    "\n",
    "And as we remember, `logits` are the un-normalized probabilities. Which means it needs to go through a non-linearity to get normalized probabilities (for our case we will use `softmax`). We want to apply softmax independently along the **last dimension for each sequence in the batch** (`-1`). This is because we're treating each sequence independently when generating the next token.\n",
    "\n",
    "And then after we have our `probabilities` we can sample `1` character at a time by using `multinomial` sampling distribution for a batch (Which makes it a `(B, C) tensor` → `(B, 1) tensor`) which is none other than the next index in a sequence.\n",
    "\n",
    "And lastly we can **concatenate** each time we generate an `index` to the `nextIndex` to keep generating it in a loop along the `Time` dimension making it a `(B, 1) tensor` → `(B, T+1) tensor` each time we generate...\n",
    "\n",
    "But we also need a stopper for our loop... And we will call it `maximumNewTokens`, which is the number of characters we want to generate from our neural network...\n",
    "\n",
    "And see how we are not going to use `loss` here during generation?\n",
    "\n",
    "So we also need to handle `loss` separately during `forward` to handle both kind of inputs and manage our memory efficiently.\n",
    "\n",
    "So, after this long explanation, we can modify our `BigramLanguageModel` now:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a89403-2d8b-43d3-8206-1bfbafb78ee1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modifying `BigramLanguageModel` for `generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7d682b-012e-4383-9e80-760851f04256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of outputs: torch.Size([32, 92])\n",
      "Loss: tensor(5.3285, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    # Constructor for the model\n",
    "    def __init__(self, vocabularySize):\n",
    "        # Initializing the embedding table\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, vocabularySize)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, indeces, labels=None):\n",
    "        # Index into embeddings to get the logits\n",
    "        logits = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Pop out the shape dimensions\n",
    "            batch, time, channel = logits.shape\n",
    "            # Stretch out the logits and labels\n",
    "            logits = logits.view(batch*time, channel)\n",
    "            labels = labels.view(batch*time)\n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "    # Generation\n",
    "    def generate(self, indeces, maximumNewTokens):\n",
    "        for _ in range(maximumNewTokens):\n",
    "            # Forward Through Model\n",
    "            logits, loss = self(indeces)\n",
    "            # Focus on the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            # Applying softmax for the last dimension\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Sample from distribution\n",
    "            nextIndex = torch.multinomial(probabilities, num_samples=1)\n",
    "            # Concatenate currentIndex with nextIndex\n",
    "            indeces = torch.cat((indeces, nextIndex), dim=1)\n",
    "        return indeces\n",
    "\n",
    "# Testing the model\n",
    "model = BigramLanguageModel(vocabularySize)\n",
    "outputs, loss = model(inputBatch, outputBatch)\n",
    "print(\"Shape of outputs:\", outputs.shape)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3a7f4-40b9-4db5-91e4-d34d27320de4",
   "metadata": {},
   "source": [
    "So we can start generating text from the model now...\n",
    "\n",
    "Now because our `generate()` expects a `(B, T) tensor`, and we decided to give the first context to be a new line character `'\\n'`, create a tensor consisting of a single `0` of `(1, 1)` dimension like this (also setting the `dtype` to `long`):\n",
    "```python\n",
    "torch.zeros((1, 1), dtype=torch.long)\n",
    "```\n",
    "To get a tensor like this:\n",
    "```python\n",
    "tensor([[0]])\n",
    "```\n",
    "\n",
    "And let's say we want to generate `100` characters from the `model`, so we will pass the same value in `maximumNewTokens` parameter.\n",
    "\n",
    "Now remember the example where the **GPT** was able to **generate** multiple responses, given the same `context`?\n",
    "\n",
    "Well, our model also produces multiple responses in a batch given the same `context`, but we are currently interested in the first response so we need to specify `[0]` to pop out the first response, but our `decode` expects a list so we need to convert it back to a list using `tolist()` method, and then we will be able to print out our first response...\n",
    "\n",
    "We can now pack all the concepts into a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf070dad-46f1-4aac-9b05-fad6d398446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"( ”wMi)nd],mRAYfSqL•p‘bI'Jfk\\3y\\Q1P'□?bZ■h’vrA?Sk? Z5;k22~A!1>E;57■z.~Bhrvq□G4e2ScxssA)FFCl~m7nnA~B\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(indeces=torch.zeros((1, 1), dtype=torch.long), maximumNewTokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dffedd3-d6a1-4da0-9387-f5b3f23b7672",
   "metadata": {},
   "source": [
    "This is the output I get the first time:\n",
    "```python\n",
    "\n",
    "\"( ”wMi)nd],mRAYfSqL•p‘bI'Jfk\\3y\\Q1P'□?bZ■h’vrA?Sk? Z5;k22~A!1>E;57■z.~Bhrvq□G4e2ScxssA)FFCl~m7nnA~B\n",
    "```\n",
    "\n",
    "Confused?\n",
    "\n",
    "Don't worry, our model is still untrained, and it just predicts garbage values as of now, and we can train our model now, to make it better...\n",
    "\n",
    "But I'd like to point out that our `generate()` method is written in a generalized way, but right now it is very ridiculous. And that is because we are feeding it the entire context of parallel examples, but right now we only have the simplest Bigram Language Model, which only for example, to predict `'?'` as a next character needed `'k'` as an example and not the entire context sequence, but we only looked at the very last piece of the context. And the point of writing the `generate()` method this was is, right now we have a Bigram Language Model, but we'd like to keep this function **fixed**, but we'd like to work later and the model is able to look further in the history. Right now the history is not used and so it looks silly, but we will be using the history later and that is why we want to do it this way..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407b155-fc8f-456c-91b3-93af51dd2d4d",
   "metadata": {},
   "source": [
    "Now we can move forward and train the model... So that our outputs become a lot less random..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3015662-31d4-47f9-9ed4-ddd304c56660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training `BigramLanguageModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7c3df-0ed9-40e8-ab66-7d7986b17976",
   "metadata": {},
   "source": [
    "Now before training we'd like to initialize an `optimizer`.\n",
    "\n",
    "Well `optimizers` are none other than the algorithms that take the `gradients` and `update` the data based on the `learning rates` and other variables.\n",
    "\n",
    "And during our `NameWeave` series, we have only ever used **Stochastic Gradient Descent (SGD)**.\n",
    "\n",
    "There are many types of `optimizers` that are out there, and the most common ones are:\n",
    "1. Gradient Descent\n",
    "2. Stochastic Gradient Descent\n",
    "3. Adagrad\n",
    "4. Adadelta\n",
    "5. RMSprop\n",
    "6. Adam\n",
    "\n",
    "You can read a lot about them in this <a href=\"https://medium.com/analytics-vidhya/this-blog-post-aims-at-explaining-the-behavior-of-different-algorithms-for-optimizing-gradient-46159a97a8c1\">Medium Post</a>.\n",
    "\n",
    "But for now we will use <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW\">`AdamW`</a> optimizer. And we will specify the parameters to `optimize` on and also the `learning rate` within the arguements of the constructor during initialization..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349934b3-d9eb-4d8a-8569-d35b37065d6e",
   "metadata": {},
   "source": [
    "And the typical good setting for the `learning rate` is roughly `3e-4` but for smaller networks such as this, we can get away with much higher learning rates such as `1e-3`...\n",
    "\n",
    "So let's initialize the optimizer now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9190e7a-80b1-46fb-ae43-339a7f1cc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9c9b9-1234-4039-9bf4-6d2ea98d13ec",
   "metadata": {},
   "source": [
    "Now let's write out our training loop.\n",
    "\n",
    "And we can remember from our old `NameWeave` series that we used to:\n",
    "1. Define a `batchSize` to train on\n",
    "2. Opened a loop for the number of iterations or `epochs`\n",
    "3. Get each batch input and output\n",
    "4. Forwarded the model to calculate `logits` and `loss`\n",
    "5. Set the gradients to `0`\n",
    "6. Called `loss.backward()` for backward pass\n",
    "7. Update the parameters using the `gradients` and the `learning rate`\n",
    "\n",
    "So we can now write the same thing in code...\n",
    "\n",
    "But unlike our `NameWeave` series, the steps for `5` and `7` are going to be a little bit different. Now that we are using the official `optimizer` from PyTorch, to set the gradients to `0` using the `optimizer` we would do something like this `optimizer.zero_grad(set_to_none=True)` and to update the parameters using the `optimizer` we would do something like this `optimizer.step()`.\n",
    "\n",
    "So let's write out the steps that we defined in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b838790-39cf-4364-aac6-fa9a85326228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.876785755157471\n",
      "4.878622531890869\n",
      "4.971441268920898\n",
      "4.972983360290527\n",
      "4.952727794647217\n",
      "4.961724758148193\n",
      "4.920673370361328\n",
      "4.9921674728393555\n",
      "4.9947509765625\n",
      "4.968419075012207\n",
      "4.937097072601318\n",
      "5.007509708404541\n",
      "5.0103230476379395\n",
      "5.033933639526367\n",
      "4.930401802062988\n",
      "4.7458176612854\n",
      "4.932359218597412\n",
      "4.892716407775879\n",
      "4.904999732971191\n",
      "4.885517120361328\n",
      "4.889068126678467\n",
      "4.9522624015808105\n",
      "4.900505542755127\n",
      "4.931491374969482\n",
      "4.893527984619141\n",
      "4.834726810455322\n",
      "4.967082500457764\n",
      "4.929368495941162\n",
      "4.919459342956543\n",
      "4.922502040863037\n",
      "5.044764041900635\n",
      "4.856174468994141\n",
      "4.875720024108887\n",
      "4.952334403991699\n",
      "4.802764415740967\n",
      "4.9507856369018555\n",
      "4.928844451904297\n",
      "4.9131693840026855\n",
      "4.853192329406738\n",
      "4.858636856079102\n",
      "4.826693058013916\n",
      "4.905089855194092\n",
      "4.839353084564209\n",
      "4.882635116577148\n",
      "4.794174671173096\n",
      "4.853604793548584\n",
      "4.845086097717285\n",
      "4.806244850158691\n",
      "4.9436869621276855\n",
      "4.926150798797607\n",
      "4.911518096923828\n",
      "4.927879333496094\n",
      "4.827909469604492\n",
      "4.833471775054932\n",
      "4.927444934844971\n",
      "4.914450645446777\n",
      "4.797436714172363\n",
      "4.864107608795166\n",
      "4.8414998054504395\n",
      "4.876592636108398\n",
      "4.767649173736572\n",
      "4.869225025177002\n",
      "4.850344657897949\n",
      "4.834407806396484\n",
      "4.832826137542725\n",
      "4.833881378173828\n",
      "4.858837604522705\n",
      "4.806900501251221\n",
      "4.891071796417236\n",
      "4.886438369750977\n",
      "4.838352203369141\n",
      "4.835249900817871\n",
      "4.941649913787842\n",
      "4.9147844314575195\n",
      "4.8390326499938965\n",
      "4.836906909942627\n",
      "4.837123870849609\n",
      "4.824272155761719\n",
      "4.810126781463623\n",
      "4.778221130371094\n",
      "4.80024528503418\n",
      "4.80885648727417\n",
      "4.767628192901611\n",
      "4.841028690338135\n",
      "4.82964563369751\n",
      "4.788620948791504\n",
      "4.890244007110596\n",
      "4.788761138916016\n",
      "4.835732936859131\n",
      "4.806081771850586\n",
      "4.7248382568359375\n",
      "4.892731189727783\n",
      "4.726743221282959\n",
      "4.86219596862793\n",
      "4.871935844421387\n",
      "4.850449562072754\n",
      "4.878635406494141\n",
      "4.877942085266113\n",
      "4.855504989624023\n",
      "4.805298328399658\n"
     ]
    }
   ],
   "source": [
    "# We define the number of epochs\n",
    "epochs = 100\n",
    "# We define the batch size\n",
    "batchSize = 32\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # Get the inputBatch and outputBatch\n",
    "    inputBatch, outputBatch = getBatch('train')\n",
    "    # Forward the model\n",
    "    logits, loss = model(inputBatch, outputBatch)\n",
    "    # Setting the gradients to None\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward to calculate gradients\n",
    "    loss.backward()\n",
    "    # Update the gradients\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877272-0735-4a06-ada9-ec410a8f3e5f",
   "metadata": {},
   "source": [
    "Seems like we are optimizing the model, let's now keep the print statement outside to print the loss at the end and train for about `20000` steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51bbcb70-df25-4992-a774-5b495ec08e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3757007122039795\n"
     ]
    }
   ],
   "source": [
    "# We define the number of epochs\n",
    "epochs = 20000\n",
    "# We define the batch size\n",
    "batchSize = 32\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # Get the inputBatch and outputBatch\n",
    "    inputBatch, outputBatch = getBatch('train')\n",
    "    # Forward the model\n",
    "    logits, loss = model(inputBatch, outputBatch)\n",
    "    # Setting the gradients to None\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward to calculate gradients\n",
    "    loss.backward()\n",
    "    # Update the gradients\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adb7a2-dc87-475d-8519-d290c4f90cfe",
   "metadata": {},
   "source": [
    "So we see that we get a loss of `2.3757` and that we have significantly come down from our old loss..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aec475-cff3-47ab-bce2-51b098203efb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sampling from Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e7277-77ce-4acc-bcc8-77781e90085a",
   "metadata": {},
   "source": [
    "So we can try to sample/generate from our model by increasing the `maximumNewTokens` to around `500` to get a sense of a bigger output...\n",
    "\n",
    "Now keep in mind, that this is still the simplest possible model that we could have, and we still won't be able to get a very good result, but for now atleast our loss has improved...\n",
    "\n",
    "So let's generate text now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f29461-fae4-48b7-9428-8abfc9ff5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "“Sifuriner m \n",
      "\n",
      "wat ol. ces \n",
      "\n",
      "n \n",
      "tigerrancoutheven’d t ayoour, rn fowachensprn sethelintese \n",
      "d HExpenecherine \n",
      "k dorerial SCetllyasino, gr pulet tse sand heiok hery, s oweve, atevealy. \n",
      "bover ulind th frishid, ls kne che me Pr, \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Sooke, .K. d awastokller at inuntiveahuromitedde thamerthaplzad p \n",
      "\n",
      "\n",
      "\n",
      "pee woofot oulplelk Thethechllond ostosn hin. Sy Mr Chererm tck Ineyoun,” g, FEDus s iny, \n",
      "Habe s on. m,” ory tsoulabeay tonde Prokheata wredinghererongergredisst nabos'~4ot \n",
      "he acalis... font d h\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(indeces=torch.zeros((1, 1), dtype=torch.long), maximumNewTokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace38de-6c85-45e5-8dcd-7f998b06aab3",
   "metadata": {},
   "source": [
    "It's still a very good improvement than what we had earlier..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94196b0-1231-4e69-b048-2425f9aafac2",
   "metadata": {},
   "source": [
    "Right now, our `tokens` or `characters` are not talking to each other (because, given the previous context of what was generated, we are looking at the very last character to make the predictions about what comes next), and we'd now like to make our `tokens` talk to each other such that they can figure out what is in the `context` so that they can make better predictions about what comes next..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94637b-e8c0-4682-b6ab-95a355afc8d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Moving Code to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63191e90-7b80-4bcc-bba2-074564616074",
   "metadata": {},
   "source": [
    "You probably know about `GPU`s by now...\n",
    "\n",
    "`GPU`s can process many pieces of data simultaneously, making them useful for machine learning, video editing, and gaming applications.\n",
    "\n",
    "So why not update our code such that it is able to run on both a `CPU` and a `GPU` assuming what is available?\n",
    "\n",
    "Great idea, but there's a slight problem... You see, there are many `GPU`s that are out there in the market, and I don't know which you specifically have. But I have an `NVIDIA` GPU. And to process our code in a `GPU` we need to set our specific `GPU` device...\n",
    "\n",
    "Right now, there are three main `GPU` companies out there in the market (Apologies if I don't know about the off market GPU brands):\n",
    "1. NVIDIA (GTX/RTX)\n",
    "2. AMD (RADEON/VEGA/RX)\n",
    "3. INTEL (ARC)\n",
    "\n",
    "Now, I will setup the `GPU` properly for my system (`NVIDIA`), but leave out links for the other two as well, because setting them up is nearly the same with a few pieces here and there..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c3aaad-652f-47e7-8747-9766a6e50524",
   "metadata": {},
   "source": [
    "Now `NVIDIA` specifically works on `CUDA` cores, which utilizes these cores in the GPU to process our data simultaneously...\n",
    "\n",
    "And in order to do that we need two things:\n",
    "1. <a href=\"https://developer.nvidia.com/cuda-downloads\">CUDA Toolkit</a>\n",
    "2. <a href=\"https://developer.nvidia.com/cudnn-downloads\">CUDnn for CUDA</a> (optional)\n",
    "\n",
    "And the installations are pretty straight forward... You just need to follow the setup for `CUDA` first and then `CUDnn` generally comes in a `.zip` package, which you can extract and copy paste the files that correspond to the `CUDA` folder..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafb1ce-0e1c-4e69-a10b-668676b994b1",
   "metadata": {},
   "source": [
    "Now `AMD` specifically uses <a href=\"https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html\">ROCm</a> for it's graphics processing...\n",
    "\n",
    "And `Intel` specifically uses <a href=\"https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/download.htm\">OpenVINO</a> for it's graphics processing...\n",
    "\n",
    "And you can choose your specific `GPU` from these three resource links..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05918c84-66ba-401e-9207-8a01ed7303b5",
   "metadata": {},
   "source": [
    "And finally if you're using `Google Colab` for this script, you can simply change your current runtime type like this:\n",
    "![Google Colab GPU Change](ExplanationMedia/Images/ColabGPUChange.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04280b08-bd37-4717-bf27-81e3cc99fb9c",
   "metadata": {},
   "source": [
    "First you can run two lines of code to check if the `GPU` is available or not using:\n",
    "```python\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "```\n",
    "\n",
    "Now we have to set our code in such a manner, such that it works on both, a `CPU` and a `GPU` depending on what we have right now...\n",
    "\n",
    "In order to do that we write this line of code:\n",
    "```python\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "```\n",
    "\n",
    "Now there is a very specific reason why we set the variable name to `device`, and that is because, during initilization of tensors and models there is an arguement that PyTorch takes in, which is known as `device` and sometimes we shift the device using `.to()` method...\n",
    "\n",
    "Now, by default, the tensors are generated on the `CPU`. Even the model is **initialized** on the CPU. Thus one has to manually ensure that the operations are done using `GPU`...\n",
    "\n",
    "To make this, we need to change our `device` to the specific device we have, and to do that we therefore need to change these things:\n",
    "1. Before returning the `inputs` and `outputs` in `getBatch()` method, we need to change the device using `.to()`, because of tensor initialization.\n",
    "2. After initializing the `model` we need to change the device using `.to()`, because of initialization of model parameters.\n",
    "3. During initialization of the `context` during generation, we need to change the device using `.to`, because of tensor initialization.\n",
    "\n",
    "So let's see the changes now:\n",
    "1. ```python\n",
    "   def getBatch(split):\n",
    "       # Take the trainingData if the split is 'train' otherwise take the validationData\n",
    "       data = trainingData if split=='train' else validationData\n",
    "       # Generates random integers of batchSize between 0 and len(data) - blockSize\n",
    "       indexes = torch.randint(high=len(data) - blockSize, size=(batchSize,))\n",
    "       # Takes the inputs and outputs after stacking them up in a single tensor\n",
    "       inputs = torch.stack([data[i:i+blockSize] for i in indexes])\n",
    "       outputs = torch.stack([data[i+1:i+blockSize+1] for i in indexes])\n",
    "       inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "       return inputs, outputs\n",
    "   ```\n",
    "2. ```python\n",
    "   model = BigramLanguageModel(vocabularySize).to(device=device)\n",
    "   ```\n",
    "3. ```python\n",
    "   context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "   print(decode(model.generate(indeces=context, maximumNewTokens=500)[0].tolist()))\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8cddbe-9b96-466b-89bb-ff3de09129d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fixing Loss Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03113b-d908-4dd9-ad1d-ba3e982a0aec",
   "metadata": {},
   "source": [
    "Right now we have the code for training:\n",
    "```python\n",
    "for _ in range(epochs):\n",
    "    # Get the inputBatch and outputBatch\n",
    "    inputBatch, outputBatch = getBatch('train')\n",
    "    # Forward the model\n",
    "    logits, loss = model(inputBatch, outputBatch)\n",
    "    # Setting the gradients to None\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward to calculate gradients\n",
    "    loss.backward()\n",
    "    # Update the gradients\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee3007-1329-422d-b454-2a077f6c2d44",
   "metadata": {},
   "source": [
    "And in our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a> notebook we have discussed how we calculate loss very cautiously, and how every batch is more-or-less lucky everytime...\n",
    "\n",
    "So we need a loss evaluation method, which averages up the `loss` over multiple batches...\n",
    "\n",
    "So we change our loss calculation to `@torch.no_grad()` decoration, and define a method for calculating loss as `estimateLoss()` such that no gradients are calculated during the loss evaluation every now and then....\n",
    "\n",
    "And we have also seen how setting the training mode to `True` like `training = True` and `False` like `training = False` can create problems in layers like `Batch Normalization`...\n",
    "\n",
    "But now that we have implemented PyTorch Modules in our code, we can do something like `model.train()` to set the mode to `training` and `model.eval()` to set the mode to `evaluation` for our model.\n",
    "\n",
    "So we can now define two hyper-parameters:\n",
    "1. `evaluationIntervals` → Which we will use to call the `estimateLoss()` based on the number of intervals\n",
    "2. `evaluationIterations` → Which we will use to control the number of times the model evaluates its performance on each dataset split\n",
    "\n",
    "So we can have a checker during training to check if the epoch iteration reaches a certain evaluation interval, we call the `estimateLoss()` method like this, and extract the losses and print them to check the losses:\n",
    "```python\n",
    "for iteration in range(epochs):\n",
    "    # Check if iteration reaches interval\n",
    "    if iteration % evaluationIntervals == 0:\n",
    "        # Save the losses in a variable\n",
    "        losses = estimateLoss()\n",
    "        # Print the losses (Training and Validation)\n",
    "        print(f\"Step {iteration}: Training Loss {losses['train']:.4f}, Validation Loss {losses['validation']:.4f}\")\n",
    "\n",
    "    \n",
    "    # Get the inputBatch and outputBatch\n",
    "    inputBatch, outputBatch = getBatch('train')\n",
    "    # Forward the model\n",
    "    logits, loss = model(inputBatch, outputBatch)\n",
    "    # Setting the gradients to None\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward to calculate gradients\n",
    "    loss.backward()\n",
    "    # Update the gradients\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "And inside the `estimateLoss()` method we first set the model to `evaluation` mode and take both the `training` and `validation` splits and take the calculate the loss of batches after forwarding them for `evaluationIterations` storing the losses in a tensor, and then after the `evaluationIterations` are completed, we average out the losses using `mean()` based on the split and set the model back to the `training` mode and return both the `training` and `validation` losses. \n",
    "\n",
    "So we get a code like this:\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def estimateLoss():\n",
    "    output = {}\n",
    "    # Set the model to evalutaion mode\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['train','validation']:\n",
    "        # Define a losses tensor for the `evaluationIterations` size\n",
    "        losses = torch.zeros(evaluationIterations)\n",
    "        for evaluationIteration in range(evaluationIterations):\n",
    "            inputBatch, outputBatch = getBatch(split)\n",
    "            logits, loss = model(inputBatch, outputBatch)\n",
    "            losses[evaluationIteration] = loss.item()\n",
    "        output[split] = losses.mean()\n",
    "        \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d2261-a1ee-4b96-98b2-ff392bc3eaf3",
   "metadata": {},
   "source": [
    "So when we call `estimateLoss()` we are going to monitor pretty accurate `training` and `validation` losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637112e3-0133-4d08-b524-ee4cc1778b48",
   "metadata": {},
   "source": [
    "But right now `model.eval()` and `model.train()` does not actually do anything, but it will come in handy later when we have layers like `Batch Normalization` and `Dropout` layers in our model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de602c0-e4b6-41bc-bf7e-3516b6e27170",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Converting Bigram To a Script - `bigram_v1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c0e70-2a03-4012-8b3e-f0b376a952d7",
   "metadata": {},
   "source": [
    "So, I'd now like to convert our entire code that we have discussed so far into a Python Script, such that the entire code can now be run in a single file, out of the box, assuming you have PyTorch installed, such that we can simplify all the intermediate work that we did...\n",
    "\n",
    "As I have pointed out earlier, I will be completing parts of the discussion and will be releasing the code scripts within the same repository under the directory `GPT Scripts`.\n",
    "\n",
    "And to run each script you just have to specify the `<filename>.py` in the terminal...\n",
    "\n",
    "For now I have named this script as `bigram_v1.py`...\n",
    "\n",
    "And you can run the file using a command like:\n",
    "```bash\n",
    "python bigram_v1.py\n",
    "```\n",
    "\n",
    "That's it... And everything will run out of the box..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca79d3-b4a0-439e-9146-600bc9ab5f91",
   "metadata": {},
   "source": [
    "# Self Attention in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca99586-17e0-4f08-a536-c3e4ef03d681",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mathematical Trick for Self Attention in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46e099e-cd12-4f70-84b7-f4712f9d0f30",
   "metadata": {},
   "source": [
    "Before we discuss `Self Attention`, we'd like to discuss a certain problem that we are currently dealing with... And get used to different ways to solve the problem as well...\n",
    "\n",
    "The problem is, right now we are only focusing on the `last` token of the context (`last` token of the `blockSize`)... But we'd like our model to look further in the context history like this:\\\n",
    "![GPTContextProblem](ExplanationMedia/Images/GPTContextProblem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4402b2-8acf-4137-ab27-09617b8785e0",
   "metadata": {},
   "source": [
    "Which makes the kind of model we want to be \"**autoregressive**\" **(AR)**. The **autoregressive model** specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85799d16-f395-4f8c-a9d3-d1605ffe198b",
   "metadata": {},
   "source": [
    "Now what we'd like to do is take a small *toy-example* and to solve the same problem differently and work our way upto an efficient solution to the problem...\n",
    "\n",
    "Let's take a very small `input` tensor and try to write it out as an example code:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "batch, time, channel = 2, 8, 3\n",
    "inputs = torch.randn(batch, time, channel)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Shape of inputs:\", inputs.shape)\n",
    "```\n",
    "For which I get:\n",
    "```python\n",
    "Inputs: tensor([[[ 2.3787e+00, -3.5896e-01, -7.1692e-01],\n",
    "         [-2.4297e-01, -1.8038e-01,  1.4882e+00],\n",
    "         [ 5.4493e-01,  3.8243e-01,  8.7188e-01],\n",
    "         [-1.9890e+00, -5.4009e-01, -1.5319e+00],\n",
    "         [-9.2356e-01,  7.2013e-01, -5.9540e-01],\n",
    "         [-1.1697e+00,  8.3635e-01,  3.5811e-01],\n",
    "         [ 3.9933e-01, -1.3606e+00,  1.0168e-01],\n",
    "         [-4.8538e-02, -1.1643e+00, -1.5403e-01]],\n",
    "\n",
    "        [[ 1.1998e+00, -8.0983e-01,  1.0315e+00],\n",
    "         [ 1.6720e+00, -1.0681e+00, -9.6532e-01],\n",
    "         [ 3.6006e-01,  3.2209e-01,  5.2594e-01],\n",
    "         [-5.4021e-01, -5.2587e-01,  1.0481e+00],\n",
    "         [-3.8775e-01, -1.3751e+00, -1.0385e-01],\n",
    "         [-9.2093e-01, -1.0048e+00, -1.4028e+00],\n",
    "         [-2.0169e+00, -5.1192e-01, -2.1998e-01],\n",
    "         [-3.3050e-01, -9.1926e-01,  8.9532e-04]]])\n",
    "Shape of inputs: torch.Size([2, 8, 3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec55d10-4b53-40cc-bcb7-4dd21e9f2aad",
   "metadata": {},
   "source": [
    "In the *toy-example* we have a tensor `inputs` with three dimensions:\n",
    "1. `batch` → Number of batches (`batchSize`)\n",
    "2. `time` → Number of tokens (characters) in a block of `blockSize`\n",
    "3. `channel` → Information of the `token` in form of embeddings (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a33e0e-3df7-4cfe-852f-1312d4132013",
   "metadata": {},
   "source": [
    "In this example we have `8` tokens (character) in the `time` dimension, and these tokens are not *talking* to each other...\n",
    "\n",
    "And now we'd like them to *\"talk to each other\"*, we'd like to couple them... And we'd like to couple them in a very specific way...\n",
    "\n",
    "For example, because we have `8` tokens in a sequence, out of these `8` tokens, if we let's say consider the `5`th token... This `5`th token should not communicate with tokens at locations `6`, `7` and `8` (or the future tokens in the sequence), and they should talk to the tokens at locations `4`, `3`, `2` and `1` (or the previous tokens in the context), such that information only flows from previous context to the current time stamp and we cannot get any information from the future because we are about to predict the future..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3a97f-8c72-4ebc-a84b-4aaf1f7d93eb",
   "metadata": {},
   "source": [
    "So, what is the easiest way for tokens to communicate?\n",
    "\n",
    "Let's say I am the `5`th token, and I want to communicate with my past tokens (at `4`, `3`, `2` & `1`), and the simplest way to communate with the past is to just do an **average** of the past with context of my own information. Or in other words, if I am the `5`th token, I would like to take up the `channels` that make-up my information at my step, and also the `channels` in my past, and I'd like to average those up to make it like a `feature vector` that *summarizes me in the context of my history*...\n",
    "\n",
    "Now once again, doing just an average is just a very weak form of \"talking\" or interaction, and makes this communication extremely **lossy**, which makes us lose a lot of information about the *spatial arrangements* of all those tokens... But for now, that's okay, because in the future solutions to the same problem, we will see how we can get this information back...\n",
    "\n",
    "So let's see different versions to solve the problem now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aeb6b0-3f55-4d9c-ba72-590afb69b377",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Version 1 - Naïve Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ccf45d-d013-45ef-8541-bf465becf1d7",
   "metadata": {},
   "source": [
    "In this approach, what we'd like to do is, for our `inputs`, for every single `batch`, for every `token` we'd like to average out all the vectors in all the `previous tokens` including the `current token`.\n",
    "\n",
    "Or\n",
    "```python\n",
    "# We want: inputs[batch, time] = mean_{i<=token} inputs[batch, i]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9118c0f9-09ad-4a4f-926a-ea878c4b7eb7",
   "metadata": {},
   "source": [
    "Now before we dive into the solution, I'd like to discuss a concept called `Bag Of Words`...\n",
    "\n",
    "So what is `Bag of Words`?\n",
    "\n",
    "\n",
    "![Bag Of Words](https://miro.medium.com/v2/resize:fit:720/format:webp/1*3K9GIOVLNu0cRvQap_KaRg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ace40-fdc5-4a8d-8494-f9c2e9c2ae33",
   "metadata": {},
   "source": [
    "The **bag-of-words model** is a model of text which uses a representation of text that is based on an unordered collection (or *\"bag\"*) of words.\n",
    "In natural language processing (NLP), the term \"bag of words\" `(BoW)` typically refers to a simple representation of text where the frequency of each word in a document is counted and represented in a vector.\n",
    "\n",
    "So why mention it?\n",
    "\n",
    "Well you see, our concept is similar but for our case, we have a character level model. \n",
    "The term \"bag of words\" `(BoW)` is used in contexts where we want to represent text data by counting the occurrences and then potentially averaging them out.\n",
    "\n",
    "And we would like to use it in our *toy-example*'s output variable name... 😂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b1f0c-24f7-446a-9188-58086226bf93",
   "metadata": {},
   "source": [
    "So now, let's write out what we want, in the for of code... To get the idea more clear now:\n",
    "```python\n",
    "# Allocating memory for output\n",
    "inputBagOfWords = torch.zeros((batch, time, channel))\n",
    "\n",
    "for b in range(batch):\n",
    "    for t in range(time):\n",
    "        # Every token in the past and current token in the batch\n",
    "        previousInput = inputs[b, :t+1] # (B, T, C) → (T, C)\n",
    "        # Mean over the token or the time dimension\n",
    "        inputBagOfWords[b, t] = torch.mean(previousInput, 0) # (B, T, C)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f683f7-c59a-4090-8ff8-19b774e407b7",
   "metadata": {},
   "source": [
    "Now because we have multiple `batches` in our example, let's compare only one batch (let's say we compare the first batch or `0`-th index) of both `inputs` and `inputBagOfWords` like this:\n",
    "```python\n",
    "print(\"Input BoW:\", inputBagOfWords[0])\n",
    "print(\"Inputs:\", inputs[0])\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "Input BoW: tensor([[ 2.3787, -0.3590, -0.7169],\n",
    "        [ 1.0679, -0.2697,  0.3856],\n",
    "        [ 0.8936, -0.0523,  0.5477],\n",
    "        [ 0.1729, -0.1742,  0.0278],\n",
    "        [-0.0464,  0.0046, -0.0968],\n",
    "        [-0.2336,  0.1432, -0.0210],\n",
    "        [-0.1432, -0.0716, -0.0035],\n",
    "        [-0.1313, -0.2082, -0.0223]])\n",
    "Inputs: tensor([[ 2.3787, -0.3590, -0.7169],\n",
    "        [-0.2430, -0.1804,  1.4882],\n",
    "        [ 0.5449,  0.3824,  0.8719],\n",
    "        [-1.9890, -0.5401, -1.5319],\n",
    "        [-0.9236,  0.7201, -0.5954],\n",
    "        [-1.1697,  0.8363,  0.3581],\n",
    "        [ 0.3993, -1.3606,  0.1017],\n",
    "        [-0.0485, -1.1643, -0.1540]])\n",
    "```\n",
    "\n",
    "And we see that we have, at every `token` or `time` dimension, the average of previous and current token, which is what we want..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91c059-9fc1-4296-b2d9-abc80ad64a92",
   "metadata": {},
   "source": [
    "But in this process we see that we use nested `for-loops` which is extremely inefficient...\n",
    "\n",
    "And now next, what we will see is that we can be extremely efficient with the same problem with `Matrix Multiplication`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e1b37-c9f8-401a-a8c3-4295784f36df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Version 2 - Matrix Multiplication Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe51ec-8e98-43e2-8426-76b16d1124c0",
   "metadata": {},
   "source": [
    "To understand the matrix multiplication approach we will use another *toy-example* for our *toy-example*...\n",
    "\n",
    "Suppose we have two matrices `a` and `b` of sizes `(3, 3)` and `(3, 2)` respectively, and we understand that the resultant matrix `c` will be of shape `(3, 2)` and will be the dot product of columns and rows of the two matrices.\n",
    "\n",
    "For example,\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1\\ 1\\ 1\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9\\ 4\\\\\n",
    "3\\ 9\\\\\n",
    "4\\ 8\\\\\n",
    "\\end{bmatrix}}_{\\text{b}}\n",
    "\\text{=}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "16\\ 21\\\\\n",
    "16\\ 21\\\\\n",
    "16\\ 21\\\\\n",
    "\\end{bmatrix}}_{\\text{c}}\n",
    "\\rightarrow \\text{c = a @ b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4a5e0-b698-4071-8882-2083b3a9aaa5",
   "metadata": {},
   "source": [
    "Let's try to write the same example with code:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(low=0, high=10, size=(3, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"-----\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1de2c-f1fb-44ab-b080-6adf4e2de8b5",
   "metadata": {},
   "source": [
    "Right now we have a very *boring* matrices `a` of just `1`s where it represents `weights` like a linear layer, and `b` represents the `inputs` similarly.\n",
    "\n",
    "And we have repeating elements because we are calculating the same `columns` of `b` with every `row` of `1`s in `a` for each item in `c`...\n",
    "\n",
    "Now instead if we take a lower triangluar matrix of `1`s and keep all the other elements as `0`s for `a`, we have something like this:\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1\\ 0\\ 0\\\\\n",
    "1\\ 1\\ 0\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71085ca-b665-41ef-b8c1-6c45c4da12fa",
   "metadata": {},
   "source": [
    "To do this, we have a method <a href=\"https://pytorch.org/docs/stable/generated/torch.tril.html\">torch.tril()</a> in PyTorch...\n",
    "\n",
    "So we can now modify our code and look at the resulting matrices:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(low=0, high=10, size=(3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"-----\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "```\n",
    "\n",
    "Which gives us something like this:\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1\\ 0\\ 0\\\\\n",
    "1\\ 1\\ 0\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9\\ 4\\\\\n",
    "3\\ 9\\\\\n",
    "4\\ 8\\\\\n",
    "\\end{bmatrix}}_{\\text{b}}\n",
    "\\text{=}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9\\ 4\\\\\n",
    "12\\ 13\\\\\n",
    "16\\ 21\\\\\n",
    "\\end{bmatrix}}_{\\text{c}}\n",
    "\\rightarrow \\text{c = a @ b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa433f3-f016-43d0-9837-9091cf768b9c",
   "metadata": {},
   "source": [
    "See how because of these `0`s the resultant matrix `c` is just a result of an **incremental addition (`sum`) of their respective `columns`**?\n",
    "\n",
    "And in the same fashion, because we all know average(`mean`) is just the addition of all the elements divided by the number of elements, you can start to see how the average(`mean`) would come into the picture now...\n",
    "\n",
    "So because we are dealing with the `weights`(`a`) and trying to manipulate them, and during matrix multiplication in `a` the `rows` play the role, so we can now average(`mean`) them using `normalization` of individual elements, where every `row` sums to one, to get and **incremental average (`mean`) of their respective `columns` in the resultant `c` matrix**...\n",
    "\n",
    "So our code now looks like:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(low=0, high=10, size=(3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"-----\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "```\n",
    "Which gives us:\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1.0000\\ 0.0000\\ 0.0000\\\\\n",
    "0.5000\\ 0.5000\\ 0.0000\\\\\n",
    "0.3333\\ 0.3333\\ 0.3333\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9\\ 4\\\\\n",
    "3\\ 9\\\\\n",
    "4\\ 8\\\\\n",
    "\\end{bmatrix}}_{\\text{b}}\n",
    "\\text{=}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9.0000 \\ 4.0000\\\\\n",
    "6.0000 \\ 6.5000\\\\\n",
    "5.3333 \\ 7.0000\\\\\n",
    "\\end{bmatrix}}_{\\text{c}}\n",
    "\\rightarrow \\text{c = a @ b}\n",
    "$$\n",
    "\n",
    "And this is exactly similar to our original **naïve** approach that we did...\n",
    "\n",
    "Now let's go back to our original *toy-example* and implement this now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbb3ea-7aa9-4b56-9624-defd67792545",
   "metadata": {},
   "source": [
    "Remember how we considered `a` to be `weights` and `b` to be `inputs`...\n",
    "\n",
    "Let's first initialize the `weights` and `inputs` the same way we did before to perform a `Matrix Multiplication` now and think how the `broadcasting` works out for us because we have `batch` dimensions as well...\n",
    "\n",
    "For now we are dealing with the `tokens` or the `time` dimension, so our weights matrix would be of shape `time` by `time`...\n",
    "\n",
    "And we will take `inputBagOfWords` and rename it to `inputBagOfWordsV2` where `V2` represents **version** `2` just so we can compare them later...\n",
    "\n",
    "So our code looks something like this:\n",
    "```python\n",
    "weights = torch.tril(torch.ones(time, time))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "inputBagOfWordsV2 = weights @ inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a7942-6459-454c-95c1-e69d315e32c9",
   "metadata": {},
   "source": [
    "Now let's think through the **broadcasting** of the matrix multiplication operation...\n",
    "\n",
    "For now we have `weights` of two dimensions of size `(8, 8)` or `(T, T)` for our *toy-example* and `inputs` of three dimensions of size `(2, 8, 3)` or `(B, T, C)`...\n",
    "\n",
    "Now during broadcasting, the `weights` will add another `batch` dimension to make the broadcasting work from `(T, T)` to `(B, T, T)` and then perform the multiplication...\n",
    "\n",
    "Which ultimately results in a **batched matrix multiplication** where the multiplication will be applied to all the **batch elements in parallel** and individually. And for each **batch element** there will be a mutliplication between `(T, T)` and `(T, C)` exactly like the operation we discussed earlier...\n",
    "\n",
    "And the resultant `tensor` would be of shape `(B, T, C)` which will make `inputBagOfWords` completely identical to `inputBagOfWordsV2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e3c76-9232-42ef-961d-23378b8eb883",
   "metadata": {},
   "source": [
    "So we can now compare both the the `inputBagOfWords` and `inputBagOfWords2` with <a href=\"https://pytorch.org/docs/stable/generated/torch.allclose.html\">torch.allclose()</a> like this:\n",
    "```python\n",
    "torch.allclose(inputBagOfWords, inputBagOfWordsV2)\n",
    "```\n",
    "for which we get:\n",
    "```python\n",
    "True\n",
    "```\n",
    "\n",
    "And if we want to compare them manually, because both of them are long `tensor`s we can compare the first batch like them to see that they are completely similar like this:\n",
    "```python\n",
    "print(\"First batch of inputBagOfWords:\", inputBagOfWords[0]) \n",
    "print(\"First batch of inputBagOfWordsV2:\", inputBagOfWordsV2[0])\n",
    "```\n",
    "for which we get:\n",
    "```python\n",
    "First batch of inputBagOfWords: tensor([[ 2.3787, -0.3590, -0.7169],\r\n",
    "        [ 1.0679, -0.2697,  0.3856],\r\n",
    "        [ 0.8936, -0.0523,  0.5477],\r\n",
    "        [ 0.1729, -0.1742,  0.0278],\r\n",
    "        [-0.0464,  0.0046, -0.0968],\r\n",
    "        [-0.2336,  0.1432, -0.0210],\r\n",
    "        [-0.1432, -0.0716, -0.0035],\r\n",
    "        [-0.1313, -0.2082, -0.0223]])\r\n",
    "First batch of inputBagOfWordsV2: tensor([[ 2.3787, -0.3590, -0.7169],\r\n",
    "        [ 1.0679, -0.2697,  0.3856],\r\n",
    "        [ 0.8936, -0.0523,  0.5477],\r\n",
    "        [ 0.1729, -0.1742,  0.0278],\r\n",
    "        [-0.0464,  0.0046, -0.0968],\r\n",
    "        [-0.2336,  0.1432, -0.0210],\r\n",
    "        [-0.1432, -0.0716, -0.0035],\r\n",
    "        [-0.1313, -0.2082, -0.0223]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29237a-4a68-4191-bdf9-7e1c183a2dad",
   "metadata": {},
   "source": [
    "So let's conclude what we saw here...\n",
    "\n",
    "We saw that, **we can do weighted aggregation of our past `tokens` or `characters` by using `Matrix Multiplication` of `weights` and the `inputs`, where `weights` are a matrix of lower-triangular fashion of `1`s and other elements as `0`s, and we are doing weighted `sum` and `normalizing` them to get the *rolling* `average` or `mean`**...\n",
    "\n",
    "Now we will look at another way of doing this same exact operation using `softmax`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefbbaa8-6944-499b-969c-036e4a01e042",
   "metadata": {},
   "source": [
    "## Version 3 - Adding Softmax Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fb95c-2a54-4b3a-9ae8-061874695c72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0444203b-5a95-44a0-ad1d-e749f7543304",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85a98b8-d206-434c-aaf3-09365d25300a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1112f38-fbb7-43ce-a602-4dc0e95f2db9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e868b35-3c04-4b55-975b-31276cb6788e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b931a1b-c03c-4707-8693-15eb07c5665a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ed89b27-d70f-402c-9b5e-3e40b210d33f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eff58c89-0370-48b4-89a1-4c95f2932235",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Version 4 - Crux of Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e83982-1444-46a0-a722-7fea08a1fdd9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e7c58cb-ad1b-4c30-9d30-059732e8b89b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Notes on Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b1128-94c9-4ed8-a01c-d60e54490e9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20dc482a-b344-4546-8058-4cc4ae05de94",
   "metadata": {},
   "source": [
    "## Testing for self, will delete later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b56930-70af-4ca3-97a5-16a9f30b0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cd581fc-42e8-49d9-a054-7c9ba38983b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[[ 2.3787e+00, -3.5896e-01, -7.1692e-01],\n",
      "         [-2.4297e-01, -1.8038e-01,  1.4882e+00],\n",
      "         [ 5.4493e-01,  3.8243e-01,  8.7188e-01],\n",
      "         [-1.9890e+00, -5.4009e-01, -1.5319e+00],\n",
      "         [-9.2356e-01,  7.2013e-01, -5.9540e-01],\n",
      "         [-1.1697e+00,  8.3635e-01,  3.5811e-01],\n",
      "         [ 3.9933e-01, -1.3606e+00,  1.0168e-01],\n",
      "         [-4.8538e-02, -1.1643e+00, -1.5403e-01]],\n",
      "\n",
      "        [[ 1.1998e+00, -8.0983e-01,  1.0315e+00],\n",
      "         [ 1.6720e+00, -1.0681e+00, -9.6532e-01],\n",
      "         [ 3.6006e-01,  3.2209e-01,  5.2594e-01],\n",
      "         [-5.4021e-01, -5.2587e-01,  1.0481e+00],\n",
      "         [-3.8775e-01, -1.3751e+00, -1.0385e-01],\n",
      "         [-9.2093e-01, -1.0048e+00, -1.4028e+00],\n",
      "         [-2.0169e+00, -5.1192e-01, -2.1998e-01],\n",
      "         [-3.3050e-01, -9.1926e-01,  8.9532e-04]]])\n",
      "Shape of inputs: torch.Size([2, 8, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(69420)\n",
    "\n",
    "batch, time, channel = 2, 8, 3\n",
    "inputs = torch.randn(batch, time, channel)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Shape of inputs:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5556920e-776e-42f1-a721-c2fa5ba454c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBagOfWords = torch.zeros((batch, time, channel))\n",
    "for b in range(batch):\n",
    "    for t in range(time):\n",
    "        previousInput = inputs[b, :t+1]\n",
    "        inputBagOfWords[b, t] = torch.mean(previousInput, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5bec474-993c-4780-bc1f-2a8c40adcd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input BoW: tensor([[ 2.3787, -0.3590, -0.7169],\n",
      "        [ 1.0679, -0.2697,  0.3856],\n",
      "        [ 0.8936, -0.0523,  0.5477],\n",
      "        [ 0.1729, -0.1742,  0.0278],\n",
      "        [-0.0464,  0.0046, -0.0968],\n",
      "        [-0.2336,  0.1432, -0.0210],\n",
      "        [-0.1432, -0.0716, -0.0035],\n",
      "        [-0.1313, -0.2082, -0.0223]])\n",
      "Inputs: tensor([[ 2.3787, -0.3590, -0.7169],\n",
      "        [-0.2430, -0.1804,  1.4882],\n",
      "        [ 0.5449,  0.3824,  0.8719],\n",
      "        [-1.9890, -0.5401, -1.5319],\n",
      "        [-0.9236,  0.7201, -0.5954],\n",
      "        [-1.1697,  0.8363,  0.3581],\n",
      "        [ 0.3993, -1.3606,  0.1017],\n",
      "        [-0.0485, -1.1643, -0.1540]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input BoW:\", inputBagOfWords[0])\n",
    "print(\"Inputs:\", inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7520df1-b9df-4fe2-b337-a45aaf8454de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 3]), torch.Size([2, 8, 3]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputBagOfWords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a9da16e-7f6a-4180-a858-85c702ab415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "-----\n",
      "b=\n",
      "tensor([[9., 4.],\n",
      "        [3., 9.],\n",
      "        [4., 8.]])\n",
      "-----\n",
      "c=\n",
      "tensor([[9.0000, 4.0000],\n",
      "        [6.0000, 6.5000],\n",
      "        [5.3333, 7.0000]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(69420)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(low=0, high=10, size=(3, 2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"-----\")\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97b5345c-4d4b-4b54-950c-2cf1a92bce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tril(torch.ones(time, time))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "inputBagOfWordsV2 = weights @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2058b7e-294b-4ef0-81d0-efaf187fcd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "832508d2-bbc9-4d8f-85bb-94a7308dbe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(inputBagOfWords, inputBagOfWordsV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aa60ea9-a9d9-4626-9d0f-33bddd0f73a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch of inputBagOfWords: tensor([[ 2.3787, -0.3590, -0.7169],\n",
      "        [ 1.0679, -0.2697,  0.3856],\n",
      "        [ 0.8936, -0.0523,  0.5477],\n",
      "        [ 0.1729, -0.1742,  0.0278],\n",
      "        [-0.0464,  0.0046, -0.0968],\n",
      "        [-0.2336,  0.1432, -0.0210],\n",
      "        [-0.1432, -0.0716, -0.0035],\n",
      "        [-0.1313, -0.2082, -0.0223]])\n",
      "First batch of inputBagOfWordsV2: tensor([[ 2.3787, -0.3590, -0.7169],\n",
      "        [ 1.0679, -0.2697,  0.3856],\n",
      "        [ 0.8936, -0.0523,  0.5477],\n",
      "        [ 0.1729, -0.1742,  0.0278],\n",
      "        [-0.0464,  0.0046, -0.0968],\n",
      "        [-0.2336,  0.1432, -0.0210],\n",
      "        [-0.1432, -0.0716, -0.0035],\n",
      "        [-0.1313, -0.2082, -0.0223]])\n"
     ]
    }
   ],
   "source": [
    "print(\"First batch of inputBagOfWords:\", inputBagOfWords[0]) \n",
    "print(\"First batch of inputBagOfWordsV2:\", inputBagOfWordsV2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f6397-7d7f-4edf-8611-9c21b538a235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97200bd5-1003-43c7-9e1b-f2217ecd2390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
