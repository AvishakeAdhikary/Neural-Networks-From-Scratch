{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9b417b-c07f-4ff7-bf54-dd89f05626ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Welcome to GPT from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35e52d-ea1b-4e9a-89ee-55b892e9d40f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Things to discuss before starting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b496593-73b0-4847-9abc-6a5d19e67d9e",
   "metadata": {},
   "source": [
    "This notebook is a continuation of my previous notebooks in order:\n",
    "1. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/Neural%20Network%20with%20Derivatives.ipynb\">Neural Networks with Derivatives</a>\n",
    "2. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a>\n",
    "3. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Multi%20Layer%20Perceptron.ipynb\">NameWeave - Multi Layer Perceptron</a>\n",
    "4. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a>\n",
    "5. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Manual%20Back%20Propagation.ipynb\">NameWeave - Manual Back Propagation</a>\n",
    "6. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20WaveNet.ipynb\">NameWeave - WaveNet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b91d8-8d4b-498f-8932-8c8b8506087e",
   "metadata": {},
   "source": [
    "Which means, I will be using a lot of the terminologies, explanations, and code from the previous notebooks that I have created in the series...\n",
    "\n",
    "And we will gradually build a complete **GPT** from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff5339-c626-48e0-9135-cb81cfa60d57",
   "metadata": {},
   "source": [
    "This notebook will be a little bit different from the previous notebooks that I have created previously and I will discuss the changes in a bit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a5bb2-04a6-4442-ba16-9f36a2b2eafc",
   "metadata": {},
   "source": [
    "We will use a completely new dataset `Harry_Potter_Books.txt` which is a combined raw text of all the Harry Potter books by J. K. Rowling combined into a single `text` file, instead of `Indian_Names.txt` which we have used in the previous notebooks that contained Indian Firstnames crawled from a website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a01b1-4c47-4a51-8e74-33fb867f41f2",
   "metadata": {},
   "source": [
    "We will also keep softwares like <a href=\"https://chat.openai.com/\">ChatGPT</a>, <a href=\"https://gemini.google.com/app\">Google Gemini</a>, and other Large Language Models (LLM's) in mind and create our own little **Generative Pre-Trained Transformer (GPT)**...\n",
    "\n",
    "And you would probably know by now what these models are and what they do...\n",
    "\n",
    "Our **GPT** is going to be a *character level language model*, instead of a *sub-word-tokenized model* which softwares like ChatGPT and Google Gemini use in their models, and we will discuss everything in a bit...\n",
    "\n",
    "And we will not write all the `TORCH.NN` modules from scratch now, because we have already covered the most important ones already in our previous notebooks, instead we will implement the networks based on `TORCH.NN` library from PyTorch...\n",
    "\n",
    "And most importantly we will start from the simplest model (Bigram Model) and modify the same model within the same notebook and make our way upto the entire `Transformer` architecture...\n",
    "\n",
    "I have created an entire folder as `GPT Scripts` in the root of this repository to save each script for you to run them without even having to use jupyter notebooks. Rather you can simple use the `<filename>.py` to run each model to see how they perform on the same dataset as we move up, using:\n",
    "```bash\n",
    "python <filename>.py\n",
    "```\n",
    "\n",
    "So, it's going to me a long journey and it if going to be legen...wait-for-it...dary. Legendary!!!\\\n",
    "![Barney Stinson Wink](https://media.tenor.com/nJ3EeUPhVKkAAAAM/barny-stinson.gif)\n",
    "\n",
    "So let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b5a45-f476-4f09-a2dd-ecef880a9205",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Understanding GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d72da0-a8c5-4180-bdf4-3bedef669acd",
   "metadata": {},
   "source": [
    "So what is a **GPT**?\n",
    "\n",
    "Well, **GPT** expands for the terminology as **Generative Pre-Trained Transformer**.\n",
    "\n",
    "You see how it consists of three words?\n",
    "\n",
    "Let's look at it in context of each word:\n",
    "1. Generative → Generates New Content\n",
    "2. Pre-Trained → Pre-trained on a dataset\n",
    "3. Transformer → Transformer architecture is being followed to make up the model (Don't worry we will discuss this later)\n",
    "\n",
    "That was easy..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1d95f-acec-487c-84f0-07262f7b7274",
   "metadata": {},
   "source": [
    "Let's understand what **GPT** can do currently...\n",
    "\n",
    "Let's take `ChatGPT` as an example as of now and let's see it's capabilities...\n",
    "\n",
    "![ChatGPT Current Capabilities](https://miro.medium.com/v2/resize:fit:679/1*_3AM0Yhc7qgCvZ_X1L8mhw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40702e51-6551-4716-8635-83d3a6b6ab12",
   "metadata": {},
   "source": [
    "We see that `GPT` goes from left to right and generates text sequentially..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86d794-f0e4-4a1a-831c-a2ff1e6657de",
   "metadata": {},
   "source": [
    "I wanted to show another thing:\n",
    "![ChatGPT Different Responses](ExplanationMedia/Images/ChatGPT_Different_Response.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d7c99-fc6d-4bbc-b687-b2737aa06840",
   "metadata": {},
   "source": [
    "See how we get a different response each time?\n",
    "\n",
    "Which hints us that it is more like a probabilistic system, which is for any one `prompt` it can give us multiple answers..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307442b-c05f-4618-8094-6201513200c6",
   "metadata": {},
   "source": [
    "Now, this is just one example of a `prompt`, and people have come up with billions of different prompts as of now, and in fact there are many websites that index the interactions with `ChatGPT` as well.\n",
    "\n",
    "You can look at this <a href=\"https://writesonic.com/blog/best-chatgpt-examples\">website</a> as an example.\n",
    "\n",
    "We see that it is a very remarkable system, and it is what we call a **\"Language Model\"**.\n",
    "\n",
    "Or in other words, it models the sequence of words or characters (or \"tokens\" more generally) and it knows how words follow each other in English language (even other languages)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2eefa5-76b9-452d-93c5-3817d91de573",
   "metadata": {},
   "source": [
    "Let's understand what **GPT** does from it's perspective...\n",
    "\n",
    "Well it is trying to complete the sequence...\n",
    "\n",
    "In other words, the `inputs` or `task` that we give to the GPT model, it treats it as a *start of a sequence* and it tries to complete the sequence as a whole. Which makes it a language model in this sense...\n",
    "\n",
    "You would think that it is utterly ridiculous and that we cannot just model an entire architecture and make it act like a helpful assistant.\n",
    "\n",
    "Well that is the beauty of it. And we will discuss all the under-the-hood components of what makes a software like `ChatGPT` work.\n",
    "\n",
    "So, What is the neural network architecture under-the-hood that models this sequence of words/characters/tokens?\n",
    "\n",
    "That comes from this paper from Google: <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> from 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bfc2f-df64-4d23-9859-3fb78cce4dcd",
   "metadata": {},
   "source": [
    "This was a landmark paper in Artificial Intelligence that proposed the `Transformer` architecture. But if you start reading this paper, it may seem like a pretty random *machine-translation* paper. And that's because, I think the authors did not fully anticipate the impact it would create in this domain in the years to come...\n",
    "\n",
    "Let's look at the original `Transformer` architecture as of now:\n",
    "![Transformer Archtecture](ExplanationMedia/Images/Transformer_Model_Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffa820-1b0c-4290-b557-1f1719754dfe",
   "metadata": {},
   "source": [
    "And this `Transformer` architecture was copy pasted in huge amount of applications in most recent years...\n",
    "\n",
    "And what we'd like to do now is create something like `ChatGPT`. But we would not be able to completely clone `ChatGPT` because it is a way more serious *production-grade* system which currently requires *thousands* of GPUs and *millions* of dollars to train the network, and also it is trained on a very good *chunk* of internet data. And there are a lot of **pre-training** and **fine-tuning** stages to it.\n",
    "\n",
    "Rather we would like to create a transformer-based language model, and in our case it is going to be a character level language model. And we also don't want to train on a *chunk* of internet, rather we need a smaller dataset (I proposed we work with `Harry_Potter_Books.txt` which is roughly a `7MB` file). And we would try to model how these characters in this dataset, follow each other.\n",
    "\n",
    "Let's take this paragraph for example:\n",
    "```python\n",
    "\"\"\"\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
    "were proud to say that they were perfectly normal, \n",
    "thank you very much.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Given a chunk of these characters in the past:\n",
    "```python\n",
    "\"\"\"\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
    "were proud to say that the\n",
    "\"\"\"\n",
    "```\n",
    "The `Transformer` model will look at these characters as a context in the past, and it is going to predict that the letter `'y'` is likely to come next in the sequence. And it is going to produce (generate) character sequences that look like Harry Potter. And in that process it is going to model all the patterns inside this data.\n",
    "\n",
    "And once we have trained the model, our model will be able to generate *infinite `Harry Potter`*\n",
    "\n",
    "![Harry Potter Woo](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExcjBmNzJ5N2EzMDQzeTB3cXV4ODN5ZGJkdWlldHhleGw3d3hpMGRhMyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/TJO5x5QQM72Q0weWXN/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843798f4-fca5-420d-8caf-4bfea1eb2245",
   "metadata": {},
   "source": [
    "So let's install the required dependencies and load our dataset up and look into the data and what it looks like first..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715def1-2630-4f41-a531-73ce25bbf959",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e06f1d-1362-491a-b516-46f9488e02a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fb2c8-d5ea-4a57-9e1b-a6789e187c66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2270b6-af58-43e5-bbd4-56f13d2f5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74bbb7-345b-4257-852a-0a25b32fda0c",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030a829-eb7a-4432-a021-738bbfb1f758",
   "metadata": {},
   "source": [
    "This time, I will divide the dataset loading part into two forms:\n",
    "1. If you're trying to use `Google Colab` to run the code\n",
    "2. If you're trying to use `Jupyter Notebook locally` to run the code\n",
    "And you can choose between either one of those with our desired mode..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750cf030-de56-4833-b069-70bb13d5d206",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## For Google Colab users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b9527-67ba-4a64-a77a-55bb42734c98",
   "metadata": {},
   "source": [
    "This will download the `Harry_Potter_Books.txt` into your current folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a81b0b-d3f5-4595-9b8d-d8da1ec4b080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/AvishakeAdhikary/Neural-Networks-From-Scratch/main/Datasets/Harry_Potter_Books.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dde7bc-c5c2-4f8f-8f38-3dc419f2c75c",
   "metadata": {},
   "source": [
    "Now you can load up the dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53794f1-26c2-48a7-b282-bff93d6297f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Harry_Potter_Books.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHarry_Potter_Books.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Harry_Potter_Books.txt'"
     ]
    }
   ],
   "source": [
    "with open('Harry_Potter_Books.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1825e0-4180-4279-a8c0-06b8329efa32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## For local Jupyter Notebook users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623e165-f912-4676-9107-41ea3479c2b0",
   "metadata": {},
   "source": [
    "You don't have to download the dataset if you have the entire repository cloned.\n",
    "\n",
    "The dataset `Harry_Potter_Books.txt` is already located inside the `Datasets` directory...\n",
    "\n",
    "So we can simply open the file and look at its content by specifying the relative path..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70974e14-fe05-4cfb-b794-e6773301ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/Harry_Potter_Books.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431416c3-581d-45e6-a660-228adea1ae5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c635c2-e349-458a-b406-b42edb18c275",
   "metadata": {},
   "source": [
    "We can look at the length of the entire dataset and it's number of characters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20aa4c0-dd36-4178-ae4d-6c287dce74d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset in Characters:  6765190\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Dataset in Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004d6ac-34e8-4d8a-8258-35f93fe646a3",
   "metadata": {},
   "source": [
    "We see that it is roughly `6-million` characters...\n",
    "\n",
    "And if you want to look at the first `1000` characters we can do:\n",
    "```python\n",
    "print(text[:1000])\n",
    "```\n",
    "Which prints the output:\n",
    "```python\n",
    "\"\"\"\n",
    "/ \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "THE BOY WHO LIVED \r\n",
    "\r\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \r\n",
    "were proud to say that they were perfectly normal, \r\n",
    "thank you very much. They were the last people you’d \r\n",
    "expect to be involved in anything strange or \r\n",
    "mysterious, because they just didn’t hold with such \r\n",
    "nonsense. \r\n",
    "\r\n",
    "Mr. Dursley was the director of a firm called \r\n",
    "Grunnings, which made drills. He was a big, beefy \r\n",
    "man with hardly any neck, although he did have a \r\n",
    "very large mustache. Mrs. Dursley was thin and \r\n",
    "blonde and had nearly twice the usual amount of \r\n",
    "neck, which came in very useful as she spent so \r\n",
    "much of her time craning over garden fences, spying \r\n",
    "on the neighbors. The Dursley s had a small son \r\n",
    "called Dudley and in their opinion there was no finer \r\n",
    "boy anywhere. \r\n",
    "\r\n",
    "The Dursleys had everything they wanted, but they \r\n",
    "also had a secret, and their greatest fear was that \r\n",
    "somebody would discover it. They didn’t think they \r\n",
    "could bear it if anyone found out about the Potters. \r\n",
    "Mrs. Potter was Mrs. Dursl\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8478f344-d22f-4895-93d1-08b79c709e2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee4734-5dbc-4f5a-95d1-e25e7f74e268",
   "metadata": {},
   "source": [
    "We can now start building our vocabulary, just like we did in our previous notebooks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff89869c-5e80-489e-a3c5-7c6a7ef58ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '—', '‘', '’', '“', '”', '•', '■', '□']\n",
      "Vocabulary Size: 92\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text))) # Gives us all the characters in the english alphabet, hopefully our dataset has all of them\n",
    "vocabularySize = len(characters) # We define a common vocabulary size\n",
    "print(\"Characters:\", characters)\n",
    "print(\"Vocabulary Size:\", vocabularySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4882e5-a2d1-4d15-9939-372895dc2b39",
   "metadata": {},
   "source": [
    "So we have a possible `vocabulary` of `92` characters that our model will be able to see or emit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efcb11-c293-4b73-8b73-09b979a3c0cf",
   "metadata": {},
   "source": [
    "Now we would like to develop a strategy to <strong><i>tokenize</i></strong> our input `text`.\n",
    "\n",
    "And when we say **tokenize** we generally mean to convert raw text as a string to some sequence of integers according to some vocabulary of possible elements...\n",
    "\n",
    "For us, because we are developing a character level language model, so we are simply going to be translating individual `characters` into `integers`.\n",
    "\n",
    "And we will build `4` things here:\n",
    "1. String to Index Vocabulary → `stoi` → A map of `characters` to `integers`\n",
    "2. Index to String Vocabulary → `itos` → A map of `integers` to `characters`\n",
    "3. Token Encoder → That will encode sequence of characters into indeces\n",
    "4. Token Decoder → That will encode sequence of encoded indeces into characters\n",
    "\n",
    "And you will be able to recognize the first two from our previous notebooks...\n",
    "\n",
    "Before we dive in, let's understand the python concept of `lambda` functions, which some you might have forgotten...\n",
    "\n",
    "So what are `lambda` functions?\n",
    "\n",
    "Python Lambda Functions are *anonymous functions* means that the function is without a name. As we already know the `def` keyword is used to define a normal function in Python. Similarly, the `lambda` keyword is used to define an anonymous function in Python.\n",
    "\n",
    "Syntax: `lambda arguments : expression`\n",
    "\n",
    "For example:\n",
    "```python\n",
    "output = lambda input: input+1\n",
    "print(output(input=1))\n",
    "```\n",
    "We would print `2`.\n",
    "\n",
    "Now, let me first run it, then I will explain it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1dcd86-01ee-4ff2-9ecd-7366263bced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOI: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '>': 26, '?': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, 'Z': 53, '\\\\': 54, ']': 55, 'a': 56, 'b': 57, 'c': 58, 'd': 59, 'e': 60, 'f': 61, 'g': 62, 'h': 63, 'i': 64, 'j': 65, 'k': 66, 'l': 67, 'm': 68, 'n': 69, 'o': 70, 'p': 71, 'q': 72, 'r': 73, 's': 74, 't': 75, 'u': 76, 'v': 77, 'w': 78, 'x': 79, 'y': 80, 'z': 81, '|': 82, '~': 83, '—': 84, '‘': 85, '’': 86, '“': 87, '”': 88, '•': 89, '■': 90, '□': 91}\n",
      "ITOS: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '%', 5: '&', 6: \"'\", 7: '(', 8: ')', 9: '*', 10: ',', 11: '-', 12: '.', 13: '/', 14: '0', 15: '1', 16: '2', 17: '3', 18: '4', 19: '5', 20: '6', 21: '7', 22: '8', 23: '9', 24: ':', 25: ';', 26: '>', 27: '?', 28: 'A', 29: 'B', 30: 'C', 31: 'D', 32: 'E', 33: 'F', 34: 'G', 35: 'H', 36: 'I', 37: 'J', 38: 'K', 39: 'L', 40: 'M', 41: 'N', 42: 'O', 43: 'P', 44: 'Q', 45: 'R', 46: 'S', 47: 'T', 48: 'U', 49: 'V', 50: 'W', 51: 'X', 52: 'Y', 53: 'Z', 54: '\\\\', 55: ']', 56: 'a', 57: 'b', 58: 'c', 59: 'd', 60: 'e', 61: 'f', 62: 'g', 63: 'h', 64: 'i', 65: 'j', 66: 'k', 67: 'l', 68: 'm', 69: 'n', 70: 'o', 71: 'p', 72: 'q', 73: 'r', 74: 's', 75: 't', 76: 'u', 77: 'v', 78: 'w', 79: 'x', 80: 'y', 81: 'z', 82: '|', 83: '~', 84: '—', 85: '‘', 86: '’', 87: '“', 88: '”', 89: '•', 90: '■', 91: '□'}\n",
      "Encoded Text:  [39, 60, 62, 60, 69, 59, 56, 73, 80]\n",
      "Decoded Text:  Legendary\n"
     ]
    }
   ],
   "source": [
    "stoi = {character:index for index, character in enumerate(characters)}\n",
    "itos = {index:character for index, character in enumerate(characters)}\n",
    "encode = lambda string: [stoi[character] for character in string] # Token Encoder that takes in a string as an input, and outputs a list of integers\n",
    "decode = lambda list: ''.join([itos[index] for index in list]) # Token Decoder that takes in the encoded list of integers and outputs the decoded string\n",
    "\n",
    "print(\"STOI:\", stoi)\n",
    "print(\"ITOS:\", itos)\n",
    "print(\"Encoded Text: \", encode(\"Legendary\"))\n",
    "print(\"Decoded Text: \", decode(encode(\"Legendary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8b2f0-a80c-49e9-97a9-5965c01139b6",
   "metadata": {},
   "source": [
    "The `Token Encoder` here takes in a `string` or a `sequence of characters` and encodes it into a `list of integers` based on `stoi` mapping. And the `Token Decoder` takes in the encoded `list of integers` and decodes it based on `itos` mapping to get back the exact same string...\n",
    "\n",
    "In other words, it is more like a translation of `characters` into `integers` and `integers` into `characters`, because our model is going to be a character level language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3be15a-b6d6-4607-9bfa-eab54fa8dee6",
   "metadata": {},
   "source": [
    "Now this is only one of many possible `encodings` or `tokenizers` that are out there in the world right now...\n",
    "\n",
    "And people have come up with many such `tokenizers`, for example, Google uses <a href=\"https://github.com/google/sentencepiece\">`sentencepiece`</a>, OpenAI uses <a href=\"https://github.com/openai/tiktoken\">`tiktoken`</a>...\n",
    "\n",
    "And these `tokenizers` which are out there are more like `sub-word` tokenizers, which are **not** encoding `entire words` and also **not** encoding `individual characters`, and more like a `sub-word` unit level `tokenizers` which is usually what's adapted in practice...\n",
    "\n",
    "As an example let's take `tiktoken` vocabulary which uses `Byte-Pair Encoding (BPE)` to encode these `tokens`:\\\n",
    "![Tiktoken Vocabulary](ExplanationMedia/Images/Tiktoken_Vocabulary.png)\n",
    "\n",
    "We see that `tiktoken` has a vocabulary of roughly `50257` which for us is just `92`.\n",
    "\n",
    "And when we try to encode a sample string in `tiktoken`, we get:\\\n",
    "![Tiktoken Example](ExplanationMedia/Images/TikToken_Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b5404-df32-49ae-a752-ff97a9aeb981",
   "metadata": {},
   "source": [
    "We see that we only get `3` outputs for and entire string of `9` characters...\n",
    "\n",
    "Which means that we can *trade-off* `sequences of integers` and `vocabularies`...\n",
    "\n",
    "In other words, we can have a very long `sequences of integers` and very short `vocabularies` or we can have very short `sequences of integers` and very long `vocabularies`...\n",
    "\n",
    "But for now I'd like to keep our `tokenizer` extremely simple using our own character-level tokenizer (meaning we have very small `vocabulary`) and very simple `encode` and `decode` functions, but we do get very long `sequences of integers` as a result...\n",
    "\n",
    "Don't worry, if you'd like I will build a `tokenizer` in the future...\n",
    "\n",
    "So let's now move forward..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27569cc5-193d-4d54-b2e4-798f630390db",
   "metadata": {},
   "source": [
    "Now that we have a `token encoder` and a `token decoder` or effectively a `tokenizer` we can move forward and encode our entire `Harry Potter` dataset...\n",
    "\n",
    "And we will use <a href=\"https://pytorch.org/\">PyTorch</a> library for that:\n",
    "![PyTorch Logo](ExplanationMedia/Images/PyTorchLogo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0b5d2-4800-437b-a6c7-754ce0492e28",
   "metadata": {},
   "source": [
    "So we can now wrap our `text` data after `encoding` it into a `tensor` of datatype `long` because we want *floating-point numbers* to do mathematical transformations on this data later like this:\n",
    "```python\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783af38b-46c0-4477-ab9f-927fe13527ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff76a2-4be5-4eda-ae6b-5688a1104c94",
   "metadata": {},
   "source": [
    "And then we can check the `shape` and `type` of this data and print out the first `100` characters, just like we did before (without decoding it) like this:\n",
    "```python\n",
    "print(data.shape, data.dtype)\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "torch.Size([6765190]) torch.int64\n",
    "```\n",
    "And:\n",
    "```python\n",
    "print(data[:100])\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "tensor([13,  1,  0,  0,  0,  0,  0, 47, 35, 32,  1, 29, 42, 52,  1, 50, 35, 42,\r\n",
    "         1, 39, 36, 49, 32, 31,  1,  0,  0, 40, 73, 12,  1, 56, 69, 59,  1, 40,\r\n",
    "        73, 74, 12,  1, 31, 76, 73, 74, 67, 60, 80, 10,  1, 70, 61,  1, 69, 76,\r\n",
    "        68, 57, 60, 73,  1, 61, 70, 76, 73, 10,  1, 43, 73, 64, 77, 60, 75,  1,\r\n",
    "        31, 73, 64, 77, 60, 10,  1,  0, 78, 60, 73, 60,  1, 71, 73, 70, 76, 59,\r\n",
    "         1, 75, 70,  1, 74, 56, 80,  1, 75, 63])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bda6f2-cf98-47ee-a401-875c08ad876a",
   "metadata": {},
   "source": [
    "And we see that we have a massive list of integers and is an identical translation of the first `100` characters exactly in the `text` file...\n",
    "\n",
    "And the entire dataset is just stretched out into a very large `sequence of integers`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fbe11-71fd-4baf-8fcf-a06d55d37a0d",
   "metadata": {},
   "source": [
    "Now before we move on with our progress, we would like to do one more thing that is we'd like to split our dataset into a `Train` and `Validation` split...\n",
    "\n",
    "So let's do that..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83671c-4c1f-426e-8a38-6bac731d79c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Splitting the Dataset into `Training` and `Validation` splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59615b21-02d3-4231-9872-8ecf8c038948",
   "metadata": {},
   "source": [
    "Now I'd like to split our data into a split of:\n",
    "1. First 90% into `Training` Split\n",
    "2. Last 10% into `Validation` Split\n",
    "\n",
    "And we are doing this to understand, to what extent our model is `overfitting`...\n",
    "\n",
    "Because we don't want our model to copy and create the exact book of `Harry Potter` instead, we want a model that will create `Harry Potter` like text...\n",
    "\n",
    "And here's how I do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40503ff9-d386-4899-ad3b-3de99801faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintyPercentOfDatasetLength = int((0.9 * len(data)))\n",
    "trainingData = data[:nintyPercentOfDatasetLength] # Data up till 90% of the length\n",
    "validationData = data[nintyPercentOfDatasetLength:] # Data from 90% of the length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5531f0-f95f-4e0e-86dc-447955cfd57f",
   "metadata": {},
   "source": [
    "We can now move on to the next part..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
