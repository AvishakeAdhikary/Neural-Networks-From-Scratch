{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9b417b-c07f-4ff7-bf54-dd89f05626ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Welcome to GPT from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35e52d-ea1b-4e9a-89ee-55b892e9d40f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Things to discuss before starting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b496593-73b0-4847-9abc-6a5d19e67d9e",
   "metadata": {},
   "source": [
    "This notebook is a continuation of my previous notebooks in order:\n",
    "1. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/Neural%20Network%20with%20Derivatives.ipynb\">Neural Networks with Derivatives</a>\n",
    "2. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a>\n",
    "3. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Multi%20Layer%20Perceptron.ipynb\">NameWeave - Multi Layer Perceptron</a>\n",
    "4. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a>\n",
    "5. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Manual%20Back%20Propagation.ipynb\">NameWeave - Manual Back Propagation</a>\n",
    "6. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20WaveNet.ipynb\">NameWeave - WaveNet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b91d8-8d4b-498f-8932-8c8b8506087e",
   "metadata": {},
   "source": [
    "Which means, I will be using a lot of the terminologies, explanations, and code from the previous notebooks that I have created in the series...\n",
    "\n",
    "And we will gradually build a complete **GPT** from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff5339-c626-48e0-9135-cb81cfa60d57",
   "metadata": {},
   "source": [
    "This notebook will be a little bit different from the previous notebooks that I have created previously and I will discuss the changes in a bit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a5bb2-04a6-4442-ba16-9f36a2b2eafc",
   "metadata": {},
   "source": [
    "We will use a completely new dataset `Harry_Potter_Books.txt` which is a combined raw text of all the Harry Potter books by J. K. Rowling combined into a single `text` file, instead of `Indian_Names.txt` which we have used in the previous notebooks that contained Indian Firstnames crawled from a website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a01b1-4c47-4a51-8e74-33fb867f41f2",
   "metadata": {},
   "source": [
    "We will also keep softwares like <a href=\"https://chat.openai.com/\">ChatGPT</a>, <a href=\"https://gemini.google.com/app\">Google Gemini</a>, and other Large Language Models (LLM's) in mind and create our own little **Generative Pre-Trained Transformer (GPT)**...\n",
    "\n",
    "And you would probably know by now what these models are and what they do...\n",
    "\n",
    "Our **GPT** is going to be a *character level language model*, instead of a *sub-word-tokenized model* which softwares like ChatGPT and Google Gemini use in their models, and we will discuss everything in a bit...\n",
    "\n",
    "And we will not write all the `TORCH.NN` modules from scratch now, because we have already covered the most important ones already in our previous notebooks, instead we will implement the networks based on `TORCH.NN` library from PyTorch...\n",
    "\n",
    "And most importantly we will start from the simplest model (Bigram Model) and modify the same model within the same notebook and make our way upto the entire `Transformer` architecture...\n",
    "\n",
    "I have created an entire folder as `GPT Scripts` in the root of this repository to save each script for you to run them without even having to use jupyter notebooks. Rather you can simple use the `<filename>.py` to run each model to see how they perform on the same dataset as we move up, using:\n",
    "```bash\n",
    "python <filename>.py\n",
    "```\n",
    "\n",
    "One more thing to discuss is we remember from our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20WaveNet.ipynb\">NameWeave - WaveNet</a> notebook that we refered to multiple dimensions in a tensor as our own made up names, but the thing to know is that in real world these multiple dimensions have names, specifically **batch**, **time** and **channel** dimensions like this in order:\n",
    "\n",
    "$$\n",
    "(B, T, C)\\rightarrow(Batch, Time, Channel)\n",
    "$$\n",
    "\n",
    "And we will refer to these dimensions using the actual names used in real world this time...\n",
    "\n",
    "So, it's going to me a long journey and it if going to be legen...wait-for-it...dary. Legendary!!!\\\n",
    "![Barney Stinson Wink](https://media.tenor.com/nJ3EeUPhVKkAAAAM/barny-stinson.gif)\n",
    "\n",
    "So let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b5a45-f476-4f09-a2dd-ecef880a9205",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Understanding GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d72da0-a8c5-4180-bdf4-3bedef669acd",
   "metadata": {},
   "source": [
    "So what is a **GPT**?\n",
    "\n",
    "Well, **GPT** expands for the terminology as **Generative Pre-Trained Transformer**.\n",
    "\n",
    "You see how it consists of three words?\n",
    "\n",
    "Let's look at it in context of each word:\n",
    "1. Generative → Generates New Content\n",
    "2. Pre-Trained → Pre-trained on a dataset\n",
    "3. Transformer → Transformer architecture is being followed to make up the model (Don't worry we will discuss this later)\n",
    "\n",
    "That was easy..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1d95f-acec-487c-84f0-07262f7b7274",
   "metadata": {},
   "source": [
    "Let's understand what **GPT** can do currently...\n",
    "\n",
    "Let's take `ChatGPT` as an example as of now and let's see it's capabilities...\n",
    "\n",
    "![ChatGPT Current Capabilities](https://miro.medium.com/v2/resize:fit:679/1*_3AM0Yhc7qgCvZ_X1L8mhw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40702e51-6551-4716-8635-83d3a6b6ab12",
   "metadata": {},
   "source": [
    "We see that `GPT` goes from left to right and generates text sequentially..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86d794-f0e4-4a1a-831c-a2ff1e6657de",
   "metadata": {},
   "source": [
    "I wanted to show another thing:\n",
    "![ChatGPT Different Responses](ExplanationMedia/Images/ChatGPT_Different_Response.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d7c99-fc6d-4bbc-b687-b2737aa06840",
   "metadata": {},
   "source": [
    "See how we get a different response each time?\n",
    "\n",
    "Which hints us that it is more like a probabilistic system, which is for any one `prompt` it can give us multiple answers..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307442b-c05f-4618-8094-6201513200c6",
   "metadata": {},
   "source": [
    "Now, this is just one example of a `prompt`, and people have come up with billions of different prompts as of now, and in fact there are many websites that index the interactions with `ChatGPT` as well.\n",
    "\n",
    "You can look at this <a href=\"https://writesonic.com/blog/best-chatgpt-examples\">website</a> as an example.\n",
    "\n",
    "We see that it is a very remarkable system, and it is what we call a **\"Language Model\"**.\n",
    "\n",
    "Or in other words, it models the sequence of words or characters (or \"tokens\" more generally) and it knows how words follow each other in English language (even other languages)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2eefa5-76b9-452d-93c5-3817d91de573",
   "metadata": {},
   "source": [
    "Let's understand what **GPT** does from it's perspective...\n",
    "\n",
    "Well it is trying to complete the sequence...\n",
    "\n",
    "In other words, the `inputs` or `task` that we give to the GPT model, it treats it as a *start of a sequence* and it tries to complete the sequence as a whole. Which makes it a language model in this sense...\n",
    "\n",
    "You would think that it is utterly ridiculous and that we cannot just model an entire architecture and make it act like a helpful assistant.\n",
    "\n",
    "Well that is the beauty of it. And we will discuss all the under-the-hood components of what makes a software like `ChatGPT` work.\n",
    "\n",
    "So, What is the neural network architecture under-the-hood that models this sequence of words/characters/tokens?\n",
    "\n",
    "That comes from this paper from Google: <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> from 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bfc2f-df64-4d23-9859-3fb78cce4dcd",
   "metadata": {},
   "source": [
    "This was a landmark paper in Artificial Intelligence that proposed the `Transformer` architecture. But if you start reading this paper, it may seem like a pretty random *machine-translation* paper. And that's because, I think the authors did not fully anticipate the impact it would create in this domain in the years to come...\n",
    "\n",
    "Let's look at the original `Transformer` architecture as of now:\n",
    "![Transformer Archtecture](ExplanationMedia/Images/Transformer_Model_Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffa820-1b0c-4290-b557-1f1719754dfe",
   "metadata": {},
   "source": [
    "And this `Transformer` architecture was copy pasted in huge amount of applications in most recent years...\n",
    "\n",
    "And what we'd like to do now is create something like `ChatGPT`. But we would not be able to completely clone `ChatGPT` because it is a way more serious *production-grade* system which currently requires *thousands* of GPUs and *millions* of dollars to train the network, and also it is trained on a very good *chunk* of internet data. And there are a lot of **pre-training** and **fine-tuning** stages to it.\n",
    "\n",
    "Rather we would like to create a transformer-based language model, and in our case it is going to be a character level language model. And we also don't want to train on a *chunk* of internet, rather we need a smaller dataset (I proposed we work with `Harry_Potter_Books.txt` which is roughly a `7MB` file). And we would try to model how these characters in this dataset, follow each other.\n",
    "\n",
    "Let's take this paragraph for example:\n",
    "```python\n",
    "\"\"\"\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
    "were proud to say that they were perfectly normal, \n",
    "thank you very much.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Given a chunk of these characters in the past:\n",
    "```python\n",
    "\"\"\"\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
    "were proud to say that the\n",
    "\"\"\"\n",
    "```\n",
    "The `Transformer` model will look at these characters as a context in the past, and it is going to predict that the letter `'y'` is likely to come next in the sequence. And it is going to produce (generate) character sequences that look like Harry Potter. And in that process it is going to model all the patterns inside this data.\n",
    "\n",
    "And once we have trained the model, our model will be able to generate *infinite `Harry Potter`*\n",
    "\n",
    "![Harry Potter Woo](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExcjBmNzJ5N2EzMDQzeTB3cXV4ODN5ZGJkdWlldHhleGw3d3hpMGRhMyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/TJO5x5QQM72Q0weWXN/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843798f4-fca5-420d-8caf-4bfea1eb2245",
   "metadata": {},
   "source": [
    "So let's install the required dependencies and load our dataset up and look into the data and what it looks like first..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715def1-2630-4f41-a531-73ce25bbf959",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e06f1d-1362-491a-b516-46f9488e02a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fb2c8-d5ea-4a57-9e1b-a6789e187c66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2270b6-af58-43e5-bbd4-56f13d2f5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74bbb7-345b-4257-852a-0a25b32fda0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030a829-eb7a-4432-a021-738bbfb1f758",
   "metadata": {},
   "source": [
    "This time, I will divide the dataset loading part into two forms:\n",
    "1. If you're trying to use `Google Colab` to run the code\n",
    "2. If you're trying to use `Jupyter Notebook locally` to run the code\n",
    "And you can choose between either one of those with our desired mode..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750cf030-de56-4833-b069-70bb13d5d206",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## For Google Colab users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b9527-67ba-4a64-a77a-55bb42734c98",
   "metadata": {},
   "source": [
    "This will download the `Harry_Potter_Books.txt` into your current folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a81b0b-d3f5-4595-9b8d-d8da1ec4b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/AvishakeAdhikary/Neural-Networks-From-Scratch/main/Datasets/Harry_Potter_Books.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dde7bc-c5c2-4f8f-8f38-3dc419f2c75c",
   "metadata": {},
   "source": [
    "Now you can load up the dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53794f1-26c2-48a7-b282-bff93d6297f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Harry_Potter_Books.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1825e0-4180-4279-a8c0-06b8329efa32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## For local Jupyter Notebook users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623e165-f912-4676-9107-41ea3479c2b0",
   "metadata": {},
   "source": [
    "You don't have to download the dataset if you have the entire repository cloned.\n",
    "\n",
    "The dataset `Harry_Potter_Books.txt` is already located inside the `Datasets` directory...\n",
    "\n",
    "So we can simply open the file and look at its content by specifying the relative path..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70974e14-fe05-4cfb-b794-e6773301ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/Harry_Potter_Books.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431416c3-581d-45e6-a660-228adea1ae5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c635c2-e349-458a-b406-b42edb18c275",
   "metadata": {},
   "source": [
    "We can look at the length of the entire dataset and it's number of characters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20aa4c0-dd36-4178-ae4d-6c287dce74d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset in Characters:  6765190\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Dataset in Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004d6ac-34e8-4d8a-8258-35f93fe646a3",
   "metadata": {},
   "source": [
    "We see that it is roughly `6-million` characters...\n",
    "\n",
    "And if you want to look at the first `1000` characters we can do:\n",
    "```python\n",
    "print(text[:1000])\n",
    "```\n",
    "Which prints the output:\n",
    "```python\n",
    "\"\"\"\n",
    "/ \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "THE BOY WHO LIVED \r\n",
    "\r\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \r\n",
    "were proud to say that they were perfectly normal, \r\n",
    "thank you very much. They were the last people you’d \r\n",
    "expect to be involved in anything strange or \r\n",
    "mysterious, because they just didn’t hold with such \r\n",
    "nonsense. \r\n",
    "\r\n",
    "Mr. Dursley was the director of a firm called \r\n",
    "Grunnings, which made drills. He was a big, beefy \r\n",
    "man with hardly any neck, although he did have a \r\n",
    "very large mustache. Mrs. Dursley was thin and \r\n",
    "blonde and had nearly twice the usual amount of \r\n",
    "neck, which came in very useful as she spent so \r\n",
    "much of her time craning over garden fences, spying \r\n",
    "on the neighbors. The Dursley s had a small son \r\n",
    "called Dudley and in their opinion there was no finer \r\n",
    "boy anywhere. \r\n",
    "\r\n",
    "The Dursleys had everything they wanted, but they \r\n",
    "also had a secret, and their greatest fear was that \r\n",
    "somebody would discover it. They didn’t think they \r\n",
    "could bear it if anyone found out about the Potters. \r\n",
    "Mrs. Potter was Mrs. Dursl\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8478f344-d22f-4895-93d1-08b79c709e2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee4734-5dbc-4f5a-95d1-e25e7f74e268",
   "metadata": {},
   "source": [
    "We can now start building our vocabulary, just like we did in our previous notebooks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff89869c-5e80-489e-a3c5-7c6a7ef58ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '—', '‘', '’', '“', '”', '•', '■', '□']\n",
      "Vocabulary Size: 92\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text))) # Gives us all the characters in the english alphabet, hopefully our dataset has all of them\n",
    "vocabularySize = len(characters) # We define a common vocabulary size\n",
    "print(\"Characters:\", characters)\n",
    "print(\"Vocabulary Size:\", vocabularySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4882e5-a2d1-4d15-9939-372895dc2b39",
   "metadata": {},
   "source": [
    "So we have a possible `vocabulary` of `92` characters that our model will be able to see or emit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efcb11-c293-4b73-8b73-09b979a3c0cf",
   "metadata": {},
   "source": [
    "Now we would like to develop a strategy to <strong><i>tokenize</i></strong> our input `text`.\n",
    "\n",
    "And when we say **tokenize** we generally mean to convert raw text as a string to some sequence of integers according to some vocabulary of possible elements...\n",
    "\n",
    "For us, because we are developing a character level language model, so we are simply going to be translating individual `characters` into `integers`.\n",
    "\n",
    "And we will build `4` things here:\n",
    "1. String to Index Vocabulary → `stoi` → A map of `characters` to `integers`\n",
    "2. Index to String Vocabulary → `itos` → A map of `integers` to `characters`\n",
    "3. Token Encoder → That will encode sequence of characters into indeces\n",
    "4. Token Decoder → That will encode sequence of encoded indeces into characters\n",
    "\n",
    "And you will be able to recognize the first two from our previous notebooks...\n",
    "\n",
    "Before we dive in, let's understand the python concept of `lambda` functions, which some you might have forgotten...\n",
    "\n",
    "So what are `lambda` functions?\n",
    "\n",
    "Python Lambda Functions are *anonymous functions* means that the function is without a name. As we already know the `def` keyword is used to define a normal function in Python. Similarly, the `lambda` keyword is used to define an anonymous function in Python.\n",
    "\n",
    "Syntax: `lambda arguments : expression`\n",
    "\n",
    "For example:\n",
    "```python\n",
    "output = lambda input: input+1\n",
    "print(output(input=1))\n",
    "```\n",
    "We would print `2`.\n",
    "\n",
    "Now, let me first run it, then I will explain it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b1dcd86-01ee-4ff2-9ecd-7366263bced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOI: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '>': 26, '?': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, 'Z': 53, '\\\\': 54, ']': 55, 'a': 56, 'b': 57, 'c': 58, 'd': 59, 'e': 60, 'f': 61, 'g': 62, 'h': 63, 'i': 64, 'j': 65, 'k': 66, 'l': 67, 'm': 68, 'n': 69, 'o': 70, 'p': 71, 'q': 72, 'r': 73, 's': 74, 't': 75, 'u': 76, 'v': 77, 'w': 78, 'x': 79, 'y': 80, 'z': 81, '|': 82, '~': 83, '—': 84, '‘': 85, '’': 86, '“': 87, '”': 88, '•': 89, '■': 90, '□': 91}\n",
      "ITOS: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '%', 5: '&', 6: \"'\", 7: '(', 8: ')', 9: '*', 10: ',', 11: '-', 12: '.', 13: '/', 14: '0', 15: '1', 16: '2', 17: '3', 18: '4', 19: '5', 20: '6', 21: '7', 22: '8', 23: '9', 24: ':', 25: ';', 26: '>', 27: '?', 28: 'A', 29: 'B', 30: 'C', 31: 'D', 32: 'E', 33: 'F', 34: 'G', 35: 'H', 36: 'I', 37: 'J', 38: 'K', 39: 'L', 40: 'M', 41: 'N', 42: 'O', 43: 'P', 44: 'Q', 45: 'R', 46: 'S', 47: 'T', 48: 'U', 49: 'V', 50: 'W', 51: 'X', 52: 'Y', 53: 'Z', 54: '\\\\', 55: ']', 56: 'a', 57: 'b', 58: 'c', 59: 'd', 60: 'e', 61: 'f', 62: 'g', 63: 'h', 64: 'i', 65: 'j', 66: 'k', 67: 'l', 68: 'm', 69: 'n', 70: 'o', 71: 'p', 72: 'q', 73: 'r', 74: 's', 75: 't', 76: 'u', 77: 'v', 78: 'w', 79: 'x', 80: 'y', 81: 'z', 82: '|', 83: '~', 84: '—', 85: '‘', 86: '’', 87: '“', 88: '”', 89: '•', 90: '■', 91: '□'}\n",
      "Encoded Text:  [39, 60, 62, 60, 69, 59, 56, 73, 80]\n",
      "Decoded Text:  Legendary\n"
     ]
    }
   ],
   "source": [
    "stoi = {character:index for index, character in enumerate(characters)}\n",
    "itos = {index:character for index, character in enumerate(characters)}\n",
    "encode = lambda string: [stoi[character] for character in string] # Token Encoder that takes in a string as an input, and outputs a list of integers\n",
    "decode = lambda list: ''.join([itos[index] for index in list]) # Token Decoder that takes in the encoded list of integers and outputs the decoded string\n",
    "\n",
    "print(\"STOI:\", stoi)\n",
    "print(\"ITOS:\", itos)\n",
    "print(\"Encoded Text: \", encode(\"Legendary\"))\n",
    "print(\"Decoded Text: \", decode(encode(\"Legendary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8b2f0-a80c-49e9-97a9-5965c01139b6",
   "metadata": {},
   "source": [
    "The `Token Encoder` here takes in a `string` or a `sequence of characters` and encodes it into a `list of integers` based on `stoi` mapping. And the `Token Decoder` takes in the encoded `list of integers` and decodes it based on `itos` mapping to get back the exact same string...\n",
    "\n",
    "In other words, it is more like a translation of `characters` into `integers` and `integers` into `characters`, because our model is going to be a character level language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3be15a-b6d6-4607-9bfa-eab54fa8dee6",
   "metadata": {},
   "source": [
    "Now this is only one of many possible `encodings` or `tokenizers` that are out there in the world right now...\n",
    "\n",
    "And people have come up with many such `tokenizers`, for example, Google uses <a href=\"https://github.com/google/sentencepiece\">`sentencepiece`</a>, OpenAI uses <a href=\"https://github.com/openai/tiktoken\">`tiktoken`</a>...\n",
    "\n",
    "And these `tokenizers` which are out there are more like `sub-word` tokenizers, which are **not** encoding `entire words` and also **not** encoding `individual characters`, and more like a `sub-word` unit level `tokenizers` which is usually what's adapted in practice...\n",
    "\n",
    "As an example let's take `tiktoken` vocabulary which uses `Byte-Pair Encoding (BPE)` to encode these `tokens`:\\\n",
    "![Tiktoken Vocabulary](ExplanationMedia/Images/Tiktoken_Vocabulary.png)\n",
    "\n",
    "We see that `tiktoken` has a vocabulary of roughly `50257` which for us is just `92`.\n",
    "\n",
    "And when we try to encode a sample string in `tiktoken`, we get:\\\n",
    "![Tiktoken Example](ExplanationMedia/Images/TikToken_Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b5404-df32-49ae-a752-ff97a9aeb981",
   "metadata": {},
   "source": [
    "We see that we only get `3` outputs for and entire string of `9` characters...\n",
    "\n",
    "Which means that we can *trade-off* `sequences of integers` and `vocabularies`...\n",
    "\n",
    "In other words, we can have a very long `sequences of integers` and very short `vocabularies` or we can have very short `sequences of integers` and very long `vocabularies`...\n",
    "\n",
    "But for now I'd like to keep our `tokenizer` extremely simple using our own character-level tokenizer (meaning we have very small `vocabulary`) and very simple `encode` and `decode` functions, but we do get very long `sequences of integers` as a result...\n",
    "\n",
    "Don't worry, if you'd like I will build a `tokenizer` in the future...\n",
    "\n",
    "So let's now move forward..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27569cc5-193d-4d54-b2e4-798f630390db",
   "metadata": {},
   "source": [
    "Now that we have a `token encoder` and a `token decoder` or effectively a `tokenizer` we can move forward and encode our entire `Harry Potter` dataset...\n",
    "\n",
    "And we will use <a href=\"https://pytorch.org/\">PyTorch</a> library for that:\n",
    "![PyTorch Logo](ExplanationMedia/Images/PyTorchLogo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0b5d2-4800-437b-a6c7-754ce0492e28",
   "metadata": {},
   "source": [
    "So we can now wrap our `text` data after `encoding` it into a `tensor` of datatype `long` because we want *floating-point numbers* to do mathematical transformations on this data later like this:\n",
    "```python\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "783af38b-46c0-4477-ab9f-927fe13527ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff76a2-4be5-4eda-ae6b-5688a1104c94",
   "metadata": {},
   "source": [
    "And then we can check the `shape` and `type` of this data and print out the first `100` characters, just like we did before (without decoding it) like this:\n",
    "```python\n",
    "print(data.shape, data.dtype)\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "torch.Size([6765190]) torch.int64\n",
    "```\n",
    "And:\n",
    "```python\n",
    "print(data[:100])\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "tensor([13,  1,  0,  0,  0,  0,  0, 47, 35, 32,  1, 29, 42, 52,  1, 50, 35, 42,\r\n",
    "         1, 39, 36, 49, 32, 31,  1,  0,  0, 40, 73, 12,  1, 56, 69, 59,  1, 40,\r\n",
    "        73, 74, 12,  1, 31, 76, 73, 74, 67, 60, 80, 10,  1, 70, 61,  1, 69, 76,\r\n",
    "        68, 57, 60, 73,  1, 61, 70, 76, 73, 10,  1, 43, 73, 64, 77, 60, 75,  1,\r\n",
    "        31, 73, 64, 77, 60, 10,  1,  0, 78, 60, 73, 60,  1, 71, 73, 70, 76, 59,\r\n",
    "         1, 75, 70,  1, 74, 56, 80,  1, 75, 63])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bda6f2-cf98-47ee-a401-875c08ad876a",
   "metadata": {},
   "source": [
    "And we see that we have a massive list of integers and is an identical translation of the first `100` characters exactly in the `text` file...\n",
    "\n",
    "And the entire dataset is just stretched out into a very large `sequence of integers`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fbe11-71fd-4baf-8fcf-a06d55d37a0d",
   "metadata": {},
   "source": [
    "Now before we move on with our progress, we would like to do one more thing that is we'd like to split our dataset into a `Train` and `Validation` split...\n",
    "\n",
    "So let's do that..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83671c-4c1f-426e-8a38-6bac731d79c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Splitting the Dataset into `Training` and `Validation` splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59615b21-02d3-4231-9872-8ecf8c038948",
   "metadata": {},
   "source": [
    "Now I'd like to split our data into a split of:\n",
    "1. First 90% into `Training` Split\n",
    "2. Last 10% into `Validation` Split\n",
    "\n",
    "And we are doing this to understand, to what extent our model is `overfitting`...\n",
    "\n",
    "Because we don't want our model to copy and create the exact book of `Harry Potter` instead, we want a model that will create `Harry Potter` like text...\n",
    "\n",
    "And here's how I do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40503ff9-d386-4899-ad3b-3de99801faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintyPercentOfDatasetLength = int((0.9 * len(data)))\n",
    "trainingData = data[:nintyPercentOfDatasetLength] # Data up till 90% of the length\n",
    "validationData = data[nintyPercentOfDatasetLength:] # Data from 90% of the length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5531f0-f95f-4e0e-86dc-447955cfd57f",
   "metadata": {},
   "source": [
    "We can now move on to the next part..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e41fc-c973-498c-a30d-cfb9f8c047e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Data Into Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6399b-5ed0-4b4e-b27e-69c535c78b06",
   "metadata": {},
   "source": [
    "We would like to now proceed to feed these integer sequences into the neural network so that it can train and learn those patterns...\n",
    "\n",
    "**BUT**\n",
    "\n",
    "We need to realise that we are not going to feed in the entire dataset into the neural network because that is going to be extremely computationally heavy, and rather we would load the data into small batches or *chunks* of data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b9b6c-fba0-49f5-9317-5ea8afe9e6c3",
   "metadata": {},
   "source": [
    "Now I typically use the term `block size` and specify a length to it, but this *chunk* of data can be recognized as different terminologies as well, for example `context length`.\n",
    "\n",
    "Let's start with a `blockSize` of just `8`...\n",
    "```python\n",
    "blockSize = 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f5140-2513-4d95-b747-399e00482dfd",
   "metadata": {},
   "source": [
    "Now let's look at the first *chunk* of training data (`blockSize` of `8` + `1`)...\n",
    "\n",
    "I'll explain why this `+1` is there in a bit...\n",
    "\n",
    "So we have:\n",
    "```python\n",
    "trainingData[:blockSize + 1]\n",
    "```\n",
    "For which we get the sequence:\n",
    "```python\n",
    "tensor([13,  1,  0,  0,  0,  0,  0, 47, 35])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e865ff-8b5d-4f85-8fc0-49fce59df9c1",
   "metadata": {},
   "source": [
    "Which is the first `9` characters in a `sequence` in the `training-set`...\n",
    "\n",
    "Now, I'd like to point out that when we take out a *chunk* of data like this, we actually have **multiple examples** packed within it, because all of these characters **follow** each other...\n",
    "\n",
    "And we are going to simultaneously train it at every one of these positions...\n",
    "\n",
    "And in a chunk of `9` characters, there's actually `8` individual examples packed in there...\n",
    "\n",
    "How so?\n",
    "\n",
    "Let's look at it this way...\n",
    "\n",
    "For our example:\n",
    "```python\n",
    "[13,  1,  0,  0,  0,  0,  0, 47, 35]\n",
    "```\n",
    "1. In the context of `[13]` → `1` is likely to come next,\n",
    "2. In the context of `[13,  1]` → `0` is likely to come next,\n",
    "3. In the context of `[13,  1,  0]` → `0` is likely to come next,\n",
    "4. In the context of `[13,  1,  0,  0]` → `0` is likely to come next,\n",
    "5. In the context of `[13,  1,  0,  0,  0]` → `0` is likely to come next,\n",
    "6. In the context of `[13,  1,  0,  0,  0,  0]` → `0` is likely to come next,\n",
    "7. In the context of `[13,  1,  0,  0,  0,  0,  0]` → `47` is likely to come next,\n",
    "8. In the context of `[13,  1,  0,  0,  0,  0,  0, 47]` → `35` is likely to come next.\n",
    "\n",
    "Summing upto `8` individual examples in our case, which is the `blockSize` and we take the `+1` to get the desired `label` for training..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844f6f1-c31e-4b74-bc7d-430536c6de0b",
   "metadata": {},
   "source": [
    "Let's see how we can achieve the same output in a code snippet:\n",
    "```python\n",
    "inputs = trainingData[:blockSize] # First Chuck of Characters\n",
    "outputs = trainingData[1:blockSize + 1] # First Chunk of Characters offset by 1\n",
    "for i in range(blockSize):\n",
    "    context = inputs[:i+1] # Context is inputs upto the offset\n",
    "    label = outputs[i] # Label is the offset\n",
    "    print(f\"When input example is {context}, then the label is: {label}\")\n",
    "```\n",
    "\n",
    "For which we get:\n",
    "```python\r\n",
    "When input example is tensor([13]), then the label is: 1\r\n",
    "When input example is tensor([13,  1]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0,  0,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0,  0,  0,  0]), then the label is: 47\r\n",
    "When input example is tensor([13,  1,  0,  0,  0,  0,  0, 47]), then the label is: 35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7ff83-47a3-4c18-8904-3cbab831730e",
   "metadata": {},
   "source": [
    "One more thing to mention is that we not only train these examples all the way to the context of `blockSize` just for efficiency.\n",
    "\n",
    "**We also want our network to get *\"used to\"* seeing these examples for context of as small as `1` all the way upto the `blockSize` and everything in between.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65986360-6e76-4764-9fc1-2828740745ec",
   "metadata": {},
   "source": [
    "So, during `inference` we can start sampling from as little as `1` character of context. And once it starts sampling it can go all the way upto `blockSize` and after the context of `blockSize` we can start `truncating`, because the neural network will receive more than `blockSize` inputs when its trying to predict the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56596f7-828c-4e0c-8967-ed9b7c059206",
   "metadata": {},
   "source": [
    "And these input examples that we just looked are nothing but the `Time` dimension...\n",
    "\n",
    "But we need to care about the `Batch` dimension now... And that is because, everytime we feed these *chunks* of texts into a `Transformer`, we are going to have **mini-batches** of multiple chunks of texts that are all **stacked-up** in a single tensor (this is done for efficiency, such that we could keep the `GPU`'s busy, because they are very good at parallel processing of data, and we want to process multiple *chunks* of text all at the same time, but they are processed completely independently and they don't \"talk\" to each other)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f12c1-a099-407d-9329-35ba0c3b766b",
   "metadata": {},
   "source": [
    "We will also set up a `seed` so that whatever numbers I see here in my system, you are going to see the same numbers in your system later as well...\n",
    "\n",
    "Let's now generalize our discussion into code and I will discuss what is happening in the code one by one..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5871cb-da5c-4a93-b818-7cea3baa75ae",
   "metadata": {},
   "source": [
    "More specifically let's define a `getBatch()` method try to pick out batches of `batchSize`...\n",
    "\n",
    "Now don't get confused between `blockSize` and `batchSize`...\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "\\begin{align}\n",
    "blockSize \\rightarrow \\text{The number of independent sequences of characters we want to process in parallel} \\\\\n",
    "batchSize \\rightarrow \\text{The maximum context length of predictions}\n",
    "\\end{align}\n",
    "}\n",
    "$$\n",
    "\n",
    "We will now use our older *example* code get batches of examples, but now in context of `batchSize` now, and we will pick random indeces from the entire dataset, which will then be used to process all the possible examples in sequence and their corresponding labels in a batch using `torch.stack` which essentially concatenates a sequence of tensors along a new dimension. Which makes the `inputBatch` and `outputBatch` a `(4, 8)` tensor, where each row in an `inputBatch` is a *chunk* of the training set, and `outputBatch` will be used all the way at the end during **loss-function**...\n",
    "\n",
    "So, we can spell each of these `examples in a sequence` can be spelled out just like we did before to get their corresponding `labels` for each of these examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3535142b-a9e1-4d18-98c8-16ae1e486be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: torch.Size([4, 8])  Values: tensor([[70, 70, 67,  1, 70, 61,  1, 75],\n",
      "        [59,  1,  0, 56, 75,  1, 75, 63],\n",
      "        [59,  1,  0, 71, 67, 60, 69, 75],\n",
      "        [75, 63, 60,  1, 68, 56, 81, 60]])\n",
      "Outputs: torch.Size([4, 8])  Values: tensor([[70, 67,  1, 70, 61,  1, 75, 63],\n",
      "        [ 1,  0, 56, 75,  1, 75, 63, 60],\n",
      "        [ 1,  0, 71, 67, 60, 69, 75, 80],\n",
      "        [63, 60,  1, 68, 56, 81, 60, 10]])\n",
      "---------------------------------------------\n",
      "When input example is [70], then the label is: 70\n",
      "When input example is [70, 70], then the label is: 67\n",
      "When input example is [70, 70, 67], then the label is: 1\n",
      "When input example is [70, 70, 67, 1], then the label is: 70\n",
      "When input example is [70, 70, 67, 1, 70], then the label is: 61\n",
      "When input example is [70, 70, 67, 1, 70, 61], then the label is: 1\n",
      "When input example is [70, 70, 67, 1, 70, 61, 1], then the label is: 75\n",
      "When input example is [70, 70, 67, 1, 70, 61, 1, 75], then the label is: 63\n",
      "When input example is [59], then the label is: 1\n",
      "When input example is [59, 1], then the label is: 0\n",
      "When input example is [59, 1, 0], then the label is: 56\n",
      "When input example is [59, 1, 0, 56], then the label is: 75\n",
      "When input example is [59, 1, 0, 56, 75], then the label is: 1\n",
      "When input example is [59, 1, 0, 56, 75, 1], then the label is: 75\n",
      "When input example is [59, 1, 0, 56, 75, 1, 75], then the label is: 63\n",
      "When input example is [59, 1, 0, 56, 75, 1, 75, 63], then the label is: 60\n",
      "When input example is [59], then the label is: 1\n",
      "When input example is [59, 1], then the label is: 0\n",
      "When input example is [59, 1, 0], then the label is: 71\n",
      "When input example is [59, 1, 0, 71], then the label is: 67\n",
      "When input example is [59, 1, 0, 71, 67], then the label is: 60\n",
      "When input example is [59, 1, 0, 71, 67, 60], then the label is: 69\n",
      "When input example is [59, 1, 0, 71, 67, 60, 69], then the label is: 75\n",
      "When input example is [59, 1, 0, 71, 67, 60, 69, 75], then the label is: 80\n",
      "When input example is [75], then the label is: 63\n",
      "When input example is [75, 63], then the label is: 60\n",
      "When input example is [75, 63, 60], then the label is: 1\n",
      "When input example is [75, 63, 60, 1], then the label is: 68\n",
      "When input example is [75, 63, 60, 1, 68], then the label is: 56\n",
      "When input example is [75, 63, 60, 1, 68, 56], then the label is: 81\n",
      "When input example is [75, 63, 60, 1, 68, 56, 81], then the label is: 60\n",
      "When input example is [75, 63, 60, 1, 68, 56, 81, 60], then the label is: 10\n"
     ]
    }
   ],
   "source": [
    "# We define a manual seed such that you see the same numbers I see in my machine\n",
    "torch.manual_seed(69420)\n",
    "batchSize = 4 # Number of independent sequences of characters we want to process in parallel\n",
    "blockSize = 8 # Maximum context length of predictions\n",
    "\n",
    "def getBatch(split):\n",
    "    # Take the trainingData if the split is 'train' otherwise take the validationData\n",
    "    data = trainingData if split=='train' else validationData\n",
    "    # Generates random integers of batchSize between 0 and len(data) - blockSize\n",
    "    indexes = torch.randint(high=len(data) - blockSize, size=(batchSize,))\n",
    "    # Takes the inputs and outputs after stacking them up in a single tensor\n",
    "    inputs = torch.stack([data[i:i+blockSize] for i in indexes])\n",
    "    outputs = torch.stack([data[i+1:i+blockSize+1] for i in indexes])\n",
    "    return inputs, outputs\n",
    "\n",
    "# We call the method to initialize inputBatch and outputBatch\n",
    "inputBatch, outputBatch = getBatch('train')\n",
    "\n",
    "print(\"Inputs:\", inputBatch.shape, \" Values:\" , inputBatch)\n",
    "print(\"Outputs:\", outputBatch.shape, \" Values:\" , outputBatch)\n",
    "\n",
    "print('---------------------------------------------')\n",
    "\n",
    "for batchIndex in range(batchSize):\n",
    "    for blockIndex in range(blockSize):\n",
    "        context = inputBatch[batchIndex, : blockIndex+1]\n",
    "        label = outputBatch[batchIndex, blockIndex]\n",
    "        print(f\"When input example is {context.tolist()}, then the label is: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93ef1e-66a2-4e1c-a482-05699ec6e9fe",
   "metadata": {},
   "source": [
    "Now that we have our `inputBatch` and `outputBatch` we can start feeding these batches into a neural network and start getting predictions....\n",
    "\n",
    "Now we are going to start off with the simplest possible neural network, which in my opinion is a `Bigram Language Model` which was already covered in our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a> notebook in a lot of depth, and we will rather go faster and implement a `PyTorch Module` directly that implements the `Bigram Language Model`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc651c6-296a-45c2-8b59-4de0387293f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb728173-527c-42c4-879c-035a9adf9861",
   "metadata": {},
   "source": [
    "Before we implement the bigram model, I'd like to discuss this syntax in python:\n",
    "```python\n",
    "class A:\n",
    "    x = 10\n",
    "\n",
    "class B(A):\n",
    "    def show(self):\n",
    "        print(self.x)\n",
    "\n",
    "y = B()\n",
    "y.show()\n",
    "```\n",
    "For which we get the output:\n",
    "```python\n",
    "10\n",
    "```\n",
    "Which essentially means that this syntax (`B(A)`) is used for `inheritence`, which is much more helpful now to implement `Torch Modules` in our own implementations. And that is because `torch.nn.Module` contains many such methods already implemented like `forward(*input)` which let's us define a forward pass and internally manages the `__call__()` method and we can return our calculated `logits` within the `Module` that we are going to define and later call `backward()` on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4fac9-4f30-453e-9970-e6147886dec9",
   "metadata": {},
   "source": [
    "So, we understand that we need an `Embedding Look-Up Table` for each of our characters in the vocabulary. From our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Multi%20Layer%20Perceptron.ipynb\">NameWeave - Multi Layer Perceptron</a> notebook we understand that when we index into such an embedding table of two dimensions using **batches of input** of two dimensions, we get an indexed output of a three dimensional tensor of `(B, T, C)` which we can now refer as the `Batch`, `Time` and `Channel` dimensions of that tensor, which essentially is nothing but for each input of a batch, it picks out a row of the embedding table. In our case, `Batch` is `4` which is the `batchSize`, `Time` is `8` which is the `blockSize` and `Channel` is the embedding dimension we will specify...\n",
    "\n",
    "These indexed embeddings, for now can be interpreted as the `logits` or the scores for the next character in a sequence. Or in other words, we are predicting what comes next based on just the individual identity of a single token (which means they are not *talking* to each other). For example, if a token is say `69`, the token itself will be able to make pretty decent predictions of what comes next by knowing that the token is in fact `69`...\n",
    "\n",
    "So now, let's write our first `model` out, and test it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18c46b-935b-4256-9f4e-e22ec2fa7983",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Writing out BigramModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4555314d-c991-4230-b50a-1e5f5a601ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 92])\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    # Constructor for the model\n",
    "    def __init__(self, vocabularySize):\n",
    "        # Initializing the embedding table\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, vocabularySize)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, indeces):\n",
    "        # Index into embeddings to get the logits\n",
    "        logits = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "        return logits\n",
    "\n",
    "# Testing the model\n",
    "model = BigramLanguageModel(vocabularySize)\n",
    "outputs = model(inputBatch)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1334a1-719a-4365-9a06-21b3d7f1369b",
   "metadata": {},
   "source": [
    "Looks like we do get the scores for every one of those `(4, 8)` positions..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda938d9-60bc-4b51-949f-e58210b358f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding `cross_entropy` loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7e90c-a42e-430d-9000-1738310a4331",
   "metadata": {},
   "source": [
    "Now that we have the predictions of *'what comes next'* we'd like to evaluate the `loss function`. And in the `NameWeave` series we saw that a good way to measure the `loss` or the quality of the predictions is to use the **Negetive Log Likelihood** loss which is also implemented in PyTorch under the name of `cross_entropy`. \n",
    "\n",
    "And remember how I said the `outputBatch` is required when we calculate the `loss function`? \n",
    "\n",
    "This is exactly when we would require the `outputBatch` to calculate the difference on the predictions or the `logits` and their corresponding `labels`. Or in other words, we have the identity of the next character, but how well are we predicting the next character based on the `logits`...\n",
    "\n",
    "And we'd like to call the `cross_entropy` in it's **functional** form, which means we don't have to create a `Module` for it. But when we look at the documentation of <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\">CrossEntropyLoss</a> of PyTorch, we understand that we have **multi-dimensional inputs** (because we have a `(B, T, C) tensor`), but PyTorch `CrossEntropyLoss` expects $(minibatch,C,d_1,d_2,\\ldots,d_K)$ or a `(B, C, T) tensor`.\n",
    "\n",
    "So what to do now with our `(B, T, C) tensor` that we already have?\n",
    "\n",
    "Well we will try to **reshape** our tensor now, in order to fit those `logits` as well as the `labels` which is a `(B, T) tensor`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf711fb-8cdc-454d-94eb-d11876eec7ad",
   "metadata": {},
   "source": [
    "Now because we are only interested in manipulating the `embeddings` of the `logits` (which for our case is the `channel` dimension), we can start treating everything else as a **batch-dimension**.\n",
    "\n",
    "This is good because PyTorch `CrossEntropyLoss` expects a $(minibatch,C)$ tensor...\n",
    "\n",
    "So we can stretch our 3-dimensional tensor into 2-dimensional tensor by combining all the other batch dimensions into a single dimension by multiplying the dimension values into one using `view()` method of PyTorch...\n",
    "\n",
    "And it looks something like this:\n",
    "![CrossEntropyGPTExplanation](ExplanationMedia/Images/CrossEntropyGPTExplanation.png)\n",
    "\n",
    "And then we can print out the `loss` to check where we are as well...\n",
    "\n",
    "So let's do this now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2df3c-f7cc-4ff6-a9ce-eedd3e794a6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Writing out Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c020eea-698f-4c20-adea-517ea24cd2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of outputs: torch.Size([32, 92])\n",
      "Loss: tensor(5.0409, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    # Constructor for the model\n",
    "    def __init__(self, vocabularySize):\n",
    "        # Initializing the embedding table\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, vocabularySize)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, indeces, labels):\n",
    "        # Index into embeddings to get the logits\n",
    "        logits = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "        # Pop out the shape dimensions\n",
    "        batch, time, channel = logits.shape\n",
    "        # Stretch out the logits and labels\n",
    "        logits = logits.view(batch*time, channel)\n",
    "        labels = labels.view(batch*time)\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "# Testing the model\n",
    "model = BigramLanguageModel(vocabularySize)\n",
    "outputs, loss = model(inputBatch, outputBatch)\n",
    "print(\"Shape of outputs:\", outputs.shape)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cead918-6d47-4fbc-b806-c5b51654d1d6",
   "metadata": {},
   "source": [
    "Now because we have `92` possible characters, we can actually guess what our loss should be... And we have already covered this in our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a> notebook in more details...\n",
    "\n",
    "But we are expecting our loss to be around:\n",
    "$$\n",
    "-\\ln{(P_x = \\frac{1}{92})} \\approx 4.5217\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb963f8-8605-48d4-b942-569613c7d201",
   "metadata": {},
   "source": [
    "But right now we are getting around `5.3098`, which tells us that our initial predictions are not super diffuse, and have got a little bit of entropy and so we are guessing wrong, but ultimately we are able to evaluate the loss..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943970fd-0df3-441f-bf73-8edae07d7ac7",
   "metadata": {},
   "source": [
    "Now that we can evaluate the quality of the model, we'd like to also be able to `generate` from the model... And once again I'll go a little bit faster, because I covered a lot of these already in my previous notebooks of the `NameWeave` series..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b38d03-1a7b-4722-859b-423407e9214b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding `generate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0d56a-6c51-439e-828b-eef00f4bf9a7",
   "metadata": {},
   "source": [
    "Now because we train our neural network with transformations in the **forward pass**, we can **forward** a single character index (say the new line character `'\\n'` which is also the `0-th character` according to `itos` look-up table and is a fairly good example to forward) and get the predictions (`logits`) from the neural network. \n",
    "\n",
    "Now because these `logits` that come out are a *chunk* of a batch of examples, we need to focus on the **last** character in a chunk, because we are trying to predict the next character in a block (which is none other than the `Time` dimension), so we will pop out the `-1` element of that *chunk* (which will make `(B, T, C) tensor` → `(B, C) tensor`) to get the `logits` properly in a batch.\n",
    "\n",
    "And as we remember, `logits` are the un-normalized probabilities. Which means it needs to go through a non-linearity to get normalized probabilities (for our case we will use `softmax`). We want to apply softmax independently along the **last dimension for each sequence in the batch** (`-1`). This is because we're treating each sequence independently when generating the next token.\n",
    "\n",
    "And then after we have our `probabilities` we can sample `1` character at a time by using `multinomial` sampling distribution for a batch (Which makes it a `(B, C) tensor` → `(B, 1) tensor`) which is none other than the next index in a sequence.\n",
    "\n",
    "And lastly we can **concatenate** each time we generate an `index` to the `nextIndex` to keep generating it in a loop along the `Time` dimension making it a `(B, 1) tensor` → `(B, T+1) tensor` each time we generate...\n",
    "\n",
    "But we also need a stopper for our loop... And we will call it `maximumNewTokens`, which is the number of characters we want to generate from our neural network...\n",
    "\n",
    "And see how we are not going to use `loss` here during generation?\n",
    "\n",
    "So we also need to handle `loss` separately during `forward` to handle both kind of inputs and manage our memory efficiently.\n",
    "\n",
    "So, after this long explanation, we can modify our `BigramLanguageModel` now:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a89403-2d8b-43d3-8206-1bfbafb78ee1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modifying `BigramLanguageModel` for `generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7d682b-012e-4383-9e80-760851f04256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of outputs: torch.Size([32, 92])\n",
      "Loss: tensor(5.3285, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    # Constructor for the model\n",
    "    def __init__(self, vocabularySize):\n",
    "        # Initializing the embedding table\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, vocabularySize)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, indeces, labels=None):\n",
    "        # Index into embeddings to get the logits\n",
    "        logits = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Pop out the shape dimensions\n",
    "            batch, time, channel = logits.shape\n",
    "            # Stretch out the logits and labels\n",
    "            logits = logits.view(batch*time, channel)\n",
    "            labels = labels.view(batch*time)\n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "    # Generation\n",
    "    def generate(self, indeces, maximumNewTokens):\n",
    "        for _ in range(maximumNewTokens):\n",
    "            # Forward Through Model\n",
    "            logits, loss = self(indeces)\n",
    "            # Focus on the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            # Applying softmax for the last dimension\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Sample from distribution\n",
    "            nextIndex = torch.multinomial(probabilities, num_samples=1)\n",
    "            # Concatenate currentIndex with nextIndex\n",
    "            indeces = torch.cat((indeces, nextIndex), dim=1)\n",
    "        return indeces\n",
    "\n",
    "# Testing the model\n",
    "model = BigramLanguageModel(vocabularySize)\n",
    "outputs, loss = model(inputBatch, outputBatch)\n",
    "print(\"Shape of outputs:\", outputs.shape)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3a7f4-40b9-4db5-91e4-d34d27320de4",
   "metadata": {},
   "source": [
    "So we can start generating text from the model now...\n",
    "\n",
    "Now because our `generate()` expects a `(B, T) tensor`, and we decided to give the first context to be a new line character `'\\n'`, create a tensor consisting of a single `0` of `(1, 1)` dimension like this (also setting the `dtype` to `long`):\n",
    "```python\n",
    "torch.zeros((1, 1), dtype=torch.long)\n",
    "```\n",
    "To get a tensor like this:\n",
    "```python\n",
    "tensor([[0]])\n",
    "```\n",
    "\n",
    "And let's say we want to generate `100` characters from the `model`, so we will pass the same value in `maximumNewTokens` parameter.\n",
    "\n",
    "Now remember the example where the **GPT** was able to **generate** multiple responses, given the same `context`?\n",
    "\n",
    "Well, our model also produces multiple responses in a batch given the same `context`, but we are currently interested in the first response so we need to specify `[0]` to pop out the first response, but our `decode` expects a list so we need to convert it back to a list using `tolist()` method, and then we will be able to print out our first response...\n",
    "\n",
    "We can now pack all the concepts into a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf070dad-46f1-4aac-9b05-fad6d398446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"( ”wMi)nd],mRAYfSqL•p‘bI'Jfk\\3y\\Q1P'□?bZ■h’vrA?Sk? Z5;k22~A!1>E;57■z.~Bhrvq□G4e2ScxssA)FFCl~m7nnA~B\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(indeces=torch.zeros((1, 1), dtype=torch.long), maximumNewTokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dffedd3-d6a1-4da0-9387-f5b3f23b7672",
   "metadata": {},
   "source": [
    "This is the output I get the first time:\n",
    "```python\n",
    "\n",
    "\"( ”wMi)nd],mRAYfSqL•p‘bI'Jfk\\3y\\Q1P'□?bZ■h’vrA?Sk? Z5;k22~A!1>E;57■z.~Bhrvq□G4e2ScxssA)FFCl~m7nnA~B\n",
    "```\n",
    "\n",
    "Confused?\n",
    "\n",
    "Don't worry, our model is still untrained, and it just predicts garbage values as of now, and we can train our model now, to make it better...\n",
    "\n",
    "But I'd like to point out that our `generate()` method is written in a generalized way, but right now it is very ridiculous. And that is because we are feeding it the entire context of parallel examples, but right now we only have the simplest Bigram Language Model, which only for example, to predict `'?'` as a next character needed `'k'` as an example and not the entire context sequence, but we only looked at the very last piece of the context. And the point of writing the `generate()` method this was is, right now we have a Bigram Language Model, but we'd like to keep this function **fixed**, but we'd like to work later and the model is able to look further in the history. Right now the history is not used and so it looks silly, but we will be using the history later and that is why we want to do it this way..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407b155-fc8f-456c-91b3-93af51dd2d4d",
   "metadata": {},
   "source": [
    "Now we can move forward and train the model... So that our outputs become a lot less random..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3015662-31d4-47f9-9ed4-ddd304c56660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training `BigramLanguageModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7c3df-0ed9-40e8-ab66-7d7986b17976",
   "metadata": {},
   "source": [
    "Now before training we'd like to initialize an `optimizer`.\n",
    "\n",
    "Well `optimizers` are none other than the algorithms that take the `gradients` and `update` the data based on the `learning rates` and other variables.\n",
    "\n",
    "And during our `NameWeave` series, we have only ever used **Stochastic Gradient Descent (SGD)**.\n",
    "\n",
    "There are many types of `optimizers` that are out there, and the most common ones are:\n",
    "1. Gradient Descent\n",
    "2. Stochastic Gradient Descent\n",
    "3. Adagrad\n",
    "4. Adadelta\n",
    "5. RMSprop\n",
    "6. Adam\n",
    "\n",
    "You can read a lot about them in this <a href=\"https://medium.com/analytics-vidhya/this-blog-post-aims-at-explaining-the-behavior-of-different-algorithms-for-optimizing-gradient-46159a97a8c1\">Medium Post</a>.\n",
    "\n",
    "But for now we will use <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW\">`AdamW`</a> optimizer. And we will specify the parameters to `optimize` on and also the `learning rate` within the arguements of the constructor during initialization..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349934b3-d9eb-4d8a-8569-d35b37065d6e",
   "metadata": {},
   "source": [
    "And the typical good setting for the `learning rate` is roughly `3e-4` but for smaller networks such as this, we can get away with much higher learning rates such as `1e-3`...\n",
    "\n",
    "So let's initialize the optimizer now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9190e7a-80b1-46fb-ae43-339a7f1cc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9c9b9-1234-4039-9bf4-6d2ea98d13ec",
   "metadata": {},
   "source": [
    "Now let's write out our training loop.\n",
    "\n",
    "And we can remember from our old `NameWeave` series that we used to:\n",
    "1. Define a `batchSize` to train on\n",
    "2. Opened a loop for the number of iterations or `epochs`\n",
    "3. Get each batch input and output\n",
    "4. Forwarded the model to calculate `logits` and `loss`\n",
    "5. Set the gradients to `0`\n",
    "6. Called `loss.backward()` for backward pass\n",
    "7. Update the parameters using the `gradients` and the `learning rate`\n",
    "\n",
    "So we can now write the same thing in code...\n",
    "\n",
    "But unlike our `NameWeave` series, the steps for `5` and `7` are going to be a little bit different. Now that we are using the official `optimizer` from PyTorch, to set the gradients to `0` using the `optimizer` we would do something like this `optimizer.zero_grad(set_to_none=True)` and to update the parameters using the `optimizer` we would do something like this `optimizer.step()`.\n",
    "\n",
    "So let's write out the steps that we defined in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b838790-39cf-4364-aac6-fa9a85326228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.876785755157471\n",
      "4.878622531890869\n",
      "4.971441268920898\n",
      "4.972983360290527\n",
      "4.952727794647217\n",
      "4.961724758148193\n",
      "4.920673370361328\n",
      "4.9921674728393555\n",
      "4.9947509765625\n",
      "4.968419075012207\n",
      "4.937097072601318\n",
      "5.007509708404541\n",
      "5.0103230476379395\n",
      "5.033933639526367\n",
      "4.930401802062988\n",
      "4.7458176612854\n",
      "4.932359218597412\n",
      "4.892716407775879\n",
      "4.904999732971191\n",
      "4.885517120361328\n",
      "4.889068126678467\n",
      "4.9522624015808105\n",
      "4.900505542755127\n",
      "4.931491374969482\n",
      "4.893527984619141\n",
      "4.834726810455322\n",
      "4.967082500457764\n",
      "4.929368495941162\n",
      "4.919459342956543\n",
      "4.922502040863037\n",
      "5.044764041900635\n",
      "4.856174468994141\n",
      "4.875720024108887\n",
      "4.952334403991699\n",
      "4.802764415740967\n",
      "4.9507856369018555\n",
      "4.928844451904297\n",
      "4.9131693840026855\n",
      "4.853192329406738\n",
      "4.858636856079102\n",
      "4.826693058013916\n",
      "4.905089855194092\n",
      "4.839353084564209\n",
      "4.882635116577148\n",
      "4.794174671173096\n",
      "4.853604793548584\n",
      "4.845086097717285\n",
      "4.806244850158691\n",
      "4.9436869621276855\n",
      "4.926150798797607\n",
      "4.911518096923828\n",
      "4.927879333496094\n",
      "4.827909469604492\n",
      "4.833471775054932\n",
      "4.927444934844971\n",
      "4.914450645446777\n",
      "4.797436714172363\n",
      "4.864107608795166\n",
      "4.8414998054504395\n",
      "4.876592636108398\n",
      "4.767649173736572\n",
      "4.869225025177002\n",
      "4.850344657897949\n",
      "4.834407806396484\n",
      "4.832826137542725\n",
      "4.833881378173828\n",
      "4.858837604522705\n",
      "4.806900501251221\n",
      "4.891071796417236\n",
      "4.886438369750977\n",
      "4.838352203369141\n",
      "4.835249900817871\n",
      "4.941649913787842\n",
      "4.9147844314575195\n",
      "4.8390326499938965\n",
      "4.836906909942627\n",
      "4.837123870849609\n",
      "4.824272155761719\n",
      "4.810126781463623\n",
      "4.778221130371094\n",
      "4.80024528503418\n",
      "4.80885648727417\n",
      "4.767628192901611\n",
      "4.841028690338135\n",
      "4.82964563369751\n",
      "4.788620948791504\n",
      "4.890244007110596\n",
      "4.788761138916016\n",
      "4.835732936859131\n",
      "4.806081771850586\n",
      "4.7248382568359375\n",
      "4.892731189727783\n",
      "4.726743221282959\n",
      "4.86219596862793\n",
      "4.871935844421387\n",
      "4.850449562072754\n",
      "4.878635406494141\n",
      "4.877942085266113\n",
      "4.855504989624023\n",
      "4.805298328399658\n"
     ]
    }
   ],
   "source": [
    "# We define the number of epochs\n",
    "epochs = 100\n",
    "# We define the batch size\n",
    "batchSize = 32\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # Get the inputBatch and outputBatch\n",
    "    inputBatch, outputBatch = getBatch('train')\n",
    "    # Forward the model\n",
    "    logits, loss = model(inputBatch, outputBatch)\n",
    "    # Setting the gradients to None\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward to calculate gradients\n",
    "    loss.backward()\n",
    "    # Update the gradients\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877272-0735-4a06-ada9-ec410a8f3e5f",
   "metadata": {},
   "source": [
    "Seems like we are optimizing the model, let's now keep the print statement outside to print the loss at the end and train for about `20000` steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51bbcb70-df25-4992-a774-5b495ec08e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3757007122039795\n"
     ]
    }
   ],
   "source": [
    "# We define the number of epochs\n",
    "epochs = 20000\n",
    "# We define the batch size\n",
    "batchSize = 32\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # Get the inputBatch and outputBatch\n",
    "    inputBatch, outputBatch = getBatch('train')\n",
    "    # Forward the model\n",
    "    logits, loss = model(inputBatch, outputBatch)\n",
    "    # Setting the gradients to None\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward to calculate gradients\n",
    "    loss.backward()\n",
    "    # Update the gradients\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adb7a2-dc87-475d-8519-d290c4f90cfe",
   "metadata": {},
   "source": [
    "So we see that we get a loss of `2.3757` and that we have significantly come down from our old loss..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aec475-cff3-47ab-bce2-51b098203efb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sampling from Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e7277-77ce-4acc-bcc8-77781e90085a",
   "metadata": {},
   "source": [
    "So we can try to sample/generate from our model by increasing the `maximumNewTokens` to around `500` to get a sense of a bigger output...\n",
    "\n",
    "Now keep in mind, that this is still the simplest possible model that we could have, and we still won't be able to get a very good result, but for now atleast our loss has improved...\n",
    "\n",
    "So let's generate text now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f29461-fae4-48b7-9428-8abfc9ff5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "“Sifuriner m \n",
      "\n",
      "wat ol. ces \n",
      "\n",
      "n \n",
      "tigerrancoutheven’d t ayoour, rn fowachensprn sethelintese \n",
      "d HExpenecherine \n",
      "k dorerial SCetllyasino, gr pulet tse sand heiok hery, s oweve, atevealy. \n",
      "bover ulind th frishid, ls kne che me Pr, \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Sooke, .K. d awastokller at inuntiveahuromitedde thamerthaplzad p \n",
      "\n",
      "\n",
      "\n",
      "pee woofot oulplelk Thethechllond ostosn hin. Sy Mr Chererm tck Ineyoun,” g, FEDus s iny, \n",
      "Habe s on. m,” ory tsoulabeay tonde Prokheata wredinghererongergredisst nabos'~4ot \n",
      "he acalis... font d h\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(indeces=torch.zeros((1, 1), dtype=torch.long), maximumNewTokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace38de-6c85-45e5-8dcd-7f998b06aab3",
   "metadata": {},
   "source": [
    "It's still a very good improvement than what we had earlier..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94196b0-1231-4e69-b048-2425f9aafac2",
   "metadata": {},
   "source": [
    "Right now, our `tokens` or `characters` are not talking to each other (because, given the previous context of what was generated, we are looking at the very last character to make the predictions about what comes next), and we'd now like to make our `tokens` talk to each other such that they can figure out what is in the `context` so that they can make better predictions about what comes next..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94637b-e8c0-4682-b6ab-95a355afc8d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Moving Code to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63191e90-7b80-4bcc-bba2-074564616074",
   "metadata": {},
   "source": [
    "You probably know about `GPU`s by now...\n",
    "\n",
    "`GPU`s can process many pieces of data simultaneously, making them useful for machine learning, video editing, and gaming applications.\n",
    "\n",
    "So why not update our code such that it is able to run on both a `CPU` and a `GPU` assuming what is available?\n",
    "\n",
    "Great idea, but there's a slight problem... You see, there are many `GPU`s that are out there in the market, and I don't know which you specifically have. But I have an `NVIDIA` GPU. And to process our code in a `GPU` we need to set our specific `GPU` device...\n",
    "\n",
    "Right now, there are three main `GPU` companies out there in the market (Apologies if I don't know about the off market GPU brands):\n",
    "1. NVIDIA (GTX/RTX)\n",
    "2. AMD (RADEON/VEGA/RX)\n",
    "3. INTEL (ARC)\n",
    "\n",
    "Now, I will setup the `GPU` properly for my system (`NVIDIA`), but leave out links for the other two as well, because setting them up is nearly the same with a few pieces here and there..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c3aaad-652f-47e7-8747-9766a6e50524",
   "metadata": {},
   "source": [
    "Now `NVIDIA` specifically works on `CUDA` cores, which utilizes these cores in the GPU to process our data simultaneously...\n",
    "\n",
    "And in order to do that we need two things:\n",
    "1. <a href=\"https://developer.nvidia.com/cuda-downloads\">CUDA Toolkit</a>\n",
    "2. <a href=\"https://developer.nvidia.com/cudnn-downloads\">CUDnn for CUDA</a> (optional)\n",
    "\n",
    "And the installations are pretty straight forward... You just need to follow the setup for `CUDA` first and then `CUDnn` generally comes in a `.zip` package, which you can extract and copy paste the files that correspond to the `CUDA` folder..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafb1ce-0e1c-4e69-a10b-668676b994b1",
   "metadata": {},
   "source": [
    "Now `AMD` specifically uses <a href=\"https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html\">ROCm</a> for it's graphics processing...\n",
    "\n",
    "And `Intel` specifically uses <a href=\"https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/download.htm\">OpenVINO</a> for it's graphics processing...\n",
    "\n",
    "And you can choose your specific `GPU` from these three resource links..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05918c84-66ba-401e-9207-8a01ed7303b5",
   "metadata": {},
   "source": [
    "And finally if you're using `Google Colab` for this script, you can simply change your current runtime type like this:\n",
    "![Google Colab GPU Change](ExplanationMedia/Images/ColabGPUChange.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04280b08-bd37-4717-bf27-81e3cc99fb9c",
   "metadata": {},
   "source": [
    "First you can run two lines of code to check if the `GPU` is available or not using:\n",
    "```python\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "```\n",
    "\n",
    "Now we have to set our code in such a manner, such that it works on both, a `CPU` and a `GPU` depending on what we have right now...\n",
    "\n",
    "In order to do that we write this line of code:\n",
    "```python\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "```\n",
    "\n",
    "Now there is a very specific reason why we set the variable name to `device`, and that is because, during initilization of tensors and models there is an arguement that PyTorch takes in, which is known as `device` and sometimes we shift the device using `.to()` method...\n",
    "\n",
    "Now, by default, the tensors are generated on the `CPU`. Even the model is **initialized** on the CPU. Thus one has to manually ensure that the operations are done using `GPU`...\n",
    "\n",
    "To make this, we need to change our `device` to the specific device we have, and to do that we therefore need to change these things:\n",
    "1. Before returning the `inputs` and `outputs` in `getBatch()` method, we need to change the device using `.to()`, because of tensor initialization.\n",
    "2. After initializing the `model` we need to change the device using `.to()`, because of initialization of model parameters.\n",
    "3. During initialization of the `context` during generation, we need to change the device using `.to`, because of tensor initialization.\n",
    "\n",
    "So let's see the changes now:\n",
    "1. ```python\n",
    "   def getBatch(split):\n",
    "       # Take the trainingData if the split is 'train' otherwise take the validationData\n",
    "       data = trainingData if split=='train' else validationData\n",
    "       # Generates random integers of batchSize between 0 and len(data) - blockSize\n",
    "       indexes = torch.randint(high=len(data) - blockSize, size=(batchSize,))\n",
    "       # Takes the inputs and outputs after stacking them up in a single tensor\n",
    "       inputs = torch.stack([data[i:i+blockSize] for i in indexes])\n",
    "       outputs = torch.stack([data[i+1:i+blockSize+1] for i in indexes])\n",
    "       inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "       return inputs, outputs\n",
    "   ```\n",
    "2. ```python\n",
    "   model = BigramLanguageModel(vocabularySize).to(device=device)\n",
    "   ```\n",
    "3. ```python\n",
    "   context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "   print(decode(model.generate(indeces=context, maximumNewTokens=500)[0].tolist()))\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8cddbe-9b96-466b-89bb-ff3de09129d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fixing Loss Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03113b-d908-4dd9-ad1d-ba3e982a0aec",
   "metadata": {},
   "source": [
    "Right now we have the code for training:\n",
    "```python\n",
    "for _ in range(epochs):\n",
    "    # Get the inputBatch and outputBatch\n",
    "    inputBatch, outputBatch = getBatch('train')\n",
    "    # Forward the model\n",
    "    logits, loss = model(inputBatch, outputBatch)\n",
    "    # Setting the gradients to None\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward to calculate gradients\n",
    "    loss.backward()\n",
    "    # Update the gradients\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee3007-1329-422d-b454-2a077f6c2d44",
   "metadata": {},
   "source": [
    "And in our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a> notebook we have discussed how we calculate loss very cautiously, and how every batch is more-or-less lucky everytime...\n",
    "\n",
    "So we need a loss evaluation method, which averages up the `loss` over multiple batches...\n",
    "\n",
    "So we change our loss calculation to `@torch.no_grad()` decoration, and define a method for calculating loss as `estimateLoss()` such that no gradients are calculated during the loss evaluation every now and then....\n",
    "\n",
    "And we have also seen how setting the training mode to `True` like `training = True` and `False` like `training = False` can create problems in layers like `Batch Normalization`...\n",
    "\n",
    "But now that we have implemented PyTorch Modules in our code, we can do something like `model.train()` to set the mode to `training` and `model.eval()` to set the mode to `evaluation` for our model.\n",
    "\n",
    "So we can now define two hyper-parameters:\n",
    "1. `evaluationIntervals` → Which we will use to call the `estimateLoss()` based on the number of intervals\n",
    "2. `evaluationIterations` → Which we will use to control the number of times the model evaluates its performance on each dataset split\n",
    "\n",
    "So we can have a checker during training to check if the epoch iteration reaches a certain evaluation interval, we call the `estimateLoss()` method like this, and extract the losses and print them to check the losses:\n",
    "```python\n",
    "for iteration in range(epochs):\n",
    "    # Check if iteration reaches interval\n",
    "    if iteration % evaluationIntervals == 0:\n",
    "        # Save the losses in a variable\n",
    "        losses = estimateLoss()\n",
    "        # Print the losses (Training and Validation)\n",
    "        print(f\"Step {iteration}: Training Loss {losses['train']:.4f}, Validation Loss {losses['validation']:.4f}\")\n",
    "\n",
    "    \n",
    "    # Get the inputBatch and outputBatch\n",
    "    inputBatch, outputBatch = getBatch('train')\n",
    "    # Forward the model\n",
    "    logits, loss = model(inputBatch, outputBatch)\n",
    "    # Setting the gradients to None\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward to calculate gradients\n",
    "    loss.backward()\n",
    "    # Update the gradients\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "And inside the `estimateLoss()` method we first set the model to `evaluation` mode and take both the `training` and `validation` splits and take the calculate the loss of batches after forwarding them for `evaluationIterations` storing the losses in a tensor, and then after the `evaluationIterations` are completed, we average out the losses using `mean()` based on the split and set the model back to the `training` mode and return both the `training` and `validation` losses. \n",
    "\n",
    "So we get a code like this:\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def estimateLoss():\n",
    "    output = {}\n",
    "    # Set the model to evalutaion mode\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['train','validation']:\n",
    "        # Define a losses tensor for the `evaluationIterations` size\n",
    "        losses = torch.zeros(evaluationIterations)\n",
    "        for evaluationIteration in range(evaluationIterations):\n",
    "            inputBatch, outputBatch = getBatch(split)\n",
    "            logits, loss = model(inputBatch, outputBatch)\n",
    "            losses[evaluationIteration] = loss.item()\n",
    "        output[split] = losses.mean()\n",
    "        \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d2261-a1ee-4b96-98b2-ff392bc3eaf3",
   "metadata": {},
   "source": [
    "So when we call `estimateLoss()` we are going to monitor pretty accurate `training` and `validation` losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637112e3-0133-4d08-b524-ee4cc1778b48",
   "metadata": {},
   "source": [
    "But right now `model.eval()` and `model.train()` does not actually do anything, but it will come in handy later when we have layers like `Batch Normalization` and `Dropout` layers in our model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de602c0-e4b6-41bc-bf7e-3516b6e27170",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Converting Bigram To a Script - `bigram_v1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c0e70-2a03-4012-8b3e-f0b376a952d7",
   "metadata": {},
   "source": [
    "So, I'd now like to convert our entire code that we have discussed so far into a Python Script, such that the entire code can now be run in a single file, out of the box, assuming you have PyTorch installed, such that we can simplify all the intermediate work that we did...\n",
    "\n",
    "As I have pointed out earlier, I will be completing parts of the discussion and will be releasing the code scripts within the same repository under the directory `GPT Scripts`.\n",
    "\n",
    "And to run each script you just have to specify the `<filename>.py` in the terminal...\n",
    "\n",
    "For now I have named this script as `bigram_v1.py`...\n",
    "\n",
    "And you can run the file using a command like:\n",
    "```bash\n",
    "python bigram_v1.py\n",
    "```\n",
    "\n",
    "That's it... And everything will run out of the box..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a55252-890c-44bd-9631-7d4bfff303b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Keeping track of losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e76d75-14f5-44a6-9dbc-76c830f0037a",
   "metadata": {},
   "source": [
    "We will also keep tracking losses from now, so that we can compare our models, as we continue to modify it in the future...\n",
    "\n",
    "For now we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251cbf7-4836-415d-991b-1a8d065776b3",
   "metadata": {},
   "source": [
    "Losses at `bigram_v1.py`:\n",
    "```python\n",
    "Step 29500: Training Loss 2.4385, Validation Loss 2.4322\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca79d3-b4a0-439e-9146-600bc9ab5f91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Self Attention** in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca99586-17e0-4f08-a536-c3e4ef03d681",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mathematical Trick for Self Attention in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46e099e-cd12-4f70-84b7-f4712f9d0f30",
   "metadata": {},
   "source": [
    "Before we discuss `Self Attention`, we'd like to discuss a certain problem that we are currently dealing with... And get used to different ways to solve the problem as well...\n",
    "\n",
    "The problem is, right now we are only focusing on the `last` token of the context (`last` token of the `blockSize`)... But we'd like our model to look further in the context history like this:\\\n",
    "![GPTContextProblem](ExplanationMedia/Images/GPTContextProblem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4402b2-8acf-4137-ab27-09617b8785e0",
   "metadata": {},
   "source": [
    "Which makes the kind of model we want to be \"**autoregressive**\" **(AR)**. The **autoregressive model** specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85799d16-f395-4f8c-a9d3-d1605ffe198b",
   "metadata": {},
   "source": [
    "Now what we'd like to do is take a small *toy-example* and to solve the same problem differently and work our way upto an efficient solution to the problem...\n",
    "\n",
    "Let's take a very small `input` tensor and try to write it out as an example code:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "batch, time, channel = 2, 8, 3\n",
    "inputs = torch.randn(batch, time, channel)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Shape of inputs:\", inputs.shape)\n",
    "```\n",
    "For which I get:\n",
    "```python\n",
    "Inputs: tensor([[[ 2.3787e+00, -3.5896e-01, -7.1692e-01],\n",
    "         [-2.4297e-01, -1.8038e-01,  1.4882e+00],\n",
    "         [ 5.4493e-01,  3.8243e-01,  8.7188e-01],\n",
    "         [-1.9890e+00, -5.4009e-01, -1.5319e+00],\n",
    "         [-9.2356e-01,  7.2013e-01, -5.9540e-01],\n",
    "         [-1.1697e+00,  8.3635e-01,  3.5811e-01],\n",
    "         [ 3.9933e-01, -1.3606e+00,  1.0168e-01],\n",
    "         [-4.8538e-02, -1.1643e+00, -1.5403e-01]],\n",
    "\n",
    "        [[ 1.1998e+00, -8.0983e-01,  1.0315e+00],\n",
    "         [ 1.6720e+00, -1.0681e+00, -9.6532e-01],\n",
    "         [ 3.6006e-01,  3.2209e-01,  5.2594e-01],\n",
    "         [-5.4021e-01, -5.2587e-01,  1.0481e+00],\n",
    "         [-3.8775e-01, -1.3751e+00, -1.0385e-01],\n",
    "         [-9.2093e-01, -1.0048e+00, -1.4028e+00],\n",
    "         [-2.0169e+00, -5.1192e-01, -2.1998e-01],\n",
    "         [-3.3050e-01, -9.1926e-01,  8.9532e-04]]])\n",
    "Shape of inputs: torch.Size([2, 8, 3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec55d10-4b53-40cc-bcb7-4dd21e9f2aad",
   "metadata": {},
   "source": [
    "In the *toy-example* we have a tensor `inputs` with three dimensions:\n",
    "1. `batch` → Number of batches (`batchSize`)\n",
    "2. `time` → Number of tokens (characters) in a block of `blockSize`\n",
    "3. `channel` → Information of the `token` in form of embeddings (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a33e0e-3df7-4cfe-852f-1312d4132013",
   "metadata": {},
   "source": [
    "In this example we have `8` tokens (character) in the `time` dimension, and these tokens are not *talking* to each other...\n",
    "\n",
    "And now we'd like them to *\"talk to each other\"*, we'd like to couple them... And we'd like to couple them in a very specific way...\n",
    "\n",
    "For example, because we have `8` tokens in a sequence, out of these `8` tokens, if we let's say consider the `5`th token... This `5`th token should not communicate with tokens at locations `6`, `7` and `8` (or the future tokens in the sequence), and they should talk to the tokens at locations `4`, `3`, `2` and `1` (or the previous tokens in the context), such that information only flows from previous context to the current time stamp and we cannot get any information from the future because we are about to predict the future..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3a97f-8c72-4ebc-a84b-4aaf1f7d93eb",
   "metadata": {},
   "source": [
    "So, what is the easiest way for tokens to communicate?\n",
    "\n",
    "Let's say I am the `5`th token, and I want to communicate with my past tokens (at `4`, `3`, `2` & `1`), and the simplest way to communate with the past is to just do an **average** of the past with context of my own information. Or in other words, if I am the `5`th token, I would like to take up the `channels` that make-up my information at my step, and also the `channels` in my past, and I'd like to average those up to make it like a `feature vector` that *summarizes me in the context of my history*...\n",
    "\n",
    "Now once again, doing just an average is just a very weak form of \"talking\" or interaction, and makes this communication extremely **lossy**, which makes us lose a lot of information about the *spatial arrangements* of all those tokens... But for now, that's okay, because in the future solutions to the same problem, we will see how we can get this information back...\n",
    "\n",
    "So let's see different versions to solve the problem now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aeb6b0-3f55-4d9c-ba72-590afb69b377",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Version 1 - Naïve Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ccf45d-d013-45ef-8541-bf465becf1d7",
   "metadata": {},
   "source": [
    "In this approach, what we'd like to do is, for our `inputs`, for every single `batch`, for every `token` we'd like to average out all the vectors in all the `previous tokens` including the `current token`.\n",
    "\n",
    "Or\n",
    "```python\n",
    "# We want: inputs[batch, time] = mean_{i<=token} inputs[batch, i]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9118c0f9-09ad-4a4f-926a-ea878c4b7eb7",
   "metadata": {},
   "source": [
    "Now before we dive into the solution, I'd like to discuss a concept called `Bag Of Words`...\n",
    "\n",
    "So what is `Bag of Words`?\n",
    "\n",
    "\n",
    "![Bag Of Words](https://miro.medium.com/v2/resize:fit:720/format:webp/1*3K9GIOVLNu0cRvQap_KaRg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ace40-fdc5-4a8d-8494-f9c2e9c2ae33",
   "metadata": {},
   "source": [
    "The **bag-of-words model** is a model of text which uses a representation of text that is based on an unordered collection (or *\"bag\"*) of words.\n",
    "In natural language processing (NLP), the term \"bag of words\" `(BoW)` typically refers to a simple representation of text where the frequency of each word in a document is counted and represented in a vector.\n",
    "\n",
    "So why mention it?\n",
    "\n",
    "Well you see, our concept is similar but for our case, we have a character level model. \n",
    "The term \"bag of words\" `(BoW)` is used in contexts where we want to represent text data by counting the occurrences and then potentially averaging them out.\n",
    "\n",
    "And we would like to use it in our *toy-example*'s output variable name... 😂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b1f0c-24f7-446a-9188-58086226bf93",
   "metadata": {},
   "source": [
    "So now, let's write out what we want, in the for of code... To get the idea more clear now:\n",
    "```python\n",
    "# Allocating memory for output\n",
    "inputBagOfWords = torch.zeros((batch, time, channel))\n",
    "\n",
    "for b in range(batch):\n",
    "    for t in range(time):\n",
    "        # Every token in the past and current token in the batch\n",
    "        previousInput = inputs[b, :t+1] # (B, T, C) → (T, C)\n",
    "        # Mean over the token or the time dimension\n",
    "        inputBagOfWords[b, t] = torch.mean(previousInput, 0) # (B, T, C)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f683f7-c59a-4090-8ff8-19b774e407b7",
   "metadata": {},
   "source": [
    "Now because we have multiple `batches` in our example, let's compare only one batch (let's say we compare the first batch or `0`-th index) of both `inputs` and `inputBagOfWords` like this:\n",
    "```python\n",
    "print(\"Input BoW:\", inputBagOfWords[0])\n",
    "print(\"Inputs:\", inputs[0])\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "Input BoW: tensor([[ 2.3787, -0.3590, -0.7169],\n",
    "        [ 1.0679, -0.2697,  0.3856],\n",
    "        [ 0.8936, -0.0523,  0.5477],\n",
    "        [ 0.1729, -0.1742,  0.0278],\n",
    "        [-0.0464,  0.0046, -0.0968],\n",
    "        [-0.2336,  0.1432, -0.0210],\n",
    "        [-0.1432, -0.0716, -0.0035],\n",
    "        [-0.1313, -0.2082, -0.0223]])\n",
    "Inputs: tensor([[ 2.3787, -0.3590, -0.7169],\n",
    "        [-0.2430, -0.1804,  1.4882],\n",
    "        [ 0.5449,  0.3824,  0.8719],\n",
    "        [-1.9890, -0.5401, -1.5319],\n",
    "        [-0.9236,  0.7201, -0.5954],\n",
    "        [-1.1697,  0.8363,  0.3581],\n",
    "        [ 0.3993, -1.3606,  0.1017],\n",
    "        [-0.0485, -1.1643, -0.1540]])\n",
    "```\n",
    "\n",
    "And we see that we have, at every `token` or `time` dimension, the average of previous and current token, which is what we want..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91c059-9fc1-4296-b2d9-abc80ad64a92",
   "metadata": {},
   "source": [
    "But in this process we see that we use nested `for-loops` which is extremely inefficient...\n",
    "\n",
    "And now next, what we will see is that we can be extremely efficient with the same problem with `Matrix Multiplication`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e1b37-c9f8-401a-a8c3-4295784f36df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Version 2 - Matrix Multiplication Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe51ec-8e98-43e2-8426-76b16d1124c0",
   "metadata": {},
   "source": [
    "To understand the matrix multiplication approach we will use another *toy-example* for our *toy-example*...\n",
    "\n",
    "Suppose we have two matrices `a` and `b` of sizes `(3, 3)` and `(3, 2)` respectively, and we understand that the resultant matrix `c` will be of shape `(3, 2)` and will be the dot product of columns and rows of the two matrices.\n",
    "\n",
    "For example,\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1\\ 1\\ 1\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9\\ 4\\\\\n",
    "3\\ 9\\\\\n",
    "4\\ 8\\\\\n",
    "\\end{bmatrix}}_{\\text{b}}\n",
    "\\text{=}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "16\\ 21\\\\\n",
    "16\\ 21\\\\\n",
    "16\\ 21\\\\\n",
    "\\end{bmatrix}}_{\\text{c}}\n",
    "\\rightarrow \\text{c = a @ b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4a5e0-b698-4071-8882-2083b3a9aaa5",
   "metadata": {},
   "source": [
    "Let's try to write the same example with code:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(low=0, high=10, size=(3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"-----\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1de2c-f1fb-44ab-b080-6adf4e2de8b5",
   "metadata": {},
   "source": [
    "Right now we have a very *boring* matrices `a` of just `1`s where it represents `weights` like a linear layer, and `b` represents the `inputs` similarly.\n",
    "\n",
    "And we have repeating elements because we are calculating the same `columns` of `b` with every `row` of `1`s in `a` for each item in `c`...\n",
    "\n",
    "Now instead if we take a lower triangluar matrix of `1`s and keep all the other elements as `0`s for `a`, we have something like this:\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1\\ 0\\ 0\\\\\n",
    "1\\ 1\\ 0\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71085ca-b665-41ef-b8c1-6c45c4da12fa",
   "metadata": {},
   "source": [
    "To do this, we have a method <a href=\"https://pytorch.org/docs/stable/generated/torch.tril.html\">torch.tril()</a> in PyTorch...\n",
    "\n",
    "So we can now modify our code and look at the resulting matrices:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(low=0, high=10, size=(3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"-----\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "```\n",
    "\n",
    "Which gives us something like this:\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1\\ 0\\ 0\\\\\n",
    "1\\ 1\\ 0\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9\\ 4\\\\\n",
    "3\\ 9\\\\\n",
    "4\\ 8\\\\\n",
    "\\end{bmatrix}}_{\\text{b}}\n",
    "\\text{=}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9\\ 4\\\\\n",
    "12\\ 13\\\\\n",
    "16\\ 21\\\\\n",
    "\\end{bmatrix}}_{\\text{c}}\n",
    "\\rightarrow \\text{c = a @ b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa433f3-f016-43d0-9837-9091cf768b9c",
   "metadata": {},
   "source": [
    "See how because of these `0`s the resultant matrix `c` is just a result of an **incremental addition (`sum`) of their respective `columns`**?\n",
    "\n",
    "And in the same fashion, because we all know average(`mean`) is just the addition of all the elements divided by the number of elements, you can start to see how the average(`mean`) would come into the picture now...\n",
    "\n",
    "So because we are dealing with the `weights`(`a`) and trying to manipulate them, and during matrix multiplication in `a` the `rows` play the role, so we can now average(`mean`) them using `normalization` of individual elements, where every `row` sums to one, to get and **incremental average (`mean`) of their respective `columns` in the resultant `c` matrix**...\n",
    "\n",
    "So our code now looks like:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(low=0, high=10, size=(3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"-----\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "```\n",
    "Which gives us:\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1.0000\\ 0.0000\\ 0.0000\\\\\n",
    "0.5000\\ 0.5000\\ 0.0000\\\\\n",
    "0.3333\\ 0.3333\\ 0.3333\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9\\ 4\\\\\n",
    "3\\ 9\\\\\n",
    "4\\ 8\\\\\n",
    "\\end{bmatrix}}_{\\text{b}}\n",
    "\\text{=}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9.0000 \\ 4.0000\\\\\n",
    "6.0000 \\ 6.5000\\\\\n",
    "5.3333 \\ 7.0000\\\\\n",
    "\\end{bmatrix}}_{\\text{c}}\n",
    "\\rightarrow \\text{c = a @ b}\n",
    "$$\n",
    "\n",
    "And this is exactly similar to our original **naïve** approach that we did...\n",
    "\n",
    "Now let's go back to our original *toy-example* and implement this now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbb3ea-7aa9-4b56-9624-defd67792545",
   "metadata": {},
   "source": [
    "Remember how we considered `a` to be `weights` and `b` to be `inputs`...\n",
    "\n",
    "Let's first initialize the `weights` and `inputs` the same way we did before to perform a `Matrix Multiplication` now and think how the `broadcasting` works out for us because we have `batch` dimensions as well...\n",
    "\n",
    "For now we are dealing with the `tokens` or the `time` dimension, so our weights matrix would be of shape `time` by `time`...\n",
    "\n",
    "And we will take `inputBagOfWords` and rename it to `inputBagOfWordsV2` where `V2` represents **version** `2` just so we can compare them later...\n",
    "\n",
    "So our code looks something like this:\n",
    "```python\n",
    "weights = torch.tril(torch.ones(time, time))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "inputBagOfWordsV2 = weights @ inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a7942-6459-454c-95c1-e69d315e32c9",
   "metadata": {},
   "source": [
    "Now let's think through the **broadcasting** of the matrix multiplication operation...\n",
    "\n",
    "For now we have `weights` of two dimensions of size `(8, 8)` or `(T, T)` for our *toy-example* and `inputs` of three dimensions of size `(2, 8, 3)` or `(B, T, C)`...\n",
    "\n",
    "Now during broadcasting, the `weights` will add another `batch` dimension to make the broadcasting work from `(T, T)` to `(B, T, T)` and then perform the multiplication...\n",
    "\n",
    "Which ultimately results in a **batched matrix multiplication** where the multiplication will be applied to all the **batch elements in parallel** and individually. And for each **batch element** there will be a mutliplication between `(T, T)` and `(T, C)` exactly like the operation we discussed earlier...\n",
    "\n",
    "And the resultant `tensor` would be of shape `(B, T, C)` which will make `inputBagOfWords` completely identical to `inputBagOfWordsV2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e3c76-9232-42ef-961d-23378b8eb883",
   "metadata": {},
   "source": [
    "So we can now compare both the the `inputBagOfWords` and `inputBagOfWords2` with <a href=\"https://pytorch.org/docs/stable/generated/torch.allclose.html\">torch.allclose()</a> like this:\n",
    "```python\n",
    "torch.allclose(inputBagOfWords, inputBagOfWordsV2)\n",
    "```\n",
    "for which we get:\n",
    "```python\n",
    "True\n",
    "```\n",
    "\n",
    "And if we want to compare them manually, because both of them are long `tensor`s we can compare the first batch like them to see that they are completely similar like this:\n",
    "```python\n",
    "print(\"First batch of inputBagOfWords:\", inputBagOfWords[0]) \n",
    "print(\"First batch of inputBagOfWordsV2:\", inputBagOfWordsV2[0])\n",
    "```\n",
    "for which we get:\n",
    "```python\n",
    "First batch of inputBagOfWords: tensor([[ 2.3787, -0.3590, -0.7169],\r\n",
    "        [ 1.0679, -0.2697,  0.3856],\r\n",
    "        [ 0.8936, -0.0523,  0.5477],\r\n",
    "        [ 0.1729, -0.1742,  0.0278],\r\n",
    "        [-0.0464,  0.0046, -0.0968],\r\n",
    "        [-0.2336,  0.1432, -0.0210],\r\n",
    "        [-0.1432, -0.0716, -0.0035],\r\n",
    "        [-0.1313, -0.2082, -0.0223]])\r\n",
    "First batch of inputBagOfWordsV2: tensor([[ 2.3787, -0.3590, -0.7169],\r\n",
    "        [ 1.0679, -0.2697,  0.3856],\r\n",
    "        [ 0.8936, -0.0523,  0.5477],\r\n",
    "        [ 0.1729, -0.1742,  0.0278],\r\n",
    "        [-0.0464,  0.0046, -0.0968],\r\n",
    "        [-0.2336,  0.1432, -0.0210],\r\n",
    "        [-0.1432, -0.0716, -0.0035],\r\n",
    "        [-0.1313, -0.2082, -0.0223]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29237a-4a68-4191-bdf9-7e1c183a2dad",
   "metadata": {},
   "source": [
    "So let's conclude what we saw here...\n",
    "\n",
    "We saw that, **we can do weighted aggregation of our past `tokens` or `characters` by using `Matrix Multiplication` of `weights` and the `inputs`, where `weights` are a matrix of lower-triangular fashion of `1`s and other elements as `0`s, and we are doing weighted `sum` and `normalizing` them to get the *rolling* `average` or `mean`**...\n",
    "\n",
    "Now we will look at another way of doing this same exact operation using `softmax`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefbbaa8-6944-499b-969c-036e4a01e042",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Version 3 - Adding Softmax Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b96bf0-6db2-4f49-a27c-20bdc98ec6ec",
   "metadata": {},
   "source": [
    "This time, I will explain what we are doing after I implement the same thing with softmax code for our *toy-example* of our *toy-example*...\n",
    "\n",
    "So let's see what our new approach looks like:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "lowerTrianguarMatrix = torch.tril(torch.ones(3, 3))\n",
    "print(\"lowerTrianguarMatrix=\")\n",
    "print(lowerTrianguarMatrix)\n",
    "print(\"-----\")\n",
    "a = torch.zeros(3, 3)\n",
    "print(\"a (at initialization)=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "a = a.masked_fill(lowerTrianguarMatrix == 0, float('-inf'))\n",
    "print(\"a (after masking)=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "a = F.softmax(a, dim=-1)\n",
    "b = torch.randint(low=0, high=10, size=(3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a (after softmax)=\")\n",
    "print(a)\n",
    "print(\"-----\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"-----\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "```\n",
    "\n",
    "Here you see that I am printing the transformation of `a` or essentially `weights` \n",
    "\n",
    "For which we get something like this:\n",
    "```python\n",
    "lowerTrianguarMatrix=\n",
    "tensor([[1., 0., 0.],\n",
    "        [1., 1., 0.],\n",
    "        [1., 1., 1.]])\n",
    "\"-----\"\n",
    "a (at initialization)=\n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.]])\n",
    "\"-----\"\n",
    "a (after masking)=\n",
    "tensor([[0., -inf, -inf],\n",
    "        [0., 0., -inf],\n",
    "        [0., 0., 0.]])\n",
    "\"-----\"\n",
    "a (after softmax)=\n",
    "tensor([[1.0000, 0.0000, 0.0000],\n",
    "        [0.5000, 0.5000, 0.0000],\n",
    "        [0.3333, 0.3333, 0.3333]])\n",
    "\"-----\"\n",
    "b=\n",
    "tensor([[9., 4.],\n",
    "        [3., 9.],\n",
    "        [4., 8.]])\n",
    "\"-----\"\n",
    "c=\n",
    "tensor([[9.0000, 4.0000],\n",
    "        [6.0000, 6.5000],\n",
    "        [5.3333, 7.0000]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444203b-5a95-44a0-ad1d-e749f7543304",
   "metadata": {},
   "source": [
    "Now let me explain what is happening in this transformation...\n",
    "\n",
    "We see that instead of initializing the `a` matrix as `1`s and taking the lower triangular version of that, we initialize a `lowerTrianguarMatrix` with the same dimensions (`time`). And we initialize the `a` matrix of `0`s of the same dimension (`time`) to allocate memory for our further operation...\n",
    "\n",
    "Then upon using this line:\n",
    "```python\n",
    "a = a.masked_fill(lowerTrianguarMatrix == 0, float('-inf'))\n",
    "```\n",
    "We are essentially telling that, for all the elements in `lowerTrianguarMatrix` is `0`, make `a` fill themselves with `-inf`...\n",
    "\n",
    "Which looks something like this:\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1\\ 0\\ 0\\\\\n",
    "1\\ 1\\ 0\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "\\end{bmatrix}}_{\\text{lowerTrianguarMatrix}}\n",
    "\\xrightarrow[\\text{Masked Fill}]{\\text{ element == 0}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "0\\ -\\infty\\ -\\infty\\\\\n",
    "0\\ \\ \\ \\ \\ \\ \\ \\ 0\\ -\\infty\\\\\n",
    "0\\ \\ \\ \\ \\ \\ \\ \\ 0\\ \\ \\ \\ \\ \\ \\ \\ 0\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a98b8-d206-434c-aaf3-09365d25300a",
   "metadata": {},
   "source": [
    "This `a` then goes through a `softmax()`, which is a non-linearity function that normalizes the values to sum to `1`(by exponentiating the values and dividing by the sum of the dimension specified)...\n",
    "\n",
    "And we can recall that:\n",
    "1. $e^0 = 1$\n",
    "2. $e^{-\\infty} = 0$\n",
    "\n",
    "And we want to normalize the `rows` of the `weights` which is also the `last` dimension of the matrix `a`, so we can now pass the dimension of normalization to `softmax()` as `-1` like this (we are also doing this because later we are going to deal with `batches` which is an extra dimension):\n",
    "```python\n",
    "a = F.softmax(a, dim=-1)\n",
    "```\n",
    "Which looks ultimately looks like:\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "0\\ -\\infty\\ -\\infty\\\\\n",
    "0\\ \\ \\ \\ \\ \\ \\ \\ 0\\ -\\infty\\\\\n",
    "0\\ \\ \\ \\ \\ \\ \\ \\ 0\\ \\ \\ \\ \\ \\ \\ \\ 0\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "\\underbrace{\\overrightarrow{\\text{exp()}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1\\ 0\\ 0\\\\\n",
    "1\\ 1\\ 0\\\\\n",
    "1\\ 1\\ 1\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "\\xrightarrow[\\text{Divide}]{\\text{dimension = -1}}}_{\\text{Softmax}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1.0000\\ 0.0000\\ 0.0000\\\\\n",
    "0.5000\\ 0.5000\\ 0.0000\\\\\n",
    "0.3333\\ 0.3333\\ 0.3333\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1112f38-fbb7-43ce-a602-4dc0e95f2db9",
   "metadata": {},
   "source": [
    "And then we can continue our regular `Matrix Multiplication` operation as discussed in the above version...\n",
    "\n",
    "Which we can recall, looks like this:\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "1.0000\\ 0.0000\\ 0.0000\\\\\n",
    "0.5000\\ 0.5000\\ 0.0000\\\\\n",
    "0.3333\\ 0.3333\\ 0.3333\\\\\n",
    "\\end{bmatrix}}_{\\text{a}}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9\\ 4\\\\\n",
    "3\\ 9\\\\\n",
    "4\\ 8\\\\\n",
    "\\end{bmatrix}}_{\\text{b}}\n",
    "\\text{=}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "9.0000 \\ 4.0000\\\\\n",
    "6.0000 \\ 6.5000\\\\\n",
    "5.3333 \\ 7.0000\\\\\n",
    "\\end{bmatrix}}_{\\text{c}}\n",
    "\\rightarrow \\text{c = a @ b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e868b35-3c04-4b55-975b-31276cb6788e",
   "metadata": {},
   "source": [
    "Now, let's try to write the same for our original *toy-example* in code after renaming this output as `inputBagOfWordsV3` to compare them later like we did before:\n",
    "```python\n",
    "lowerTrianguarMatrix = torch.tril(torch.ones(time, time))\n",
    "weights = torch.zeros(time, time)\n",
    "weights = weights.masked_fill(lowerTrianguarMatrix == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "inputBagOfWordsV3 = weights @ inputs\n",
    "```\n",
    "So we can now compare both the the `inputBagOfWords` and `inputBagOfWords3` with <a href=\"https://pytorch.org/docs/stable/generated/torch.allclose.html\">torch.allclose()</a> like this:\n",
    "```python\n",
    "torch.allclose(inputBagOfWords, inputBagOfWordsV3)\n",
    "```\n",
    "for which we get:\n",
    "```python\n",
    "True\n",
    "```\n",
    "\n",
    "Again we can compare both the tensors manually like this:\n",
    "```python\n",
    "print(\"First batch of inputBagOfWords:\", inputBagOfWords[0]) \n",
    "print(\"First batch of inputBagOfWordsV3:\", inputBagOfWordsV3[0])\n",
    "```\n",
    "For which we get:\n",
    "```python\n",
    "First batch of inputBagOfWords: tensor([[ 2.3787, -0.3590, -0.7169],\r\n",
    "        [ 1.0679, -0.2697,  0.3856],\r\n",
    "        [ 0.8936, -0.0523,  0.5477],\r\n",
    "        [ 0.1729, -0.1742,  0.0278],\r\n",
    "        [-0.0464,  0.0046, -0.0968],\r\n",
    "        [-0.2336,  0.1432, -0.0210],\r\n",
    "        [-0.1432, -0.0716, -0.0035],\r\n",
    "        [-0.1313, -0.2082, -0.0223]])\r\n",
    "First batch of inputBagOfWordsV3: tensor([[ 2.3787, -0.3590, -0.7169],\r\n",
    "        [ 1.0679, -0.2697,  0.3856],\r\n",
    "        [ 0.8936, -0.0523,  0.5477],\r\n",
    "        [ 0.1729, -0.1742,  0.0278],\r\n",
    "        [-0.0464,  0.0046, -0.0968],\r\n",
    "        [-0.2336,  0.1432, -0.0210],\r\n",
    "        [-0.1432, -0.0716, -0.0035],\r\n",
    "        [-0.1313, -0.2082, -0.0223]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed89b27-d70f-402c-9b5e-3e40b210d33f",
   "metadata": {},
   "source": [
    "Now, the reason that this is a bit more interesting, and also the reason that we will end up using the `softmax()` version in our `Self Attention` version is because, these `weights` begin with `0`, which you can think of as an *interaction strength* or **affinities** in this line:\n",
    "```python\n",
    "weights = torch.zeros(time, time)\n",
    "```\n",
    "Which essentially tell us, how much of each `token` from the past do we want to `aggregate` and `average` up...\n",
    "\n",
    "Next up the line:\n",
    "```python\n",
    "weights = weights.masked_fill(lowerTrianguarMatrix == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "```\n",
    "Tells us that the tokens from the **future** cannot *communicate* with each other (we will not `aggregate` anything from those `tokens`)\n",
    "\n",
    "And finally `aggregation` happens through the `Matrx Multiplication` in this line:\n",
    "```python\n",
    "inputBagOfWordsV3 = weights @ inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d904e-039f-480e-8f51-274471df3939",
   "metadata": {},
   "source": [
    "So long story short...\n",
    "\n",
    "**We can do weighted aggregations of our past elements, by using matrix multiplication in a lower triangular fashion and the elements in the lower triangular part tells us, how much of each element *fuses* into the current position**...\n",
    "\n",
    "Which we will use now in our `Self Attention` block... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc9bc7-2ed4-49b8-b3e0-bb9ae2d8a0a2",
   "metadata": {},
   "source": [
    "Now the point is, these `weights` are currently set by us to be `0`s, but these **affinities** are not going to be a constant at `0` in the `Self Attention`, but rather they are going to be **data-dependent**, and these `tokens` are going to start looking at each other and some `tokens` will find other `tokens`, more or less *interesting*, and depending on what their values are they're going to find each other *interesting* to different amounts (**affinities**)...\n",
    "\n",
    "So let us look at our approach towards `Self Attention`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf92e0-bde3-40a9-b62c-0d3af86f7336",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cleaning Up Bigram Script - `bigram_v2.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba60be-ac8f-469c-9354-d536227da846",
   "metadata": {},
   "source": [
    "Before we move on to `Self Attention` we need to clean up our `bigram_v1.py`...\n",
    "\n",
    "And there are some changes that we will discuss in this section...\n",
    "\n",
    "I will keep the original `bigram_v1.py` unchanged, and create a new script as `bigram_v2.py` so that you'd be able to see the changes...\n",
    "\n",
    "And we will also be able to run the script at each change as well...\n",
    "\n",
    "So let's discuss the changes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa7b16-998f-4ac1-a066-fa0f4e818a97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Change - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca12921-fdf2-4fc0-8bba-6346e29bb047",
   "metadata": {},
   "source": [
    "We see that inside the script we are already having the `vocabularySize` as a **global variable**,\n",
    "\n",
    "But we are still using a redundant `vocabularySize` as a parameter during constructor definition and object initialization...\n",
    "\n",
    "So we will remove the redundant memory allocation... So our code becomes:\n",
    "1. `def __init__(self, vocabularySize)` → `def __init__(self)`\n",
    "2. `model = BigramLanguageModel(vocabularySize).to(device=device)` → `model = BigramLanguageModel().to(device=device)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c9308-5942-4d63-9042-8a9b1946d3a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Change - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd67d402-cc14-4c9c-808d-36b35dada25b",
   "metadata": {},
   "source": [
    "We are going to start our neural network bigger now, so I will create a level of **indirection** here, just to introduce you to a new concept and the idea of how we can enlarge our model by including new items...\n",
    "\n",
    "We are going to introduce a new **hyper-parameter** called `numberOfEmbeddingDimensions`, which explains what it represents within it's name...\n",
    "\n",
    "For now I am going to set this **hyper-parameter** to `32` and we will change this later when we try to scale up our model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54457d9-d170-41c8-8277-8789ded144df",
   "metadata": {},
   "source": [
    "Now we want our `Embeddings` to go through a simple `Linear` layer now (as a level of **indirection**) just to understand how we can add new layers our existing model and use **hyper-parameters** in our model...\n",
    "\n",
    "To do this we need to change our code in the **class definition** of `BigramLanguageModel`:\n",
    "1. During constructor definition\n",
    "2. During forward-pass definition\n",
    "\n",
    "**During constructor definition**, we have the following code now (old):\n",
    "```python\n",
    "# Constructor for the model\n",
    "def __init__(self):\n",
    "    # Initializing the embedding table\n",
    "    super().__init__()\n",
    "    self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, vocabularySize)\n",
    "```\n",
    "Now we already understand that the `fan-in` is `vocabularySize` and the `fan-out` is also `vocabularySize`...\n",
    "\n",
    "To use this **hyper-parameter** `numberOfEmbeddingDimensions` now, we can add a `Linear` layer using `torch.nn.Linear` and change the code in following steps:\n",
    "1. changing the `fan-out` of the `Embedding` layer to be `numberOfEmbeddingDimensions`\n",
    "2. changing the `fan-in` of the `Linear` layer to be `numberOfEmbeddingDimensions`\n",
    "Which makes `numberOfEmbeddingDimensions` the intermediate **hyper-parameter** which does not affect the original `fan-in` and `fan-out`...\n",
    "\n",
    "And for a note, we will name our `Linear` layer to be `languageModelingHead`...\n",
    "\n",
    "In the context of `Large Language Models (LLMs)` like `GPT-3` or `BERT`, the term **\"head\"** refers to the additional layers or mechanisms added on top of the pre-trained base model to adapt it for specific tasks.\n",
    "\n",
    "So now our code becomes:\n",
    "```python\n",
    "# Constructor for the model\n",
    "def __init__(self):\n",
    "    # Initializing the embedding table\n",
    "    super().__init__()\n",
    "    self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, numberOfEmbeddingDimensions)\n",
    "    self.languageModelingHead = torch.nn.Linear(numberOfEmbeddingDimensions, vocabularySize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8126b-89bb-444a-b228-61cbff6b41f0",
   "metadata": {},
   "source": [
    "**During forward-pass definition** we have the following code:\n",
    "```python\n",
    "# Index into embeddings to get the logits\n",
    "logits = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "```\n",
    "\n",
    "To adapt with our current objective, we need to understand that we do not get the `logits` directly now, so we can now rename our `logits` to something like `tokenEmbeddings`...\n",
    "\n",
    "And then we can pass our `tokenEmbeddings` through our `languageModelingHead` in the next line to get the `logits`...\n",
    "\n",
    "So our code looks like this:\n",
    "```python\n",
    "# Index into embeddings to get the token embeddings\n",
    "tokenEmbeddings = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "# Pass the token embeddings through a linear layer\n",
    "logits = self.languageModelingHead(tokenEmbeddings) # (B, T, C)\n",
    "```\n",
    "\n",
    "We also need to keep in mind that the `channels` dimension in `tokenEmbeddings` are different from `logits`, because in `tokenEmbeddings` the `channel` dimension represents the **number of embedding dimensions** which for now is `32` but, in `logits` the `channel` dimension represents the output **vocabulary size** which is for our case `92`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f1ce5-c0c6-494b-b652-748c23164543",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Change - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e523f8-3f19-4745-8f16-ff0126bd633a",
   "metadata": {},
   "source": [
    "We have seen how we take the `indeces` and we have encoded them based on the *identity* of the `tokens`...\n",
    "\n",
    "But, In `Transformer` architecture, **positional encodings** are crucial for incorporating sequential information into the model's understanding of the input data. Unlike `Recurrent Neural Networks (RNNs)` or `Convolutional Neural Networks (CNNs)`, `Transformers` don't inherently understand the **order** of the input tokens since they process all tokens in parallel. `Positional Encodings` address this limitation by providing the model with information about the **position of each token in the sequence**.\n",
    "\n",
    "Positional Encodings are typically **added** to the input embeddings before feeding them into the transformer layers.\n",
    "\n",
    "Which means that we want to add another `Embedding` layer to the computation now, in order to be ready to implement our `Transformer` architecture...\n",
    "\n",
    "Which also hints us that we will make changes in the **class definition** of `BigramLanguageModel`:\n",
    "1. During constructor definition\n",
    "2. During forward-pass definition\n",
    "\n",
    "**During constructor definition**, we have the following code now from the above section:\n",
    "```python\n",
    "# Constructor for the model\n",
    "def __init__(self):\n",
    "    # Initializing the embedding table\n",
    "    super().__init__()\n",
    "    self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, numberOfEmbeddingDimensions)\n",
    "    self.languageModelingHead = torch.nn.Linear(numberOfEmbeddingDimensions, vocabularySize)\n",
    "```\n",
    "\n",
    "Now we can define another table called `positionalEmbeddingTable` and initialize it with `torch.nn.Embedding()`...\n",
    "\n",
    "Now we need to understand that we are trying to encode the positions of `tokens` at each **block**, which means that the `fan-in` of this layer is going to be of size `blockSize` and because we will try to feed this to the `Linear` layer after **adding or joining** the `tokenEmbeddingTable` and `positionalEmbeddingTable` together, we want our `fan-out` of this layer to be of shape `numberOfEmbeddingDimensions` as well, in order for the boardcasting to work out...\n",
    "\n",
    "So, **During constructor definition**, we have the following code now:\n",
    "```python\n",
    "# Constructor for the model\n",
    "def __init__(self):\n",
    "    # Initializing the embedding table\n",
    "    super().__init__()\n",
    "    self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, numberOfEmbeddingDimensions)\n",
    "    self.positionalEmbeddingTable = torch.nn.Embedding(blockSize, numberOfEmbeddingDimensions)\n",
    "    self.languageModelingHead = torch.nn.Linear(numberOfEmbeddingDimensions, vocabularySize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5554ec-1a96-4fdf-9fcc-636be7cb70f8",
   "metadata": {},
   "source": [
    "**During forward-pass definition** we have the following code:\n",
    "```python\n",
    "# Forward Pass\n",
    "def forward(self, indeces, labels=None):\n",
    "    # Index into embeddings to get the token embeddings\n",
    "    tokenEmbeddings = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "    # Pass the token embeddings through a linear layer\n",
    "    logits = self.languageModelingHead(tokenEmbeddings) # (B, T, C)\n",
    "```\n",
    "\n",
    "Now we remember that within the `indeces` we receive the inputs in the shape of `(B, T)` or `(batchSize, blockSize)`, and in order to encode the positions of the `tokens` we need the `blockSize` of the `indeces`, so we can unpack the `blockSize` or the `time` dimension of these `indeces` using `torch.shape`... Then we can arrange them in a single tensor from `0` to `blockSize - 1` (or `0` to `T-1`) using `torch.arange()`, which can be our input now for the `positionalEmbeddingTable` to create a positional embedded tensor of `(blockSize, numberOfEmbeddingDimensions)` or `(T, C)` in an output we will call `positionalEmbeddings`\n",
    "\n",
    "Now that we have both `positionalEmbeddings` and `tokenEmbeddings`, we can **add** them up into a single tensor we will call, `concatenatedEmbeddings` (internally the broadcasting will work itself out as we have seen for addition like `(B, T, C) + (T, C)`)...\n",
    "\n",
    "Also, we are using a `GPU` right now to train the model, so we will also set the `device` parameter when we use the `torch.arange()`...\n",
    "\n",
    "So now we have the following code:\n",
    "```python\n",
    "def forward(self, indeces, labels=None):\n",
    "    # Unpacking the shape of indeces\n",
    "    batch, time = indeces.shape\n",
    "\n",
    "    # Index into embeddings to get the token embeddings\n",
    "    tokenEmbeddings = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "    # Index into embeddings to get the positional embeddings\n",
    "    positionalEmbeddings = self.positionalEmbeddingTable(torch.arange(time, device=device)) # (T, C)\n",
    "    # Fuse the token embeddings and positional embeddings together to pack the information in a single tensor\n",
    "    concatenatedEmbeddings = tokenEmbeddings + positionalEmbeddings # (B, T, C)\n",
    "    # Pass the concatenated embeddings through a linear layer\n",
    "    logits = self.languageModelingHead(concatenatedEmbeddings) # (B, T, C)\n",
    "```\n",
    "\n",
    "That's it, I have modified all the changes in the `bigram_v2.py` and I have release the script as well...\n",
    "\n",
    "And right now these `concatenatedEmbeddings` don't have any use as of now in our `BigramLanguageModel`, but as we start to work in our `Self Attention` block we will see how it starts to matter...\n",
    "\n",
    "So let's now finally move on to `Self Attention`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8ed8e-f907-4fe7-851c-4b9ad60b85af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Change - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b8cfd-5803-4884-a2e9-ad09003ffc3d",
   "metadata": {},
   "source": [
    "Now we also have to make sure that during `generate()`, the `indeces` that we pass in, because we are using `positionalEmbeddings` along with `tokenEmbeddings`, the model meight train itself fine, but during the generation, we can never have more context than `blockSize` coming into the model, because if our `indeces` are more than `blockSize`, then our `positionalEmbeddingTable` is going to run out of indexing scope because it only has embeddings for upto `blockSize` (or the `fan-in`)...\n",
    "\n",
    "So we can crop the `indeces` like this:\n",
    "```python\n",
    "croppedIndices = indices[:, -blockSize:]\n",
    "```\n",
    "Which selects all rows of the `indices` tensor and selects only the last `blockSize` columns. The negative index `-blockSize:` indicates that we are selecting from the `blockSize`-th column from the end to the last column. So, `croppedIndices` will contain the last `blockSize` indices from each sequence in the batch. This ensures that during generation, only the **most recent context** of size `blockSize` is considered for the next `token` prediction.\n",
    "\n",
    "So now our `generate()` inside of `BigramLanguageModel` now looks like:\n",
    "```python\n",
    "# Generation\n",
    "def generate(self, indeces, maximumNewTokens):\n",
    "    for _ in range(maximumNewTokens):\n",
    "        # Crop the indeces upto most recent block size context\n",
    "        croppedIndeces = indeces[:, -blockSize:]\n",
    "        # Forward Through Model\n",
    "        logits, loss = self(croppedIndeces)\n",
    "        # Focus on the last time step\n",
    "        logits = logits[:, -1, :]\n",
    "        # Applying softmax for the last dimension\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        # Sample from distribution\n",
    "        nextIndex = torch.multinomial(probabilities, num_samples=1)\n",
    "        # Concatenate currentIndex with nextIndex\n",
    "        indeces = torch.cat((indeces, nextIndex), dim=1)\n",
    "    return indeces\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b209028-7f80-4929-811e-c52679985491",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Keeping track of losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6003f42-41ed-44f3-bfcf-5104bc22aa08",
   "metadata": {},
   "source": [
    "Losses at `bigram_v1.py`:\n",
    "```python\n",
    "Step 29500: Training Loss 2.4385, Validation Loss 2.4322\n",
    "```\n",
    "Losses at `bigram_v2.py`:\n",
    "```python\n",
    "Step 29500: Training Loss 2.4641, Validation Loss 2.4487\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff58c89-0370-48b4-89a1-4c95f2932235",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Version 4 - Crux of **Self Attention** - Most important part of the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dee9a8-0c03-437a-995b-e4f4d8d64db2",
   "metadata": {},
   "source": [
    "Before I define `Self Attention`, let's pickup our *toy-example* from before and discuss where we left off...\n",
    "\n",
    "So had something like this:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "batch, time, channel = 2, 8, 3\n",
    "\n",
    "inputs = torch.randn(batch, time, channel)\n",
    "\n",
    "lowerTrianguarMatrix = torch.tril(torch.ones(time, time))\n",
    "weights = torch.zeros(time, time)\n",
    "weights = weights.masked_fill(lowerTrianguarMatrix == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "inputBagOfWords = weights @ inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7ead4-14aa-424c-a0d6-e890e2ab8966",
   "metadata": {},
   "source": [
    "Here we had `inputs` of `2 batches` of `8 tokens` with `3 dimensional embeddings`... And in this code, we do a simple **average** of all the past `tokens` and the current `token`. And it does so by creating a lower triangular structure, which allows us to *mask-out* the `weights` matrix that we created. After *masking-out* we **normalize** it and multiply it with our `inputs`...\n",
    "\n",
    "And, when we initialize the **affinities** between all the different `tokens`, we see that we have `weights` where every single **row** gives us these uniform numbers that sum to `1`.\n",
    "\n",
    "But the thing now is, we don't want these numbers in `weights` to be all uniform, because we want different `tokens` to find different other `tokens` to find themselves more or less *interesting* in a **data-dependant** way...\n",
    "\n",
    "For example, because we are using a character level language model, if say I am a *vowel*, then maybe I want to look for *consonants* in my past to know what those *consonants* are and also I want that information to **flow** to me...\n",
    "\n",
    "In other words **we want to gather information from the past, and we want to do that in a data dependant way**...\n",
    "\n",
    "And this is the problem that `Self Attention` solves..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb181fc2-40ee-4522-ba2d-8f51087a2171",
   "metadata": {},
   "source": [
    "And the way `Self Attention` solves this, is...\n",
    "\n",
    "Every single `token` at each position, emits two vectors: **Query (Q)** and a **Key (K)**...\n",
    "\n",
    "We can look at **Query (Q)** and **Key (K)** in this way:\n",
    "- **Query (Q)** says *\"What am I looking for?\"*\n",
    "- **Key (K)** says *\"What do I contain?\"*\n",
    "\n",
    "And the way these `tokens` generate **affinities** or *interactions* is by a **dot-product** between the **queries** and the **keys**...\n",
    "\n",
    "Which eventually means that every `token` at every position says *\"Hey, I am a `token` and I contain a vowel **and** I am looking for consonants in my past\"* for our example... And the way they *interact* with each other is the **dot-product** between the queries and the keys..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9ff8f-78ec-4023-bc23-c08f2dc4b13c",
   "metadata": {},
   "source": [
    "For our case, we have an `inputs` tensor of shape `(B, T, C)` or `(2, 8, 3)` for our *toy-example*... Or `2 batches` of `8 tokens` having `3 embeddings`...\n",
    "\n",
    "But we need to understand that in order for our `inputs` to contribute and emit these **queries** and **keys**, we need the *information* from each `inputs` and we also need these **queries** and **keys** to emit their own *information* as outputs, which brings us to our **hyper-parameter** `headSize`...\n",
    "\n",
    "You can consider these output shapes of these **queries** and **keys** to be in form of some embeddings, and the easiest way to initialize these **queries** and **keys** is by creating a `Linear` layer without a bias and defining their `fan-in` as the *information* from each `inputs` and `fan-out` with the **hyper-parameter** `headSize` after initializing it with a value...\n",
    "\n",
    "For our case, the *information* from `inputs` that flow into these **queries** and **keys** (or `fan-in`) are of shape `channel` because thats where the original `Embeddings` are, and the *information* that flow out of these **queries** and **keys** (or `fan-out`) are of shape `headSize`... For now let's consider the `headSize` as `16`...\n",
    "\n",
    "After we initialize these **queries** and **keys** and run them through their linear transformation, our `inputs` now become from shape `(B, T, C)` to shape `(B, T, 16)` for our example... And these **queries** and **keys** can finally talk to each other after we have a **dot-product** between them (or Matrix Multiplication)...\n",
    "\n",
    "But the problem is these **queries** and **keys** are 3-dimensional now, and of shape `(B, T, 16)`... And they cannot multiply themselves because, in order for the broadcasting to work out they need to be in shape `(B, T, 16) @ (B, 16, T)`, which results in a `(B, T, T)` tensor... And in order for them to rearrange themselves in this manner, we will use <a href=\"https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose\">`transpose()`</a> and pass the parameters `(-2, -1)` as second last and last dimensions as parameters...\n",
    "\n",
    "Now they can **dot-product** fine and give us the *interation strength* or the **affinities** of these `inputs`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adc9b5-4843-4339-adf6-17d392091461",
   "metadata": {},
   "source": [
    "So now our *toy-example* code becomes:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "batch, time, channel = 2, 8, 3\n",
    "\n",
    "headSize = 16\n",
    "\n",
    "inputs = torch.randn(batch, time, channel)\n",
    "\n",
    "lowerTriangularMatrix = torch.tril(torch.ones(time, time))\n",
    "\n",
    "query = torch.nn.Linear(channel, headSize, bias=False)\n",
    "key = torch.nn.Linear(channel, headSize, bias=False)\n",
    "\n",
    "q = query(inputs) # (B, T, 16)\n",
    "k = key(inputs) # (B, T, 16)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "\n",
    "weights = weights.masked_fill(lowerTriangularMatrix == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "inputBagOfWords = weights @ inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b6e36-fd2d-4352-b69f-3a11573b10b6",
   "metadata": {},
   "source": [
    "And if we look at our weights now, they look something like this:\n",
    "```python\n",
    "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.2184, 0.7816, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.5845, 0.1986, 0.2170, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.0042, 0.7065, 0.2424, 0.0470, 0.0000, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.0238, 0.4702, 0.1258, 0.2795, 0.1007, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.0053, 0.3562, 0.0704, 0.2770, 0.0847, 0.2065, 0.0000, 0.0000],\r\n",
    "         [0.3267, 0.0463, 0.1988, 0.0257, 0.2178, 0.1656, 0.0190, 0.0000],\r\n",
    "         [0.1526, 0.0854, 0.2203, 0.0336, 0.2216, 0.2357, 0.0239, 0.0268]],\r\n",
    "\r\n",
    "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.0679, 0.9321, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.2828, 0.5033, 0.2139, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.1369, 0.0534, 0.4525, 0.3572, 0.0000, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.1286, 0.0627, 0.6126, 0.1609, 0.0352, 0.0000, 0.0000, 0.0000],\r\n",
    "         [0.1770, 0.0401, 0.4274, 0.2621, 0.0554, 0.0379, 0.0000, 0.0000],\r\n",
    "         [0.0413, 0.0016, 0.1513, 0.3671, 0.0347, 0.0171, 0.3869, 0.0000],\r\n",
    "         [0.1170, 0.0645, 0.3522, 0.1517, 0.0508, 0.0598, 0.1198, 0.0842]]],\r\n",
    "       grad_fn=<SoftmaxBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73fdc7-6c92-4cda-a56b-e691b736616b",
   "metadata": {},
   "source": [
    "Now we can see that previously our `weights` were a constant and they were applied on every single `batch` in the *same way*, but now each `batch` has different `weights` because every single batch element consists of different `tokens` at different `positions` which makes it **data-dependant**, which we can already see that how they are not exactly uniform...\n",
    "\n",
    "And doing these **dot-products** results in **affinities** and these **affinities** tend to be high if their *interaction strength* is high...\n",
    "\n",
    "And if say the weights have *high affinities* they end up utilizing more of that `token`'s *information* into the current `token`'s position and it gets to *learn* a lot about it, and vise versa..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533f7c9-e394-43b6-81b1-465dd7d67fe6",
   "metadata": {},
   "source": [
    "Now there's one more part to this `Self Attention` and that is the **Value (V)**...\n",
    "\n",
    "Which is also emitted from these `inputs`...\n",
    "\n",
    "Now we can look at these **Value (V)** as:\n",
    "- **Value (V)** says *\"What information do I have?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa4aa1-7258-478d-9b87-e2390d2c5c50",
   "metadata": {},
   "source": [
    "And the way these `tokens` now pass the *information* about them is by a **dot-product** between these **data-dependent weights** and the **values**...\n",
    "\n",
    "Which eventually means that every `token` at every position says *\"Hey, I am a `token` and I contain a vowel and I am looking for consonants in my past and my own information is kept in vector `inputs`\"*\n",
    "\n",
    "We can think `inputs` to have **private information** which only gets *communicated* or *passed around* if the *interaction* finds them *interesting*...\n",
    "\n",
    "So we can now implement **Value (V)**, the same way we initialized the **queries** and **keys**, and we can perform a dot product **after** we get the **data-dependant weights**, like this:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "batch, time, channel = 2, 8, 3\n",
    "\n",
    "headSize = 16\n",
    "\n",
    "inputs = torch.randn(batch, time, channel)\n",
    "\n",
    "lowerTriangularMatrix = torch.tril(torch.ones(time, time))\n",
    "\n",
    "query = torch.nn.Linear(channel, headSize, bias=False)\n",
    "key = torch.nn.Linear(channel, headSize, bias=False)\n",
    "value = torch.nn.Linear(channel, headSize, bias=False)\n",
    "\n",
    "q = query(inputs) # (B, T, 16)\n",
    "k = key(inputs) # (B, T, 16)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, T)\n",
    "\n",
    "weights = weights.masked_fill(lowerTriangularMatrix == 0, float('-inf')) # (B, T, T)\n",
    "weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "\n",
    "v = value(inputs) # (B, T, 16)\n",
    "\n",
    "inputBagOfWords = weights @ v # (B, T, 16)\n",
    "```\n",
    "And we see that these **values** are the ultimate thing that get aggregated and the `inputs` eventually get dissolved...\n",
    "\n",
    "So these are the **legendary lines** with which we can answer the questions asked, for each of these **queries, keys & values**:\n",
    "1. **Query (Q)** answers *\"Here's what I am interested in...\"*\n",
    "2. **Key (K)** answers *\"Here's what I have...\"*\n",
    "3. **Value (V)** answers *\"If you find me interesting, here's what here's what I will communicate to you...\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87558971-a9dd-47b1-a136-01b369eca20b",
   "metadata": {},
   "source": [
    "Let's look at the single `Self Attention` block or the `Scaled Dot Product Attention` image from the original paper:\n",
    "\n",
    "![Scaled_Dot_Product_Attention](ExplanationMedia/Images/Scaled_Dot_Product_Attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0639a5-e862-4617-8cc2-1831533f25ca",
   "metadata": {},
   "source": [
    "We see that we end up defining the exact same thing in the end...\n",
    "\n",
    "And we also see that there is an *optional* case in **Mask** part, and we will discuss this in the later notes...\n",
    "\n",
    "And we will also discuss why it is called `Scaled Dot Product Attention`...\n",
    "\n",
    "Let's now discuss a few things about `Self Attention` now to clear things up..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c58cb-ad1b-4c30-9d30-059732e8b89b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Notes on Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42acaaf3-7081-403a-aeac-f32e591ea704",
   "metadata": {},
   "source": [
    "**`Self-Attention` is a mechanism used in machine learning, to capture dependencies and relationships within input sequences.**\n",
    "\n",
    "![Scaled_Dot_Product_Attention](ExplanationMedia/Images/Scaled_Dot_Product_Attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acffea8-a4a4-4022-9a1e-9429c5ba4945",
   "metadata": {},
   "source": [
    "> Attention is a **communication mechanism**. Can be seen as `nodes` having a vector of *information* in a **directed graph** *looking at each other* and **aggregating information** with a *weighted sum* from all `nodes` that *point to them*, with **data-dependent weights**.\n",
    "\n",
    "Let's consider the following graph for example:\\\n",
    "![AttentionExplanationwithDirectedGraph](ExplanationMedia/Images/AttentionExplanationwithDirectedGraph.png)\n",
    "\n",
    "This illustration has a different structure than what we have implemented in our `Self Attention` because, our graph has `8` nodes (because of `blockSize`), and the first node is only pointed to itself, the second node is pointed to second node and itself, following the pattern upto the eighth node which is pointed to all the previous nodes and itself which can be termed as **auto regressive**...\n",
    "\n",
    "Auto Regressive Graph looks like this:\\\n",
    "![AutoRegressiveDirectedGraph](ExplanationMedia/Images/AutoRegressiveDirectedGraph.png)\n",
    "\n",
    "But in principle it can be applied to any arbitrary directed graph, and it is just a communication mechanism between the `nodes`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd01f6-171e-4da8-bcc9-ba87bbb67128",
   "metadata": {},
   "source": [
    "> There is **no notion of space**. `Attention` simply acts over a set of vectors, and so by default these `nodes` have no idea where they are positined in a space. This is why we need to **positionally encode** tokens, and give them *information* that is *anchored* to a specific position so that they *know* where they are...\n",
    "\n",
    "For example, this is different than **Convolution**, because if you run a **convolution operation** over some `input`, there is a very specific layout of *information* in space, and the **convolutional filters act in space**. But in `Attention`, we have a set of vectors out there in space, where they communicate and **if you want them to have a notion of space, you need to specifically add it**. Which is what we have done when we have calculated the `positional encodings` and added that information to the vectors..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ce7ec-b533-4deb-a537-6e6c43d1f707",
   "metadata": {},
   "source": [
    "> Each example across batch dimension is of course processed completely **independently** and never *\"talk\"* to each other...\n",
    "\n",
    "So in the analogy of a directed graph, and *toy-example* we can have something like this:\n",
    "```python\n",
    "batch, time, channel = 4, 8, 3\n",
    "```\n",
    "Where we had `4 batches` of `8 tokens`... And because we have `4` batches, we really have `4` separate pools of `8 nodes` and those `8` nodes only *talk* to each other, but in total there are `32` nodes that are being processed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497585b-d3b6-4b6e-9d90-fc0a300e8fde",
   "metadata": {},
   "source": [
    "Here in the case of **Language Modelling**, we have this specific structure of a directed graph, where the future tokens will not *communicate* with the past tokens, but, this doesn't have to be the case in general. In fact, in many cases, you want all of these `nodes` to fully *talk* to each other...\n",
    "\n",
    "For example, if you're doing *Sentiment Analysis* with a `Transformer`, you might have a number of tokens, and you may want them to *fully* talk to each other, because later you want to predict the *sentiment* of the sentence which makes it *okay* for them to talk to each other. And in those cases you will use an `encoder` block of self attention...\n",
    "\n",
    "And all it means to have an `encoder` block is to delete this line of code from our toy example:\n",
    "```python\n",
    "weights = weights.masked_fill(lowerTriangularMatrix == 0, float('-inf'))\n",
    "```\n",
    "Which makes the block completely talk to each other...\n",
    "\n",
    "And what we have already implemented in our previous *toy-example* is known as a `decoder` block. And it is called a `decoder` because we have to mask the values with an **auto regressive** lower triangular format which restricts the nodes from future to not talk to each other (because that would give away the *answer*)...\n",
    "\n",
    "But both are allowed and `Attention` supports arbitrary connection between `nodes`...\n",
    "\n",
    "So we can now safely say that:\n",
    "\n",
    "> In an **\"encoder\"** attention block we delete the single line that does masking with `lowerTrianguarMatrix`, allowing all `tokens` to communicate. This block here is called a **\"decoder\"** attention block because it has triangular masking, and is usually used in **autoregressive** settings, like language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485dfbc6-10f1-4bac-a360-33c95794e25b",
   "metadata": {},
   "source": [
    "Now, you see me mentioning `Attention` and `Self Attention`, but there is something called `Cross Attention` as well...\n",
    "\n",
    "So what is the difference between `Self Attention` and `Cross Attention`?\n",
    "\n",
    "So the reason, we are calling our code `Self Attention` is because the **keys, queries & values** are all coming from the same source which is `inputs` (which makes the *self-attending*)...\n",
    "\n",
    "But in principle, `Attention` is much more general than that...\n",
    "\n",
    "For example, in encoder-decoder transformers, we can have a case where the **queries** are produces from `inputs`, but the **keys** and the **values** come from a whole separate **external source**, and sometimes from some `encoder` blocks that we like to condition on...\n",
    "\n",
    "So `Cross Attention` is used when we have a **separate source** of nodes from where we want to pull *information* from into our `nodes`, and in `Self Attention` is used when we have a set of nodes, where we want them to *look* at each other and *talk* to each other...\n",
    "\n",
    "So now it is safe to say that:\n",
    "\n",
    "> **\"Self-Attention\"** just means that the **keys** and **values** are produced from the **same source** as **queries**. In **\"Cross-Attention\"**, the **queries** still get produced from `inputs`, but the **keys** and **values** come from some other, **external source** (e.g. an Encoder Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34545d98-6001-4ac1-ab37-b5d2661e7b4f",
   "metadata": {},
   "source": [
    "Lastly when we look at the original <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> paper, we see this equation:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Now according to the equation, we have already implemented `Attention` given the `Query`, `Key` and `Value`... We have multiplied the `Query` and `Key`, we have applied `softmax()` and also aggregating the `Value`s...\n",
    "\n",
    "But we see that we are still missing this $\\frac{1}{\\sqrt{d_k}}$ here, and the $d_k$ here is none other than the `headSize`...\n",
    "\n",
    "So why are they doing it? And also why do they call it the `Scaled Dot Product Attention`?\n",
    "\n",
    "The problem is that when we have **unit-gaussian** inputs (`0` mean and `unit` variance) and if we do what we have already implemented without the `headSize`, and check the mean and the variance like this:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "B, T = 4, 8\n",
    "headSize = 16\n",
    "\n",
    "k = torch.randn(B, T, headSize)\n",
    "q = torch.randn(B, T, headSize)\n",
    "weights = q @ k.transpose(-2, -1)\n",
    "\n",
    "print(\"Mean: \", weights.mean())\n",
    "print(\"Varince\", weights.var())\n",
    "```\n",
    "We see that our `varince` of `weights` come out in the order of `headSize`:\n",
    "```python\n",
    "Mean:  tensor(-0.0133)\n",
    "Variance:  tensor(16.1469)\n",
    "```\n",
    "\n",
    "But the moment we apply this formula during the `weights` initilization:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "B, T = 4, 8\n",
    "headSize = 16\n",
    "\n",
    "k = torch.randn(B, T, headSize)\n",
    "q = torch.randn(B, T, headSize)\n",
    "weights = q @ k.transpose(-2, -1) * headSize ** -0.5\n",
    "\n",
    "print(\"Mean: \", weights.mean())\n",
    "print(\"Variance: \", weights.var())\n",
    "```\n",
    "We get the `varince` of `weights` to be `1`:\n",
    "```python\n",
    "Mean:  tensor(-0.0033)\n",
    "Variance:  tensor(1.0092)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58790760-abd7-4db4-946b-1f33499bdc11",
   "metadata": {},
   "source": [
    "Now why is this important?\n",
    "\n",
    "We understand that our `weights` get fed into the `softmax()`...\n",
    "\n",
    "And we understand from our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a> notebook that we want these `weights` to be fairly diffuse especially during initialization...\n",
    "\n",
    "But why?\n",
    "\n",
    "The problem is, if our `weights` take on very positive or very negetive numbers inside it, `softmax()` would actually *converge* towards one-hot vectors...\n",
    "\n",
    "Let's illustrate this...\n",
    "\n",
    "Suppose we have a list of `5` simple floating point numbers:\n",
    "```python\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n",
    "```\n",
    "We get this:\n",
    "```python\n",
    "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
    "```\n",
    "But the moment we take these numbers and we start sharpening it by suppose multiplying the numbers with a big number:\n",
    "```python\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 9, dim=-1)\n",
    "```\n",
    "We get this:\n",
    "```python\n",
    "tensor([0.0228, 0.0015, 0.1382, 0.0015, 0.8359])\n",
    "```\n",
    "We see that the `softmax()` starts to sharpen towards the maximum or the highest number that is there...\n",
    "\n",
    "Which eventually means that we end up being **aggregating** the *information* about a single node... Which is not what we want... So, the `scaling` is used to control the initialized values, and hence the name `Scaled Dot Product Attention`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ba2f9-bce5-4663-b36a-106c6a53233d",
   "metadata": {},
   "source": [
    "So it is safe to say that:\n",
    "\n",
    "> `\"Scaled\"` Attention additionally divides `weights` by $\\frac{1}{\\sqrt{\\text{headSize}}}$. This makes it so when input $Q, K$ are **unit variance**, `weights` will be **unit variance** too and `softmax` will stay diffuse and not saturate too much..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19118e69-773f-4fdb-8e55-0861975c19a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Changing Script to `single_self_attention_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2394db-ef30-4c69-9574-3ef856b5e89a",
   "metadata": {},
   "source": [
    "Let's now take the knowledge that we have from `Self Attention` and implement it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b8b08-eedd-445d-9b5d-8ace32f20e4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Change - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419d8cf-ee09-49e3-8a69-4a1d58373b7c",
   "metadata": {},
   "source": [
    "Let's get our names straight first...\n",
    "\n",
    "Because now we will be using a `Transformer` style architecture, we will be renaming our model from `BigramLanguageModel` to `GPTModel`...\n",
    "\n",
    "So, we will change the code during:\n",
    "1. Model Definition\n",
    "2. Model Initialization\n",
    "\n",
    "During **Model Definition**, our code was:\n",
    "```python\n",
    "class BigramLanguageModel(torch.nn.Module):\n",
    "```\n",
    "Which now becomes:\n",
    "```python\n",
    "class GPTModel(torch.nn.Module):\n",
    "```\n",
    "And during **Model Initialization**, our code was:\n",
    "```python\n",
    "model = BigramLanguageModel().to(device=device)\n",
    "```\n",
    "Which now becomes:\n",
    "```python\n",
    "model = GPTModel().to(device=device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e08387-5396-4663-8115-7ee5d79dbdee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Change - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b3408-473e-4dad-9949-5a04e3e795f7",
   "metadata": {},
   "source": [
    "We will now create a `Head` module, that implements a **single self attention head** and takes `headSize` as a **hyper-parameter**...\n",
    "\n",
    "During initialization, we will have the **keys, queries & values** from the `Linear` projections, and because we will be placing the `Head` module right after we create the fuse the `tokenEmbeddings` and `positionalEmbeddings`, we will get the inputs to this layer as a `(B, T, C)` tensor, and if you remember correctly, in the explanation we used the `fan-in` of these **keys, queries & values** to be of shape of the `input's channel` dimension, which for our case now is none other than these fused embeddings having the channel dimension as `numberOfEmbeddingDimensions`, which means we will set the `fan-in` to these **keys, queries & values** to `numberOfEmbeddingDimensions` as well...\n",
    "\n",
    "And we will rename these `concatenatedEmbeddings` to just `embeddings` now, because we will be using the same outputs to our `languageModelingHead` in order to calculate logits... So it is better to rename it rather than creating confusion...\n",
    "\n",
    "Now because the `lowerTrianguarMatrix` is not a parameter of the `Head` module, and in PyTorch naming conventions it is called a **buffer**. And because we are inheriting the `torch.nn.Module`, we can now use the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\">`register_buffer(name, tensor, persistent=True)`</a> which takes in a tensor and a string name to register that tensor with the name, such that we can use it later in our block using `self.<buffer name>` or `self.lowerTrianguarMatrix`... And because our `inputs` have the time dimension as the `blockSize`, we can use it to initialize the tensor of `lowerTriangularMatrix` now, and because in the `masked_fill()` we check whether the `tokens` in each batch are `0` or not, we can now check if the `tokens` in each batch to be `0` or not, using this `lowerTriangularMatrix[:time, :time]` to future proof our code such that if the time dimension is changed later, it does not cause any issues...\n",
    "\n",
    "And during the `forward()` we pass the `inputs` through the function, such that it can be used to calculate the **keys** and **queries** to initialize the weights using the scaled initialization that we discussed, and then we make sure that those `weights` don't communicate with the past using the `self.lowerTriangularMatrix` mask, then we `softmax()` the weights and calculate the **value** to aggregate the `weights` with the value and return the output...\n",
    "\n",
    "So now our code looks like:\n",
    "```python\n",
    "# Head Module Definiton\n",
    "class Head(torch.nn.Module):\n",
    "    \"\"\" Single Head of Self Attention \"\"\"\n",
    "    # Constructor for the Head\n",
    "    def __init__(self, headSize):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(numberOfEmbeddingDimensions, headSize, bias=False)\n",
    "        self.query = torch.nn.Linear(numberOfEmbeddingDimensions, headSize, bias=False)\n",
    "        self.value = torch.nn.Linear(numberOfEmbeddingDimensions, headSize, bias=False)\n",
    "        self.register_buffer(name='lowerTriangularMatrix', tensor=torch.tril(torch.ones(blockSize, blockSize)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Unpacking the shape of inputs\n",
    "        batch, time, channel = inputs.shape\n",
    "        # Forwarding the inputs to keys and queries\n",
    "        k = self.key(inputs) # (B, T, C)\n",
    "        q = self.query(inputs) # (B, T, C)\n",
    "        # Initializing weights with scaled dot product\n",
    "        weights = q @ k.transpose(-2, -1) * headSize ** -0.5 # (B, T, T)\n",
    "        # Masking the weights\n",
    "        weights = weights.masked_fill(self.lowerTriangularMatrix[:time, :time] == 0, float('-inf')) # (B, T, T)\n",
    "        # Softmax the weights\n",
    "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "        # Forwarding the inputs to values\n",
    "        v = self.value(inputs) # (B, T, C)\n",
    "        # Aggregating the weights and the values\n",
    "        output = weights @ v # (B, T, C)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30979a-aa8e-4dd6-8f3b-0632ba6f7c1d",
   "metadata": {},
   "source": [
    "Now we can initialize our `Head` module inside `GPTModel` and modify the forward pass of the module...\n",
    "\n",
    "So, we will initialize the `Head` as `selfAttentionHead` variable and keep the `headSize` as `numberOfEmbeddingDimensions` as of now...\n",
    "\n",
    "And during the forward pass, right after we have the fused `tokenEmbeddings` and `positionalEmbeddings`, we will forward the `embeddings` to the `selfAttentionHead` to get the output embeddings and those output embeddings are going to go into the decoder `languageModelingHead` to create the `logits`...\n",
    "\n",
    "So now our `GPTModel` definition looks like:\n",
    "```python\n",
    "# Model Module Definition\n",
    "class GPTModel(torch.nn.Module):\n",
    "    # Constructor for the model\n",
    "    def __init__(self):\n",
    "        # Initializing the embedding table\n",
    "        super().__init__()\n",
    "        self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, numberOfEmbeddingDimensions)\n",
    "        self.positionalEmbeddingTable = torch.nn.Embedding(blockSize, numberOfEmbeddingDimensions)\n",
    "        self.selfAttentionHead = Head(headSize=headSize)\n",
    "        self.languageModelingHead = torch.nn.Linear(numberOfEmbeddingDimensions, vocabularySize)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, indeces, labels=None):\n",
    "        # Unpacking the shape of indeces\n",
    "        batch, time = indeces.shape\n",
    "\n",
    "        # Index into embeddings to get the token embeddings\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "        # Index into embeddings to get the positional embeddings\n",
    "        positionalEmbeddings = self.positionalEmbeddingTable(torch.arange(time, device=device)) # (T, C)\n",
    "        # Fuse the token embeddings and positional embeddings together to pack the information in a single tensor\n",
    "        embeddings = tokenEmbeddings + positionalEmbeddings # (B, T, C)\n",
    "        # Pass the concatenated embeddings into our self attention head\n",
    "        embeddings = self.selfAttentionHead(embeddings) # (B, T, C)\n",
    "        # Pass the embeddings through a linear layer\n",
    "        logits = self.languageModelingHead(embeddings) # (B, T, C)\n",
    "\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Pop out the shape dimensions\n",
    "            batch, time, channel = logits.shape\n",
    "            # Stretch out the logits and labels\n",
    "            logits = logits.view(batch*time, channel)\n",
    "            labels = labels.view(batch*time)\n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "    # Generation\n",
    "    def generate(self, indeces, maximumNewTokens):\n",
    "        for _ in range(maximumNewTokens):\n",
    "            # Crop the indeces upto most recent block size context\n",
    "            croppedIndeces = indeces[:, -blockSize:]\n",
    "            # Forward Through Model\n",
    "            logits, loss = self(croppedIndeces)\n",
    "            # Focus on the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            # Applying softmax for the last dimension\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            # Sample from distribution\n",
    "            nextIndex = torch.multinomial(probabilities, num_samples=1)\n",
    "            # Concatenate currentIndex with nextIndex\n",
    "            indeces = torch.cat((indeces, nextIndex), dim=1)\n",
    "        return indeces\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc685d-db30-40ac-b02f-c155eb3d082a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Change - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d174e-4be3-4fac-a590-a1d1f07ad09a",
   "metadata": {},
   "source": [
    "Before we run our first `GPT`, we will decrease the `learningRate` from `1e-2` to `1e-3`, because `Self Attention` cannot tolerate very very high learning rates, and we will also increase the `epochs` because our `learningRate` is now lower so from `30000` to say `50000`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b68e3-7053-4a19-80c1-4c7d5ee28534",
   "metadata": {},
   "source": [
    "So, let's now run it and keep track of our losses..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f2dcc-e583-42cf-8620-3a1e5d83c4d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Keeping track of losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de490bb6-b1aa-4fde-a1cf-123541a4dd81",
   "metadata": {},
   "source": [
    "Losses at `bigram_v1.py`:\n",
    "```python\n",
    "Step 29500: Training Loss 2.4385, Validation Loss 2.4322\n",
    "```\n",
    "Losses at `bigram_v2.py`:\n",
    "```python\n",
    "Step 29500: Training Loss 2.4641, Validation Loss 2.4487\n",
    "```\n",
    "Losses at `single_self_attention_gpt.py`\n",
    "```python\n",
    "Step 49500: Training Loss 2.2869, Validation Loss 2.2751\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976fe8d-4583-4ea0-9e67-f62982cc4f91",
   "metadata": {},
   "source": [
    "This seems like a nice improvement now, but our model is still not predicting good output and we get something like this:\n",
    "```bash\n",
    "\n",
    "“I fout he\n",
    "astt fery witrel at ce “Theedigh whe way’beny is, llin wary, y’s, orugh adongose.\n",
    "She, llinggtr acortinul .” Hamobou’ soryot himato ank adrdd lary ige hiovingen cksint hon ederr tited I toplim Paos colso thme he.\n",
    "\n",
    "Hame thurion ly u?”\n",
    "\n",
    "\n",
    "Here mpat theirse yin ’ree, I Icthe yound artherre gthe, wing\n",
    "hesirou’stthe dy, sle Rof winu,”\n",
    "\n",
    "“Harread ack ladng He Lithamomerd seve han oucu ath hisene te ing\n",
    "\n",
    "Shis Chory thoun, ”\n",
    "\n",
    "\n",
    "“Ley whad towourath war oe tohad ly wie here a). Then demame\n",
    "```\n",
    "\n",
    "Which means that `Self Attention` head that we have created is doing some useful communication..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc840a-54e2-43fa-9c05-a1627641dd76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fd519-6af1-4932-9913-2dea59cff5e9",
   "metadata": {},
   "source": [
    "If we now look at the <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> paper and scroll down a little bit we will see a section of `Multi-Head Attention`...\n",
    "\n",
    "So what is this new `Multi-Head Attention`?\n",
    "\n",
    "Well `Multi-Head Attention`, is just applying **multiple** attentions in **parallel** and **concatenating** the results...\n",
    "\n",
    "Let's check out the diagram according to the original paper...\n",
    "![Multi Head Attention](ExplanationMedia/Images/Multi_Head_Attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7837142-e3b0-48cd-8df3-35413450e6af",
   "metadata": {},
   "source": [
    "So we can create our own `MultiHeadAttention` module now that takes `numberOfHeads` and `headSize` as **hyper-parameters** for the module...\n",
    "\n",
    "Where `numberOfHeads` specifies the number of heads we want in our `Attention` and `headSize` specifies the head size in each of these `numberOfHeads`...\n",
    "\n",
    "We can then define the initialization of `Heads` in a list of Modules or <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html\">ModuleList</a> from PyTorch, which can later be used to act as an iterable...\n",
    "\n",
    "And during the forward pass, we run all of them in parallel in a list, and concatenate them using the last dimension, which is the `channel` dimension for us...\n",
    "\n",
    "So now the `MultiHeadAttention` module code looks like:\n",
    "```python\n",
    "# Multi-Head Attention Module Definiton\r\n",
    "class MultiHeadAttention(torch.nn.Module):\r\n",
    "    \"\"\" Multiple Heads of Self Attention in Parallel \"\"\"\r\n",
    "    # Constructor for the Multi-Head Attention\r\n",
    "    def __init__(self, numberOfHeads, headSize):\r\n",
    "        super().__init__()\r\n",
    "        self.heads = torch.nn.ModuleList([Head(headSize=headSize) for _ in range(numberOfHeads)])\r\n",
    "\r\n",
    "    # Forward Pass\r\n",
    "    def forward(self, inputs):\r\n",
    "        # Returns the concatenated heads over the channel dimension\r\n",
    "        return torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1763be-377e-4788-b88d-58c382e61a08",
   "metadata": {},
   "source": [
    "So now we don't have just a single `Attention` that has a `headSize` of `32`(because we used `headSize` as `numberOfEmbeddingDimensions` which was `32`)...\n",
    "\n",
    "So instead of having `1` communication channel(a single `Head`), we now will have `4` communication channels(`numberOfHeads`) in parallel, and each one of these communication channels will be typically smaller correspondingly... So because we have `4` communication channels, we want `8` dimensional `Self Attention` or `numberOfEmbeddingDimensions // 4` (which concatenates to give us `32`, which is the original `headSize` that we had as before)...\n",
    "\n",
    "So we will replace our older line:\n",
    "```python\n",
    "self.selfAttentionHead = Head(headSize=headSize)\n",
    "```\n",
    "\n",
    "With this line:\n",
    "```python\n",
    "self.selfAttentionHeads = MultiHeadAttention(numberOfHeads=numberOfHeads, headSize=numberOfEmbeddingDimensions//numberOfHeads)\n",
    "```\n",
    "\n",
    "And changed the forward pass from:\n",
    "```python\n",
    "# Pass the concatenated embeddings into our self attention head\n",
    "embeddings = self.selfAttentionHead(embeddings) # (B, T, C)\n",
    "```\n",
    "To this:\n",
    "```python\n",
    "# Pass the concatenated embeddings into our multihead attention\n",
    "embeddings = self.selfAttentionHeads(embeddings) # (B, T, C)\n",
    "```\n",
    "\n",
    "And if you're familiar with **convolutions**, this is kind of like a **Grouped Convolution** because, instead of having a very large convolution, we do small convolutions in groups...\n",
    "\n",
    "In multi-head self-attention, each attention head learns different relationships between `tokens` in the input sequence, enabling the model to attend to multiple aspects of the data simultaneously. Similarly, in grouped convolution, different groups of `channels` capture different features in the input, providing a diverse set of representations..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248157d4-73c7-4d86-8299-df8d2b8e59f0",
   "metadata": {},
   "source": [
    "So now let's take our knowledge and create a new script for it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb67f4-f389-4073-862c-2f5bcb744e1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Changing Script to `multi_head_attention_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494b074-bdd2-4bc6-9fd3-9c62e5c00065",
   "metadata": {},
   "source": [
    "So we will now take all the knowledge that we have from our `Multi-Head Attention` and create a new script `multi_head_attention_gpt.py` and try to check what kind of outputs we get and compare our old losses as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c391d-b258-4195-bc13-e7a18c69ff01",
   "metadata": {},
   "source": [
    "So after running the `Multi-Head Attention` we get an output like this:\n",
    "```bash\n",
    "\r\n",
    "I fhur he\r\n",
    "astt feye watred at crign. Maid! IW\r\n",
    "loaing mons, loin ward this\r\n",
    "orng the\r\n",
    "hay eak he alling to Hery rey seer had ou’ york this to hatk a Moddgard the his\r\n",
    "Page cas, the withey criged I moplionk of toh a thof hake siang thurion lyou?”\r\n",
    "\r\n",
    "Harry stan their asling the\r\n",
    "noththe you ding hering the - Jus\r\n",
    "hes roubutthersc, sle com youu,” thims Read ack ladn’t bect.”\r\n",
    "\r\n",
    "Marsp whe cas of wi; he asen by Piffagrwas got shet, thage obe leyhith reen buatthe — ieet:\r\n",
    "\r\n",
    "He nowe he ove). Tolng him u\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6077fb34-5c1f-483b-9418-7b80edd4287b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Keeping track of losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62176169-5022-49fd-9e2d-de17bd1021b3",
   "metadata": {},
   "source": [
    "Losses at `bigram_v1.py`:\n",
    "```python\n",
    "Step 29500: Training Loss 2.4385, Validation Loss 2.4322\n",
    "```\n",
    "Losses at `bigram_v2.py`:\n",
    "```python\n",
    "Step 29500: Training Loss 2.4641, Validation Loss 2.4487\n",
    "```\n",
    "Losses at `single_self_attention_gpt.py`\n",
    "```python\n",
    "Step 49500: Training Loss 2.2869, Validation Loss 2.2751\n",
    "```\n",
    "Losses at `multi_head_attention_gpt.py`\n",
    "```python\n",
    "Step 49500: Training Loss 2.0459, Validation Loss 2.0368\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8cf30-3e40-4fd5-9b96-68ca1f1dd439",
   "metadata": {},
   "source": [
    "We see that our output is still not that much amazing but our losses are definately improving...\n",
    "\n",
    "And in conclusion, it helps to have multiple communication channels because our `tokens` have a lot to talk about, which eventually makes them find and communicate many kinds of different things and gather lots of different types of data and then **decode** the output..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0a6db-7303-48c1-9d66-39f44d13848b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Complete `Transformer` Explanation from <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb7403-9acf-45f5-96b0-d69e4584acbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding Where We Are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735e27b-b3e9-48c7-90ee-5bf4a7471b79",
   "metadata": {},
   "source": [
    "Now we haven't discussed the `Transformer` architecture that is illustrated in the original <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> Paper...\n",
    "\n",
    "Let's look at the diagram again to relate what we have done till now...\n",
    "![Transformer_Model_Architecture](ExplanationMedia/Images/Transformer_Model_Architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab1511-7284-4993-80fd-cd11e0aadfce",
   "metadata": {},
   "source": [
    "We are starting to see some things that we have already implemented...\n",
    "\n",
    "Such as the `Positional Encodings` and the `Token Encodings` that **add up**, the `Masked Multi-Head Attention`...\n",
    "\n",
    "We see there's another `Multi-Head Attention` that is a `Cross Attention` into an encoder, which we are **not** going to implement, and I will explain why we won't implement that in a bit...\n",
    "\n",
    "We also see something known as `Nx` which is mentioned because the parts that we see inside a white box, are grouped into a `Block` and gets repeated `Nx` number of times...\n",
    "\n",
    "And we see that we have lots of other parts that we have not implemented yet...\n",
    "\n",
    "So let's do that one by one now. But before we do that I wanted to show you how much we have implemented of the entire illustration so far...\n",
    "![TransformerArchitectureKnownImplementation](ExplanationMedia/Images/TransformerArchitectureKnownImplementation.png)\n",
    "\n",
    "We understand that we have a lot to implement still, so let's do that now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98e5e9-8809-4516-b19a-695d5774dae1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding `Feed Forward` Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c695df-fddf-419c-9865-ee259738bac0",
   "metadata": {},
   "source": [
    "First let's focus on the `Feed Forward` part that we have here...\n",
    "\n",
    "This `Feed Forward` part is just a **Multi-Layer Perceptron** at the moment...\n",
    "\n",
    "And we'd also like to add some computation within the network now (which will be on a **per-node** level)...\n",
    "\n",
    "But why add computation when we are trying to reduce the computation in the first place?\n",
    "\n",
    "Good question... The thing is, we went way too fast to compute the `logits`. Basically, the `tokens` looked at each other in the `Multi-Head Attention` but did not really had a lot of time to *think* on what they *found* from the other `tokens`...\n",
    "\n",
    "So let's first try to write out a `Feed Forward` Module block that will have a single `Linear` layer followed by a `ReLU` non-linearity which forwards itself by calling itself on `inputs` that it gets...\n",
    "\n",
    "Now because the `Linear` layer expects a `fan-in` and `fan-out`, and because we will place the `Feed Forward` network after we get the outputs from `Multi-Head Attention` and before we pass it to the last `languageModelingHead`, we expect a `fan-in` in the shape of `numberOfEmbeddingDimensions`, and we will keep the `fan-out` to be the same, such that the inputs that `languageModelingHead` expects should be the same that comes out of this `Feed Forward` block...\n",
    "\n",
    "So our `Feed Forward` module definition looks like:\n",
    "```python\n",
    "# Feed Forward Module Definition\r\n",
    "class FeedForward(torch.nn.Module):\r\n",
    "    \"\"\" Simple Feed Forward Network \"\"\"\r\n",
    "    # Constructor for the Feed Forward Network\r\n",
    "    def __init__(self, numberOfEmbeddingDimensions):\r\n",
    "        # Initializing the layers\r\n",
    "        super().__init__()\r\n",
    "        self.network = torch.nn.Sequential(\r\n",
    "            torch.nn.Linear(numberOfEmbeddingDimensions, numberOfEmbeddingDimensions),\r\n",
    "            torch.nn.ReLU()\r\n",
    "        )\r\n",
    "\r\n",
    "    # Forward Pass\r\n",
    "    def forward(self, inputs):\r\n",
    "        return self.network(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e38165-44c3-4aaf-b580-3ea4c77b2b29",
   "metadata": {},
   "source": [
    "Now that we have the definition, we can add this `Feed Forward` by initializing it in our `GPTModel`'s constructor and forward them before the `languageModelingHead`...\n",
    "\n",
    "Like this:\n",
    "```python\n",
    "class GPTModel(torch.nn.Module):\r\n",
    "    # Constructor for the model\r\n",
    "    def __init__(self):\r\n",
    "        # Initializing the embedding table\r\n",
    "        super().__init__()\r\n",
    "        self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, numberOfEmbeddingDimensions)\r\n",
    "        self.positionalEmbeddingTable = torch.nn.Embedding(blockSize, numberOfEmbeddingDimensions)\r\n",
    "        self.selfAttentionHeads = MultiHeadAttention(numberOfHeads=numberOfHeads, headSize=numberOfEmbeddingDimensions//numberOfHeads)\r\n",
    "        self.feedforwardnetwork = FeedForward(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions)\r\n",
    "        self.languageModelingHead = torch.nn.Linear(numberOfEmbeddingDimensions, vocabularySize)\r\n",
    "\r\n",
    "    # Forward Pass\r\n",
    "    def forward(self, indeces, labels=None):\r\n",
    "        # Unpacking the shape of indeces\r\n",
    "        batch, time = indeces.shape\r\n",
    "\r\n",
    "        # Index into embeddings to get the token embeddings\r\n",
    "        tokenEmbeddings = self.tokenEmbeddingTable(indeces) # (B, T, C)\r\n",
    "        # Index into embeddings to get the positional embeddings\r\n",
    "        positionalEmbeddings = self.positionalEmbeddingTable(torch.arange(time, device=device)) # (T, C)\r\n",
    "        # Fuse the token embeddings and positional embeddings together to pack the information in a single tensor\r\n",
    "        embeddings = tokenEmbeddings + positionalEmbeddings # (B, T, C)\r\n",
    "        # Pass the concatenated embeddings into our multihead attention\r\n",
    "        embeddings = self.selfAttentionHeads(embeddings) # (B, T, C)\r\n",
    "        # Forward through the feed forward network\r\n",
    "        embeddings = self.feedforwardnetwork(embeddings) # (B, T, C)\r\n",
    "        # Pass the embeddings through a linear layer\r\n",
    "        logits = self.languageModelingHead(embeddings) # (B, T, C)\r\n",
    "\r\n",
    "        if labels is None:\r\n",
    "            loss = None\r\n",
    "        else:\r\n",
    "            # Pop out the shape dimensions\r\n",
    "            batch, time, channel = logits.shape\r\n",
    "            # Stretch out the logits and labels\r\n",
    "            logits = logits.view(batch*time, channel)\r\n",
    "            labels = labels.view(batch*time)\r\n",
    "            # Calculate loss\r\n",
    "            loss = F.cross_entropy(logits, labels)\r\n",
    "        return logits, loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5aebd9-5995-482e-8444-58158bab8814",
   "metadata": {},
   "source": [
    "Now we notice that our `Feed Forward` applies `Linear` tranformation on a **per-token** level (all the `tokens` do this independently...\n",
    "\n",
    "Which means that `Self Attention` is the communication, and once it has gathered all the *information*, in `Feed Forward` they need to think on that gathered *information*, individually..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d44dae-6317-4a1f-82b8-ba2b132c51ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Changing Script to `multi_head_attention_with_feedforward_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b084fb5-206f-47a5-8f2b-8fdecd9dec6c",
   "metadata": {},
   "source": [
    "Let's now take the knowledge that we have from our `Feed Forward` Block and implement it in a `multi_head_attention_with_feedforward_gpt.py` file and check how far do we come from our previous `losses`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18301bd9-b064-42a7-98a2-2e3119721ff5",
   "metadata": {},
   "source": [
    "The output we get:\n",
    "```bash\n",
    "\r\n",
    "th-ir, “I dong’t the weerverounselled ...”\r\n",
    "\r\n",
    "“Waid to, Potermoo lasophe\r\n",
    "whound tell dolles, stalughead to to\r\n",
    "gethe — ... sheile\r\n",
    "is a ther\r\n",
    "to Harry her. skot\r\n",
    "shor beetin a gazinged uppror as of ther\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "“Whien thed rark of it way sthrey days werelley he wplucten, — ” said no’s her, on to out the\r\n",
    "of the pam getion.\r\n",
    "Ron her. .. ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba2500-90b6-44e2-93b1-d047f767a64e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Keeping track of losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf5a16-62c6-4080-8183-14409d2c9023",
   "metadata": {},
   "source": [
    "Losses at `bigram_v1.py`:\n",
    "```python\n",
    "Step 29500: Training Loss 2.4385, Validation Loss 2.4322\n",
    "```\n",
    "Losses at `bigram_v2.py`:\n",
    "```python\n",
    "Step 29500: Training Loss 2.4641, Validation Loss 2.4487\n",
    "```\n",
    "Losses at `single_self_attention_gpt.py`\n",
    "```python\n",
    "Step 49500: Training Loss 2.2869, Validation Loss 2.2751\n",
    "```\n",
    "Losses at `multi_head_attention_gpt.py`\n",
    "```python\n",
    "Step 49500: Training Loss 2.0459, Validation Loss 2.0368\n",
    "```\n",
    "Losses at `multi_head_attention_with_feedforward_gpt.py`\n",
    "```python\n",
    "Step 49500: Training Loss 1.9708, Validation Loss 1.9792\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029dede3-bba9-4952-aeed-2661cc88752c",
   "metadata": {},
   "source": [
    "Now we would like to intersperse the *communication* with the *computation*... Which is also what the `Transformer` does, when it has blocks that *communicate* and *compute*, and it groups them and replicates them...\n",
    "\n",
    "So let's do that now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f97e5-eaea-4f60-81bd-ad4af9467ce3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Interspersing *communication* (Multi-Head Attention) & *computation* (Feed Forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d6b90-108e-490a-b077-e8ee0a64a447",
   "metadata": {},
   "source": [
    "So now, we want a `Transformer Block` module that intersperses the *communication* followed by the *computation*.\n",
    "\n",
    "The *communication* is done by the `Multi-Head Attention` and the *computation* is done using a `Feed Forward` network on all the `tokens` independently (the white box that we talked about from the original illustration)...\n",
    "\n",
    "Which means that we want to create a `TransformerBlock` Module that initializes two things:\n",
    "1. *communication* → `Multi-Head Attention`\n",
    "2. *computation* → `Feed Forward`\n",
    "\n",
    "Which means we need to pass all the hyper-parameters required by both the blocks through this `TransformerBlock`'s constructor, and we need to call both of them during forward pass with `inputs`...\n",
    "\n",
    "So now our `TransformerBlock` module looks like:\n",
    "```python\n",
    "# Transformer Block Module Definition\r\n",
    "class TransformerBlock(torch.nn.Module):\r\n",
    "    \"\"\" Communication Followed By Computation \"\"\"\r\n",
    "    # Constructor for the Transformer Block \r\n",
    "    def __init__(self, numberOfEmbeddingDimensions, numberOfHeads):\r\n",
    "        # Initializing the Transformer Block\r\n",
    "        super().__init__()\r\n",
    "        self.selfAttention = MultiHeadAttention(numberOfHeads=numberOfHeads, headSize=numberOfEmbeddingDimensions//numberOfHeads)\r\n",
    "        self.feedforwardnetwork = FeedForward(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions)\r\n",
    "\r\n",
    "    # Forward Pass\r\n",
    "    def forward(self, inputs):\r\n",
    "        embeddings = self.selfAttention(inputs) # (B, T, C)\r\n",
    "        embeddings = self.feedforwardnetwork(embeddings) # (B, T, C)\r\n",
    "        return embeddings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c2ea0-f1d9-406c-8d96-74093b7b376e",
   "metadata": {},
   "source": [
    "Now that we have our `TransformerBlock` module, we can now change our main `GPTModel`'s constructor from this:\n",
    "```python\n",
    " # Constructor for the model\n",
    "def __init__(self):\n",
    "    # Initializing the embedding table\n",
    "    super().__init__()\n",
    "    self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, numberOfEmbeddingDimensions)\n",
    "    self.positionalEmbeddingTable = torch.nn.Embedding(blockSize, numberOfEmbeddingDimensions)\n",
    "    self.selfAttentionHeads = MultiHeadAttention(numberOfHeads=numberOfHeads, headSize=numberOfEmbeddingDimensions//numberOfHeads)\n",
    "    self.feedforwardnetwork = FeedForward(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions)\n",
    "    self.languageModelingHead = torch.nn.Linear(numberOfEmbeddingDimensions, vocabularySize)\n",
    "```\n",
    "To this:\n",
    "```python\n",
    "# Constructor for the model\n",
    "def __init__(self):\n",
    "    # Initializing the model parameters\n",
    "    super().__init__()\n",
    "    self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, numberOfEmbeddingDimensions)\n",
    "    self.positionalEmbeddingTable = torch.nn.Embedding(blockSize, numberOfEmbeddingDimensions)\n",
    "    self.blocks = torch.nn.Sequential(\n",
    "        TransformerBlock(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions, numberOfHeads=numberOfHeads),\n",
    "        TransformerBlock(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions, numberOfHeads=numberOfHeads),\n",
    "        TransformerBlock(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions, numberOfHeads=numberOfHeads),\n",
    "    )\n",
    "    self.languageModelingHead = torch.nn.Linear(numberOfEmbeddingDimensions, vocabularySize)\n",
    "```\n",
    "\n",
    "And the forward pass of `GPTModel` from this:\n",
    "```python\n",
    "# Forward Pass\n",
    "def forward(self, indeces, labels=None):\n",
    "    # Unpacking the shape of indeces\n",
    "    batch, time = indeces.shape\n",
    "\n",
    "    # Index into embeddings to get the token embeddings\n",
    "    tokenEmbeddings = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "    # Index into embeddings to get the positional embeddings\n",
    "    positionalEmbeddings = self.positionalEmbeddingTable(torch.arange(time, device=device)) # (T, C)\n",
    "    # Fuse the token embeddings and positional embeddings together to pack the information in a single tensor\n",
    "    embeddings = tokenEmbeddings + positionalEmbeddings # (B, T, C)\n",
    "    # Pass the concatenated embeddings into our multihead attention\n",
    "    embeddings = self.selfAttentionHeads(embeddings) # (B, T, C)\n",
    "    # Forward through the feed forward network\n",
    "    embeddings = self.feedforwardnetwork(embeddings) # (B, T, C)\n",
    "    # Pass the embeddings through a linear layer\n",
    "    logits = self.languageModelingHead(embeddings) # (B, T, C)\n",
    "```\n",
    "To this:\n",
    "```python\n",
    "# Forward Pass\n",
    "def forward(self, indeces, labels=None):\n",
    "    # Unpacking the shape of indeces\n",
    "    batch, time = indeces.shape\n",
    "\n",
    "    # Index into embeddings to get the token embeddings\n",
    "    tokenEmbeddings = self.tokenEmbeddingTable(indeces) # (B, T, C)\n",
    "    # Index into embeddings to get the positional embeddings\n",
    "    positionalEmbeddings = self.positionalEmbeddingTable(torch.arange(time, device=device)) # (T, C)\n",
    "    # Fuse the token embeddings and positional embeddings together to pack the information in a single tensor\n",
    "    embeddings = tokenEmbeddings + positionalEmbeddings # (B, T, C)\n",
    "    # Pass the concatenated embeddings into our blocks\n",
    "    embeddings = self.blocks(embeddings) # (B, T, C)\n",
    "    # Pass the embeddings through a linear layer\n",
    "    logits = self.languageModelingHead(embeddings) # (B, T, C)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc100e-bd8a-454a-accd-ca2c455741ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Changing Script to `transformer_block_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd26759-f5ae-40bb-87b2-1fccd70d7202",
   "metadata": {},
   "source": [
    "Let's now implement this knowledge of replecating `TransformerBlock`s into a `transformer_block_gpt.py` script file..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1281a2-de75-41f2-86f1-8b6d737f575a",
   "metadata": {},
   "source": [
    "I will not keep the scores of the loss this time, because we still don't get a pretty good result and I have run it myself already...\n",
    "\n",
    "And the reason for that is, we are actually starting to get into a very deep neural network now...\n",
    "\n",
    "And deep neural networks suffer from optimization issues, and that's what we are getting to run into...\n",
    "\n",
    "So we need two more ideas that we can borrow from our original <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> Paper to resolve those difficulties...\n",
    "\n",
    "Those are:\n",
    "1. Residual Connections/Skip Connections\n",
    "2. Layer Normalization\n",
    "\n",
    "So let's now discuss the ideas..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f2425-6103-40e4-b6e8-484b23e081cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding **Skip Connections / Residual Connections**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254299a-9fa1-4f14-a6d3-fce1ae140fa7",
   "metadata": {},
   "source": [
    "Let's look at the `Transformer` architecture again where I have highlighted some parts...\n",
    "![Transformer_Model_Architecture_Skip_Connections](ExplanationMedia/Images/Transformer_Model_Architecture_Skip_Connections.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72903bbc-aed8-4a50-b5a1-cdc66e1a02d8",
   "metadata": {},
   "source": [
    "See the *red coloured* parts that I have highlighted and how they **skip** some of the smaller blocks?\n",
    "\n",
    "Those are known as **Skip Connections** or **Residual Connections**. And they come from the paper <a href=\"https://arxiv.org/abs/1512.03385\">Deep Residual Learning for Image Recognition</a> from about 2015, which we have already seen in our **NameWeave** series...\n",
    "\n",
    "Let's understand what they do..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de40303-06eb-4701-99f8-33cbd130342c",
   "metadata": {},
   "source": [
    "![Residual Block](ExplanationMedia/Images/ResidualBlock.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8b053-4f54-4aeb-8649-09f885e9f3e3",
   "metadata": {},
   "source": [
    "**Skip Connections** or **Residual Connections** transform the data we get from our `inputs`, but then we have a *skip connection* with addition with previous features...\n",
    "\n",
    "Now the way I like to visualize it, is the following..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9facd2-2cb3-4a1d-972c-76556e24b16b",
   "metadata": {},
   "source": [
    "![Residual Blocks](ExplanationMedia/Images/ResidualBlocks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a3e12-b4a0-49c7-a796-2e5b0a6cdfd2",
   "metadata": {},
   "source": [
    "In **Residual Connections** the *computation* happens from the top to bottom which is a residual pathway, and we are free to *fork off* from residual pathway, perform some other *computation* and then **project** back to the residual pathway via an addition...\n",
    "\n",
    "Which means we go from `inputs` to the targets only via these chains of additions...\n",
    "\n",
    "Why is this useful?\n",
    "\n",
    "The reason it is useful is because, if we remember from our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/Neural%20Network%20with%20Derivatives.ipynb\">Neural Network with Derivatives</a> notebook that, addition distributes gradients equally to all of its branches that got fed as the input, which means that the supervision or the gradients from the loss hop from every addition node all the way to the `inputs` and then also *fork off* from the residual blocks...\n",
    "\n",
    "But basically we have this gradient *super-highway* that goes directly from the supervision, all the way to the `inputs`, unimpeded.\n",
    "\n",
    "And these **residual blocks** are usually initialized in the beginning so that they contribute very very little to the residual pathway, but then during the optimization, they *come online* over time and they start to contribute... \n",
    "\n",
    "But at least at the initialization we can go from directly from the supervision to the `inputs` where gradients are unimpeded and they just flow, and then the blocks over time, kick in...\n",
    "\n",
    "Which dramatically helps with the optimization...\n",
    "\n",
    "So let's modify our code to have **Residual Connections** now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188a7b3-1c5d-410d-8940-0915e19c390d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Changing Script to `transformer_residual_block_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0964c-1947-4c35-8739-ebb4984c6928",
   "metadata": {},
   "source": [
    "Right now, we understand that the skip connections we saw were in the white box, which means that we want to modify our `TransformerBlock` such that it contains the residual connection itself...\n",
    "\n",
    "Let's look at the old `TransformerBlock` code that we have already:\n",
    "```python\n",
    "# Transformer Block Module Definition\r\n",
    "class TransformerBlock(torch.nn.Module):\r\n",
    "    \"\"\" Communication Followed By Computation \"\"\"\r\n",
    "    # Constructor for the Transformer Block \r\n",
    "    def __init__(self, numberOfEmbeddingDimensions, numberOfHeads):\r\n",
    "        # Initializing the Transformer Block\r\n",
    "        super().__init__()\r\n",
    "        self.selfAttention = MultiHeadAttention(numberOfHeads=numberOfHeads, headSize=numberOfEmbeddingDimensions//numberOfHeads)\r\n",
    "        self.feedforwardnetwork = FeedForward(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions)\r\n",
    "\r\n",
    "    # Forward Pass\r\n",
    "    def forward(self, inputs):\r\n",
    "        embeddings = self.selfAttention(inputs) # (B, T, C)\r\n",
    "        embeddings = self.feedforwardnetwork(embeddings) # (B, T, C)\r\n",
    "        return embeddings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ade8e-f098-4910-9562-e812ca666145",
   "metadata": {},
   "source": [
    "Now, from the previous explanation we understand that in **residual connections** we have the common `inputs` node, which branch off and do something and add up after the work is done...\n",
    "\n",
    "Which means that we want to modify our forward pass of this block such that:\n",
    "1. The inputs for the `selfAttention` do some *communication* and add themselves back to the `embeddings`\n",
    "2. The inputs for the `feedforwardnetwork` do some *computation* and add themselves back to the `embeddings`\n",
    "\n",
    "So our new forward pass looks like:\n",
    "```python\n",
    "# Forward Pass\r",
    " def forward(self, embeddings):\r\n",
    "    embeddings = embeddings + self.selfAttention(embeddings) # (B, T, C)\r\n",
    "    embeddings = embeddings + self.feedforwardnetwork(embeddings) # (B, T, C)\r\n",
    "    return embeddings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e39428-c823-436d-a6cb-bda36bf42730",
   "metadata": {},
   "source": [
    "Now our residual connection provides a shortcut for the gradients to bypass layers during backpropagation, which helps to alleviate the vanishing gradient problem...\n",
    "\n",
    "But now because we have used the residual connections in both `MultiHeadAttention` and `FeedForward` modules that we have created, we need to modify their respective initializations and forward passes as well...\n",
    "\n",
    "Let's consider the `MultiHeadAttention` first...\n",
    "\n",
    "Right now, our `MultiHeadAttention` looks like:\n",
    "```python\n",
    "# Multi-Head Attention Module Definiton\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\" Multiple Heads of Self Attention in Parallel \"\"\"\n",
    "    # Constructor for the Multi-Head Attention\n",
    "    def __init__(self, numberOfHeads, headSize):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([Head(headSize=headSize) for _ in range(numberOfHeads)])\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        # Returns the concatenated heads over the channel dimension\n",
    "        return torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "```\n",
    "Now we see that there are multiple heads of `Heads` and they end up gathering *information* with each other and we have a concatenation operation that combines all the *information* together, but now, because they gather and keep adding up more and more *information*, the concatenation increases the dimensionality of the output. Which means that we now need a way to reduce the dimensionality such that the outputs of all these heads are ready for the next stage, that is `Feed Forward`. And the easiest way to do that is to add a `Linear` layer, which we generally call `projection` when using `Transformers`...\n",
    "\n",
    "So after adding our `projection` linear transformation to our `MultiHeadAttention` our code looks like:\n",
    "```python\n",
    "# Multi-Head Attention Module Definiton\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\" Multiple Heads of Self Attention in Parallel \"\"\"\n",
    "    # Constructor for the Multi-Head Attention\n",
    "    def __init__(self, numberOfHeads, headSize):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([Head(headSize=headSize) for _ in range(numberOfHeads)])\n",
    "        self.projection = torch.nn.Linear(numberOfEmbeddingDimensions, numberOfEmbeddingDimensions)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        # Returns the concatenated heads over the channel dimension\n",
    "        output = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "        output = self.projection(inputs)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4533a-53b0-4da7-8b88-52b420bfa272",
   "metadata": {},
   "source": [
    "Now to change the `Feed Forward` part, we understand that we can do the exact same thing with our `Feed Forward` and apply a projection...\n",
    "\n",
    "This time just for simplicity, rather than having a `projection` variable, we can couple this projection into our `Sequential` layers itself, so that we don't need to change the forward pass of this Module...\n",
    "\n",
    "So previously our `FeedForward` looked like:\n",
    "```python\n",
    "# Feed Forward Module Definition\n",
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\" Simple Feed Forward Network \"\"\"\n",
    "    # Constructor for the Feed Forward Network\n",
    "    def __init__(self, numberOfEmbeddingDimensions):\n",
    "        # Initializing the layers\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(numberOfEmbeddingDimensions, numberOfEmbeddingDimensions),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        return self.network(inputs)\n",
    "```\n",
    "But now, after applying the `projection` linear transformation our `FeedForward` looks like:\n",
    "```python\n",
    "# Feed Forward Module Definition\n",
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\" Simple Feed Forward Network \"\"\"\n",
    "    # Constructor for the Feed Forward Network\n",
    "    def __init__(self, numberOfEmbeddingDimensions):\n",
    "        # Initializing the layers\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(numberOfEmbeddingDimensions, numberOfEmbeddingDimensions),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(numberOfEmbeddingDimensions, numberOfEmbeddingDimensions),\n",
    "        )\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        return self.network(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc1abd-2211-4a2f-a439-8a9292016302",
   "metadata": {},
   "source": [
    "There's one more thing that we need to take care about... That is, in the original <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> paper, we see that there is a section called *\"Position-wise Feed-Forward Networks\"*, where there is a section written:\n",
    "\n",
    "*The dimensionality of input and output is $d_{model} = 512$, and the inner-layer has dimensionality $d_{ff}=2048$*\n",
    "\n",
    "And we see that the layers have the dimensionality that is `4` times the original input and output, so we can apply the inner-layer dimensionality to our `FeedForward` now as well...\n",
    "\n",
    "So now our `FeedForward` after increasing the inner-layer dimensionality by `4` times looks like:\n",
    "```python\n",
    "# Feed Forward Module Definition\r\n",
    "class FeedForward(torch.nn.Module):\r\n",
    "    \"\"\" Simple Feed Forward Network \"\"\"\r\n",
    "    # Constructor for the Feed Forward Network\r\n",
    "    def __init__(self, numberOfEmbeddingDimensions):\r\n",
    "        # Initializing the layers\r\n",
    "        super().__init__()\r\n",
    "        self.network = torch.nn.Sequential(\r\n",
    "            torch.nn.Linear(numberOfEmbeddingDimensions, 4 * numberOfEmbeddingDimensions),\r\n",
    "            torch.nn.ReLU(),\r\n",
    "            torch.nn.Linear(4 * numberOfEmbeddingDimensions, numberOfEmbeddingDimensions),\r\n",
    "        )\r\n",
    "\r\n",
    "    # Forward Pass\r\n",
    "    def forward(self, inputs):\r\n",
    "        return self.network(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ba2b9-f772-4fb6-a0ec-94fcdb7256ea",
   "metadata": {},
   "source": [
    "So we're adding a bit of *computation* here and growing that layer, that is on the *residual block* on the side of the main *residual pathway*...\n",
    "\n",
    "So we can now make these changes to our script `transformer_residual_block_gpt.py` and run it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fce2ed-bf41-4196-a9dc-395be4732ba7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding **Layer Normalization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fa2e112-57c5-46bc-979b-2ba8735ab51e",
   "metadata": {},
   "source": [
    "Now, the second innovation that is very helpful that in optimizing very deep neural networks is right here...\n",
    "\n",
    "Now in the original `Transformer` illustration we keep seeing this block:\\\n",
    "![AddNorm](ExplanationMedia/Images/Transformer_Model_AddNorm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578e10e-f0c2-42f5-ac6d-c7a56cf11913",
   "metadata": {},
   "source": [
    "We understand that we have already implemented this `Add` part from our previous **Residual Connections** but there is still a part called `Norm` left...\n",
    "\n",
    "And this `Norm` refers to something called <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">LayerNorm</a> which is based on a paper that came out in 2016 as <a href=\"https://arxiv.org/abs/1607.06450\">\"Layer Normalization\"</a>..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af43d72-435e-47b4-895d-18c0695f8354",
   "metadata": {},
   "source": [
    "And `LayerNorm` is very very similar to `BatchNorm`...\n",
    "\n",
    "And now if we remember our old <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a>, we implemented `Batch Normalization`, and batch normalization made sure that across the *batch-dimension* any individual neuron had *unit-gaussian* distribution (`0` mean and `unit` standard deviation output)...\n",
    "\n",
    "Let me recall the code that we had before:\n",
    "```python\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        \n",
    "        self.training = training\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        if self.training:\n",
    "            input_mean = inputs.mean(0, keepdim=True)\n",
    "            input_variance = inputs.var(0, keepdim=True)\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # Update Buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef118e-d27c-4cb7-ab01-532f90ebf45d",
   "metadata": {},
   "source": [
    "Let's test this block out as well...\n",
    "\n",
    "For example, if we take `inputs` of a batch of `32` items having `100` dimensional vectors, and we pass them through the batch normalization layer like this:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "batchNorm = BatchNorm1d(100)\n",
    "inputs = torch.randn(32, 100) # Batch of 32 items with 100 dimensional vectors\n",
    "outputs = batchNorm(inputs)\n",
    "\n",
    "print(f\"Mean: {outputs[:, 0].mean()} and Standard Deviation: {outputs[:, 0].std()} (Column Normalization)\")\n",
    "print(f\"Mean: {outputs[0, :].mean()} and Standard Deviation: {outputs[0, :].std()} (Row Normalization)\")\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "Mean: 1.0009855031967163e-05 and Standard Deviation: 1.0 (Column Normalization)\n",
    "Mean: -0.09684808552265167 and Standard Deviation: 1.0564608573913574 (Row Normalization)\n",
    "```\n",
    "Which means that it ensures `0` mean and `1` standard deviation by normalizing every single column of inputs by default...\n",
    "\n",
    "But it does not normalize the rows by default, because we are just normalizing columns...\n",
    "\n",
    "Let's now implement `LayerNorm` based on our `BatchNorm`...\n",
    "\n",
    "And it is nothing but, normalizing the **rows** of the `inputs`, which means that we can change our old code:\n",
    "```python\n",
    "input_mean = inputs.mean(0, keepdim=True)\n",
    "input_variance = inputs.var(0, keepdim=True)\n",
    "```\n",
    "To this:\n",
    "```python\n",
    "input_mean = inputs.mean(1, keepdim=True)\n",
    "input_variance = inputs.var(1, keepdim=True)\n",
    "```\n",
    "\n",
    "And done...😂 We have now implemented `LayerNorm`...\n",
    "\n",
    "So if we run the same example now...\n",
    "\n",
    "We get:\n",
    "```python\n",
    "Mean: 0.03100830502808094 and Standard Deviation: 0.9175457954406738 (Column Normalization)\n",
    "Mean: 1.003980651148595e-05 and Standard Deviation: 1.0 (Row Normalization)\n",
    "```\n",
    "We see that we are now normalizing **rows** by default..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c759e83-4a22-4cde-9da0-6082c35c66d9",
   "metadata": {},
   "source": [
    "Now, because our *computation* is **not on across examples** we can remove all the things that are related to buffers and there is no distinction between training and test time...\n",
    "\n",
    "And now our we have our implemented `LayerNorm`:\n",
    "```python\n",
    "class LayerNorm():\n",
    "    def __init__(self, dimensions, epsilon=1e-5):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        input_mean = inputs.mean(1, keepdim=True)\n",
    "        input_variance = inputs.var(1, keepdim=True)\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ec658-f691-4de7-9024-a30613b6c7df",
   "metadata": {},
   "source": [
    "Which is also identical to the original PyTorch's <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\">LayerNorm</a>...\n",
    "\n",
    "So let's now implement `LayerNorm` within our transformer..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a646be-e5df-46c8-bc17-b478482b72a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Changing Script to `transformer_layernorm_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4813fb-fbef-4a4d-a766-5e1a867bd41b",
   "metadata": {},
   "source": [
    "Now before we implement the `LayerNorm` we need to discuss one more thing...\n",
    "\n",
    "In the last `5` years, a very few things have changed from the original `Transformer` architecture and this is what departs from the original implementation of the `Transformer` architecture...\n",
    "\n",
    "That is this part:\\\n",
    "![AddNorm](ExplanationMedia/Images/Transformer_Model_AddNorm.png)\n",
    "\n",
    "We see that the `Add & Norm` is applied after the `MultiHeadAttention`'s transformation and `FeedForward`'s transformation, but now it is a bit more common that the `Add & Norm` is applied before the `MultiHeadAttention`'s transformation and `FeedForward`'s transformation, which means that there's a reshuffling of the `LayerNorm`s...\n",
    "\n",
    "And this is called the **Pre-Norm Formulation** and this is the one that we're going to implement as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37820c8e-aefa-41b7-b061-a54f6f59c9bb",
   "metadata": {},
   "source": [
    "Now because the `LayerNorm` is already there in PyTorch, we can directly use the `TORCH.NN`'s `LayerNorm` implementation to keep the code simpler...\n",
    "\n",
    "And we also understand that because these `Add & Norm` are within the white block of the original `Transformer` implementation, we can modify our `TransformerBlock` during initializaiton to add these `LayerNorm`s and during forward pass to apply the normalizations right before the inputs go in for their original transformation...\n",
    "\n",
    "And because our `inputs` to these `LayerNorm` layers are in the shape of `(B, T, C)`, the `batch` and the `time` dimensions are treated as **batch-dimensions**, which means that we could specify `numberOfEmbeddingDimensions` directly as the normalized shape parameter which for our case is `32`...\n",
    "\n",
    "Which means that this is a **per-token** transformation that normalizes the features and makes them unit gaussian at initialization...\n",
    "\n",
    "Now because these `LayerNorm`s have `gamma` and `beta` trainable parameters, it will eventually create outputs that might not be unit gaussian, but the optimization will determine that...\n",
    "\n",
    "So now our `TransformerBlock` looks like:\n",
    "```python\n",
    "# Transformer Block Module Definition\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    \"\"\" Communication Followed By Computation \"\"\"\n",
    "    # Constructor for the Transformer Block \n",
    "    def __init__(self, numberOfEmbeddingDimensions, numberOfHeads):\n",
    "        # Initializing the Transformer Block\n",
    "        super().__init__()\n",
    "        self.selfAttention = MultiHeadAttention(numberOfHeads=numberOfHeads, headSize=numberOfEmbeddingDimensions//numberOfHeads)\n",
    "        self.feedforwardnetwork = FeedForward(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions)\n",
    "        self.selfAttentionLayerNorm = torch.nn.LayerNorm(numberOfEmbeddingDimensions)\n",
    "        self.feedforwardnetworkLayerNorm = torch.nn.LayerNorm(numberOfEmbeddingDimensions)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings + self.selfAttention(self.selfAttentionLayerNorm(embeddings)) # (B, T, C)\n",
    "        embeddings = embeddings + self.feedforwardnetwork(self.feedforwardnetworkLayerNorm(embeddings)) # (B, T, C)\n",
    "        return embeddings\n",
    "```\n",
    "\n",
    "Now we need to add a `LayerNorm` at the end of the model's layer and right before the final `Linear` transformation to address normalization and vanishing gradient issues...\n",
    "\n",
    "So our `GPTModel`s initialization looks like:\n",
    "```python\n",
    "# Constructor for the model\n",
    "def __init__(self):\n",
    "    # Initializing the model parameters\n",
    "    super().__init__()\n",
    "    self.tokenEmbeddingTable = torch.nn.Embedding(vocabularySize, numberOfEmbeddingDimensions)\n",
    "    self.positionalEmbeddingTable = torch.nn.Embedding(blockSize, numberOfEmbeddingDimensions)\n",
    "    self.blocks = torch.nn.Sequential(\n",
    "        TransformerBlock(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions, numberOfHeads=numberOfHeads),\n",
    "        TransformerBlock(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions, numberOfHeads=numberOfHeads),\n",
    "        TransformerBlock(numberOfEmbeddingDimensions=numberOfEmbeddingDimensions, numberOfHeads=numberOfHeads),\n",
    "        torch.nn.LayerNorm(numberOfEmbeddingDimensions),\n",
    "    )\n",
    "    self.languageModelingHead = torch.nn.Linear(numberOfEmbeddingDimensions, vocabularySize)\n",
    "```\n",
    "\n",
    "So you can feel free to train the model, for now I will release this in a script called `transformer_layernorm_gpt.py`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d3566-3dc6-483b-8dc4-3b041b26338c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding **Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42094177-60cd-499d-b3ad-47cf28f1e0b5",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\">**Dropout**</a> is a concept that comes from <a href=\"https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> from around 2014...\n",
    "\n",
    "And let's look at the paper's illustration to understand **Dropout** now...\n",
    "\n",
    "![Dropout](ExplanationMedia/Images/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bbc77-68d4-4911-a484-f21d4084944f",
   "metadata": {},
   "source": [
    "Basically it takes your neural network and every forward and backward pass **randomly shuts off some subset of neurons**...\n",
    "![Dropout Animation](ExplanationMedia/Images/dropout.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b73e7d-4e87-4e2a-8a91-c882f1185886",
   "metadata": {},
   "source": [
    "And what this does effectively is, because the mask of what being dropped out is changed every single forward and backward pass, it kind of ends up traning an ensemble of **sub networks**, and during test time, everything is fully enabled and all the sub networks are merged into a single ensemble (kind of)...\n",
    "\n",
    "And so I invite you to read the paper, but for now let's stay on the overview level of **dropout** just being a regularization technique (technique that prevents overfitting and underfitting)...\n",
    "\n",
    "So now we can sprinkle dropout accross our code to our `GPT`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ca59d-5d6c-47b1-ac27-00f31f19735a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Changing Script to `transformer_layernorm_dropout_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293bef4-87f8-4d0e-8c12-627a8fe89ca9",
   "metadata": {},
   "source": [
    "Let's now initialize these dropouts and modify the forward passes of the parts where we want out dropouts...\n",
    "\n",
    "Now, **Dropout** needs a probability arguement which by default is `0.5` which means `50%` of the neurons are dropped by default...\n",
    "\n",
    "And just to be safe, we will define the same as a hyper-parameter variable called `dropoutProbability`...\n",
    "\n",
    "Now, because our entire `GPTModel` consists the core modules `Head`, `MultiHeadAttention` and `FeedForward`, we can sprinkle `Dropout`s on these layers:\n",
    "by initializing them as:\n",
    "```python\n",
    "self.dropout = torch.nn.Dropout(p=dropoutProbability)\n",
    "```\n",
    "\n",
    "And during forward pass we can use `Dropout` in the following scenarios (after some of the transformation and non-linearity activation):\n",
    "1. Right before the *residual connection* back into the *residual pathway* in `FeedForward`\n",
    "2. Right at the end of *communication* of `MultiHeadAttention`\n",
    "3. Right after the `softmax()` of `Head` to randomly prevent some `tokens` from *communicating*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4dc91-927c-47ce-9b84-a0807b86dc53",
   "metadata": {},
   "source": [
    "So now the changes look like:\n",
    "1. ```python\n",
    "   # Feed Forward Module Definition\n",
    "    class FeedForward(torch.nn.Module):\n",
    "        \"\"\" Simple Feed Forward Network \"\"\"\n",
    "        # Constructor for the Feed Forward Network\n",
    "        def __init__(self, numberOfEmbeddingDimensions):\n",
    "            # Initializing the layers\n",
    "            super().__init__()\n",
    "            self.network = torch.nn.Sequential(\n",
    "                torch.nn.Linear(numberOfEmbeddingDimensions, 4 * numberOfEmbeddingDimensions),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(4 * numberOfEmbeddingDimensions, numberOfEmbeddingDimensions),\n",
    "                torch.nn.Dropout(p=dropoutProbability)\n",
    "            )\n",
    "    \n",
    "        # Forward Pass\n",
    "        def forward(self, inputs):\n",
    "            return self.network(inputs)\n",
    "   ```\n",
    "2. ```python\n",
    "   # Multi-Head Attention Module Definiton\n",
    "    class MultiHeadAttention(torch.nn.Module):\n",
    "        \"\"\" Multiple Heads of Self Attention in Parallel \"\"\"\n",
    "        # Constructor for the Multi-Head Attention\n",
    "        def __init__(self, numberOfHeads, headSize):\n",
    "            super().__init__()\n",
    "            self.heads = torch.nn.ModuleList([Head(headSize=headSize) for _ in range(numberOfHeads)])\n",
    "            self.projection = torch.nn.Linear(numberOfEmbeddingDimensions, numberOfEmbeddingDimensions)\n",
    "            self.dropout = torch.nn.Dropout(p=dropoutProbability)\n",
    "    \n",
    "        # Forward Pass\n",
    "        def forward(self, inputs):\n",
    "            # Returns the concatenated heads over the channel dimension\n",
    "            output = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "            output = self.dropout(self.projection(inputs))\n",
    "            return output\n",
    "   ```\n",
    "3. ```python\n",
    "   # Head Module Definiton\n",
    "    class Head(torch.nn.Module):\n",
    "        \"\"\" Single Head of Self Attention \"\"\"\n",
    "        # Constructor for the Head\n",
    "        def __init__(self, headSize):\n",
    "            super().__init__()\n",
    "            self.key = torch.nn.Linear(numberOfEmbeddingDimensions, headSize, bias=False)\n",
    "            self.query = torch.nn.Linear(numberOfEmbeddingDimensions, headSize, bias=False)\n",
    "            self.value = torch.nn.Linear(numberOfEmbeddingDimensions, headSize, bias=False)\n",
    "            self.register_buffer(name='lowerTriangularMatrix', tensor=torch.tril(torch.ones(blockSize, blockSize)))\n",
    "            self.dropout = torch.nn.Dropout(p=dropoutProbability)\n",
    "    \n",
    "        # Forward Pass\n",
    "        def forward(self, inputs):\n",
    "            # Unpacking the shape of inputs\n",
    "            batch, time, channel = inputs.shape\n",
    "            # Forwarding the inputs to keys and queries\n",
    "            k = self.key(inputs) # (B, T, C)\n",
    "            q = self.query(inputs) # (B, T, C)\n",
    "            # Initializing weights with scaled dot product\n",
    "            weights = q @ k.transpose(-2, -1) * headSize ** -0.5 # (B, T, T)\n",
    "            # Masking the weights\n",
    "            weights = weights.masked_fill(self.lowerTriangularMatrix[:time, :time] == 0, float('-inf')) # (B, T, T)\n",
    "            # Softmax the weights\n",
    "            weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "            weights = self.dropout(weights)\n",
    "            # Forwarding the inputs to values\n",
    "            v = self.value(inputs) # (B, T, C)\n",
    "            # Aggregating the weights and the values\n",
    "            output = weights @ v # (B, T, C)\n",
    "            return output\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b986d1-d54b-4adb-ae31-d3941e61429b",
   "metadata": {},
   "source": [
    "For now I will release this in a script called `transformer_layernorm_dropout_gpt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a821b8-72cf-4eb9-874b-9af6a41d62b3",
   "metadata": {},
   "source": [
    "# Scaling Up the GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656fe93e-bfa7-440f-ae6e-67e7f5e00b4d",
   "metadata": {},
   "source": [
    "We see that we have a pretty complete `Transformer` architecture now and we understand almost every component...\n",
    "![Transformer_With_Block_Explanation](ExplanationMedia/Images/Transformer_With_Block_Explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8d782-b813-463b-9d01-8c6d5ed511df",
   "metadata": {},
   "source": [
    "We can finally do some cosmetic changes and start scaling our `GPTModel` and get better results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79372e6b-4f22-4983-ad66-fcdfc6658175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c0d1d-d295-4d6f-b8b1-3b6c6f961668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3418076-5d76-47eb-bcfa-37331bc9bc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324b0b8-0c3a-47ff-90a8-e306041b42d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84abfe44-9f88-41aa-bd56-90d8e775d466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9450d9-38d7-4acf-aeb7-d1bab524127d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
