{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9b417b-c07f-4ff7-bf54-dd89f05626ba",
   "metadata": {},
   "source": [
    "# Welcome to GPT from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35e52d-ea1b-4e9a-89ee-55b892e9d40f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Things to discuss before starting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b496593-73b0-4847-9abc-6a5d19e67d9e",
   "metadata": {},
   "source": [
    "This notebook is a continuation of my previous notebooks in order:\n",
    "1. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/Neural%20Network%20with%20Derivatives.ipynb\">Neural Networks with Derivatives</a>\n",
    "2. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a>\n",
    "3. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Multi%20Layer%20Perceptron.ipynb\">NameWeave - Multi Layer Perceptron</a>\n",
    "4. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a>\n",
    "5. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20Manual%20Back%20Propagation.ipynb\">NameWeave - Manual Back Propagation</a>\n",
    "6. <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20WaveNet.ipynb\">NameWeave - WaveNet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b91d8-8d4b-498f-8932-8c8b8506087e",
   "metadata": {},
   "source": [
    "Which means, I will be using a lot of the terminologies, explanations, and code from the previous notebooks that I have created in the series...\n",
    "\n",
    "And we will gradually build a complete **GPT** from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff5339-c626-48e0-9135-cb81cfa60d57",
   "metadata": {},
   "source": [
    "This notebook will be a little bit different from the previous notebooks that I have created previously and I will discuss the changes in a bit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a5bb2-04a6-4442-ba16-9f36a2b2eafc",
   "metadata": {},
   "source": [
    "We will use a completely new dataset `Harry_Potter_Books.txt` which is a combined raw text of all the Harry Potter books by J. K. Rowling combined into a single `text` file, instead of `Indian_Names.txt` which we have used in the previous notebooks that contained Indian Firstnames crawled from a website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a01b1-4c47-4a51-8e74-33fb867f41f2",
   "metadata": {},
   "source": [
    "We will also keep softwares like <a href=\"https://chat.openai.com/\">ChatGPT</a>, <a href=\"https://gemini.google.com/app\">Google Gemini</a>, and other Large Language Models (LLM's) in mind and create our own little **Generative Pre-Trained Transformer (GPT)**...\n",
    "\n",
    "And you would probably know by now what these models are and what they do...\n",
    "\n",
    "Our **GPT** is going to be a *character level language model*, instead of a *sub-word-tokenized model* which softwares like ChatGPT and Google Gemini use in their models, and we will discuss everything in a bit...\n",
    "\n",
    "And we will not write all the `TORCH.NN` modules from scratch now, because we have already covered the most important ones already in our previous notebooks, instead we will implement the networks based on `TORCH.NN` library from PyTorch...\n",
    "\n",
    "And most importantly we will start from the simplest model (Bigram Model) and modify the same model within the same notebook and make our way upto the entire `Transformer` architecture...\n",
    "\n",
    "I have created an entire folder as `GPT Scripts` in the root of this repository to save each script for you to run them without even having to use jupyter notebooks. Rather you can simple use the `<filename>.py` to run each model to see how they perform on the same dataset as we move up, using:\n",
    "```bash\n",
    "python <filename>.py\n",
    "```\n",
    "\n",
    "One more thing to discuss is we remember from our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20-%20WaveNet.ipynb\">NameWeave - WaveNet</a> notebook that we refered to multiple dimensions in a tensor as our own made up names, but the thing to know is that in real world these multiple dimensions have names, specifically **batch**, **time** and **channel** dimensions like this in order:\n",
    "\n",
    "$$\n",
    "(B, T, C)\\rightarrow(Batch, Time, Channel)\n",
    "$$\n",
    "\n",
    "And we will refer to these dimensions using the actual names used in real world this time...\n",
    "\n",
    "So, it's going to me a long journey and it if going to be legen...wait-for-it...dary. Legendary!!!\\\n",
    "![Barney Stinson Wink](https://media.tenor.com/nJ3EeUPhVKkAAAAM/barny-stinson.gif)\n",
    "\n",
    "So let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b5a45-f476-4f09-a2dd-ecef880a9205",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Understanding GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d72da0-a8c5-4180-bdf4-3bedef669acd",
   "metadata": {},
   "source": [
    "So what is a **GPT**?\n",
    "\n",
    "Well, **GPT** expands for the terminology as **Generative Pre-Trained Transformer**.\n",
    "\n",
    "You see how it consists of three words?\n",
    "\n",
    "Let's look at it in context of each word:\n",
    "1. Generative → Generates New Content\n",
    "2. Pre-Trained → Pre-trained on a dataset\n",
    "3. Transformer → Transformer architecture is being followed to make up the model (Don't worry we will discuss this later)\n",
    "\n",
    "That was easy..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1d95f-acec-487c-84f0-07262f7b7274",
   "metadata": {},
   "source": [
    "Let's understand what **GPT** can do currently...\n",
    "\n",
    "Let's take `ChatGPT` as an example as of now and let's see it's capabilities...\n",
    "\n",
    "![ChatGPT Current Capabilities](https://miro.medium.com/v2/resize:fit:679/1*_3AM0Yhc7qgCvZ_X1L8mhw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40702e51-6551-4716-8635-83d3a6b6ab12",
   "metadata": {},
   "source": [
    "We see that `GPT` goes from left to right and generates text sequentially..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86d794-f0e4-4a1a-831c-a2ff1e6657de",
   "metadata": {},
   "source": [
    "I wanted to show another thing:\n",
    "![ChatGPT Different Responses](ExplanationMedia/Images/ChatGPT_Different_Response.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d7c99-fc6d-4bbc-b687-b2737aa06840",
   "metadata": {},
   "source": [
    "See how we get a different response each time?\n",
    "\n",
    "Which hints us that it is more like a probabilistic system, which is for any one `prompt` it can give us multiple answers..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307442b-c05f-4618-8094-6201513200c6",
   "metadata": {},
   "source": [
    "Now, this is just one example of a `prompt`, and people have come up with billions of different prompts as of now, and in fact there are many websites that index the interactions with `ChatGPT` as well.\n",
    "\n",
    "You can look at this <a href=\"https://writesonic.com/blog/best-chatgpt-examples\">website</a> as an example.\n",
    "\n",
    "We see that it is a very remarkable system, and it is what we call a **\"Language Model\"**.\n",
    "\n",
    "Or in other words, it models the sequence of words or characters (or \"tokens\" more generally) and it knows how words follow each other in English language (even other languages)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2eefa5-76b9-452d-93c5-3817d91de573",
   "metadata": {},
   "source": [
    "Let's understand what **GPT** does from it's perspective...\n",
    "\n",
    "Well it is trying to complete the sequence...\n",
    "\n",
    "In other words, the `inputs` or `task` that we give to the GPT model, it treats it as a *start of a sequence* and it tries to complete the sequence as a whole. Which makes it a language model in this sense...\n",
    "\n",
    "You would think that it is utterly ridiculous and that we cannot just model an entire architecture and make it act like a helpful assistant.\n",
    "\n",
    "Well that is the beauty of it. And we will discuss all the under-the-hood components of what makes a software like `ChatGPT` work.\n",
    "\n",
    "So, What is the neural network architecture under-the-hood that models this sequence of words/characters/tokens?\n",
    "\n",
    "That comes from this paper from Google: <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> from 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bfc2f-df64-4d23-9859-3fb78cce4dcd",
   "metadata": {},
   "source": [
    "This was a landmark paper in Artificial Intelligence that proposed the `Transformer` architecture. But if you start reading this paper, it may seem like a pretty random *machine-translation* paper. And that's because, I think the authors did not fully anticipate the impact it would create in this domain in the years to come...\n",
    "\n",
    "Let's look at the original `Transformer` architecture as of now:\n",
    "![Transformer Archtecture](ExplanationMedia/Images/Transformer_Model_Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffa820-1b0c-4290-b557-1f1719754dfe",
   "metadata": {},
   "source": [
    "And this `Transformer` architecture was copy pasted in huge amount of applications in most recent years...\n",
    "\n",
    "And what we'd like to do now is create something like `ChatGPT`. But we would not be able to completely clone `ChatGPT` because it is a way more serious *production-grade* system which currently requires *thousands* of GPUs and *millions* of dollars to train the network, and also it is trained on a very good *chunk* of internet data. And there are a lot of **pre-training** and **fine-tuning** stages to it.\n",
    "\n",
    "Rather we would like to create a transformer-based language model, and in our case it is going to be a character level language model. And we also don't want to train on a *chunk* of internet, rather we need a smaller dataset (I proposed we work with `Harry_Potter_Books.txt` which is roughly a `7MB` file). And we would try to model how these characters in this dataset, follow each other.\n",
    "\n",
    "Let's take this paragraph for example:\n",
    "```python\n",
    "\"\"\"\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
    "were proud to say that they were perfectly normal, \n",
    "thank you very much.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Given a chunk of these characters in the past:\n",
    "```python\n",
    "\"\"\"\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
    "were proud to say that the\n",
    "\"\"\"\n",
    "```\n",
    "The `Transformer` model will look at these characters as a context in the past, and it is going to predict that the letter `'y'` is likely to come next in the sequence. And it is going to produce (generate) character sequences that look like Harry Potter. And in that process it is going to model all the patterns inside this data.\n",
    "\n",
    "And once we have trained the model, our model will be able to generate *infinite `Harry Potter`*\n",
    "\n",
    "![Harry Potter Woo](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExcjBmNzJ5N2EzMDQzeTB3cXV4ODN5ZGJkdWlldHhleGw3d3hpMGRhMyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/TJO5x5QQM72Q0weWXN/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843798f4-fca5-420d-8caf-4bfea1eb2245",
   "metadata": {},
   "source": [
    "So let's install the required dependencies and load our dataset up and look into the data and what it looks like first..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715def1-2630-4f41-a531-73ce25bbf959",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e06f1d-1362-491a-b516-46f9488e02a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fb2c8-d5ea-4a57-9e1b-a6789e187c66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2270b6-af58-43e5-bbd4-56f13d2f5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74bbb7-345b-4257-852a-0a25b32fda0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030a829-eb7a-4432-a021-738bbfb1f758",
   "metadata": {},
   "source": [
    "This time, I will divide the dataset loading part into two forms:\n",
    "1. If you're trying to use `Google Colab` to run the code\n",
    "2. If you're trying to use `Jupyter Notebook locally` to run the code\n",
    "And you can choose between either one of those with our desired mode..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750cf030-de56-4833-b069-70bb13d5d206",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## For Google Colab users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b9527-67ba-4a64-a77a-55bb42734c98",
   "metadata": {},
   "source": [
    "This will download the `Harry_Potter_Books.txt` into your current folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a81b0b-d3f5-4595-9b8d-d8da1ec4b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/AvishakeAdhikary/Neural-Networks-From-Scratch/main/Datasets/Harry_Potter_Books.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dde7bc-c5c2-4f8f-8f38-3dc419f2c75c",
   "metadata": {},
   "source": [
    "Now you can load up the dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53794f1-26c2-48a7-b282-bff93d6297f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Harry_Potter_Books.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1825e0-4180-4279-a8c0-06b8329efa32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## For local Jupyter Notebook users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623e165-f912-4676-9107-41ea3479c2b0",
   "metadata": {},
   "source": [
    "You don't have to download the dataset if you have the entire repository cloned.\n",
    "\n",
    "The dataset `Harry_Potter_Books.txt` is already located inside the `Datasets` directory...\n",
    "\n",
    "So we can simply open the file and look at its content by specifying the relative path..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70974e14-fe05-4cfb-b794-e6773301ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/Harry_Potter_Books.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431416c3-581d-45e6-a660-228adea1ae5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c635c2-e349-458a-b406-b42edb18c275",
   "metadata": {},
   "source": [
    "We can look at the length of the entire dataset and it's number of characters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20aa4c0-dd36-4178-ae4d-6c287dce74d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset in Characters:  6765190\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Dataset in Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004d6ac-34e8-4d8a-8258-35f93fe646a3",
   "metadata": {},
   "source": [
    "We see that it is roughly `6-million` characters...\n",
    "\n",
    "And if you want to look at the first `1000` characters we can do:\n",
    "```python\n",
    "print(text[:1000])\n",
    "```\n",
    "Which prints the output:\n",
    "```python\n",
    "\"\"\"\n",
    "/ \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "THE BOY WHO LIVED \r\n",
    "\r\n",
    "Mr. and Mrs. Dursley, of number four, Privet Drive, \r\n",
    "were proud to say that they were perfectly normal, \r\n",
    "thank you very much. They were the last people you’d \r\n",
    "expect to be involved in anything strange or \r\n",
    "mysterious, because they just didn’t hold with such \r\n",
    "nonsense. \r\n",
    "\r\n",
    "Mr. Dursley was the director of a firm called \r\n",
    "Grunnings, which made drills. He was a big, beefy \r\n",
    "man with hardly any neck, although he did have a \r\n",
    "very large mustache. Mrs. Dursley was thin and \r\n",
    "blonde and had nearly twice the usual amount of \r\n",
    "neck, which came in very useful as she spent so \r\n",
    "much of her time craning over garden fences, spying \r\n",
    "on the neighbors. The Dursley s had a small son \r\n",
    "called Dudley and in their opinion there was no finer \r\n",
    "boy anywhere. \r\n",
    "\r\n",
    "The Dursleys had everything they wanted, but they \r\n",
    "also had a secret, and their greatest fear was that \r\n",
    "somebody would discover it. They didn’t think they \r\n",
    "could bear it if anyone found out about the Potters. \r\n",
    "Mrs. Potter was Mrs. Dursl\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8478f344-d22f-4895-93d1-08b79c709e2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee4734-5dbc-4f5a-95d1-e25e7f74e268",
   "metadata": {},
   "source": [
    "We can now start building our vocabulary, just like we did in our previous notebooks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff89869c-5e80-489e-a3c5-7c6a7ef58ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '—', '‘', '’', '“', '”', '•', '■', '□']\n",
      "Vocabulary Size: 92\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text))) # Gives us all the characters in the english alphabet, hopefully our dataset has all of them\n",
    "vocabularySize = len(characters) # We define a common vocabulary size\n",
    "print(\"Characters:\", characters)\n",
    "print(\"Vocabulary Size:\", vocabularySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4882e5-a2d1-4d15-9939-372895dc2b39",
   "metadata": {},
   "source": [
    "So we have a possible `vocabulary` of `92` characters that our model will be able to see or emit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efcb11-c293-4b73-8b73-09b979a3c0cf",
   "metadata": {},
   "source": [
    "Now we would like to develop a strategy to <strong><i>tokenize</i></strong> our input `text`.\n",
    "\n",
    "And when we say **tokenize** we generally mean to convert raw text as a string to some sequence of integers according to some vocabulary of possible elements...\n",
    "\n",
    "For us, because we are developing a character level language model, so we are simply going to be translating individual `characters` into `integers`.\n",
    "\n",
    "And we will build `4` things here:\n",
    "1. String to Index Vocabulary → `stoi` → A map of `characters` to `integers`\n",
    "2. Index to String Vocabulary → `itos` → A map of `integers` to `characters`\n",
    "3. Token Encoder → That will encode sequence of characters into indeces\n",
    "4. Token Decoder → That will encode sequence of encoded indeces into characters\n",
    "\n",
    "And you will be able to recognize the first two from our previous notebooks...\n",
    "\n",
    "Before we dive in, let's understand the python concept of `lambda` functions, which some you might have forgotten...\n",
    "\n",
    "So what are `lambda` functions?\n",
    "\n",
    "Python Lambda Functions are *anonymous functions* means that the function is without a name. As we already know the `def` keyword is used to define a normal function in Python. Similarly, the `lambda` keyword is used to define an anonymous function in Python.\n",
    "\n",
    "Syntax: `lambda arguments : expression`\n",
    "\n",
    "For example:\n",
    "```python\n",
    "output = lambda input: input+1\n",
    "print(output(input=1))\n",
    "```\n",
    "We would print `2`.\n",
    "\n",
    "Now, let me first run it, then I will explain it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b1dcd86-01ee-4ff2-9ecd-7366263bced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOI: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '>': 26, '?': 27, 'A': 28, 'B': 29, 'C': 30, 'D': 31, 'E': 32, 'F': 33, 'G': 34, 'H': 35, 'I': 36, 'J': 37, 'K': 38, 'L': 39, 'M': 40, 'N': 41, 'O': 42, 'P': 43, 'Q': 44, 'R': 45, 'S': 46, 'T': 47, 'U': 48, 'V': 49, 'W': 50, 'X': 51, 'Y': 52, 'Z': 53, '\\\\': 54, ']': 55, 'a': 56, 'b': 57, 'c': 58, 'd': 59, 'e': 60, 'f': 61, 'g': 62, 'h': 63, 'i': 64, 'j': 65, 'k': 66, 'l': 67, 'm': 68, 'n': 69, 'o': 70, 'p': 71, 'q': 72, 'r': 73, 's': 74, 't': 75, 'u': 76, 'v': 77, 'w': 78, 'x': 79, 'y': 80, 'z': 81, '|': 82, '~': 83, '—': 84, '‘': 85, '’': 86, '“': 87, '”': 88, '•': 89, '■': 90, '□': 91}\n",
      "ITOS: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '%', 5: '&', 6: \"'\", 7: '(', 8: ')', 9: '*', 10: ',', 11: '-', 12: '.', 13: '/', 14: '0', 15: '1', 16: '2', 17: '3', 18: '4', 19: '5', 20: '6', 21: '7', 22: '8', 23: '9', 24: ':', 25: ';', 26: '>', 27: '?', 28: 'A', 29: 'B', 30: 'C', 31: 'D', 32: 'E', 33: 'F', 34: 'G', 35: 'H', 36: 'I', 37: 'J', 38: 'K', 39: 'L', 40: 'M', 41: 'N', 42: 'O', 43: 'P', 44: 'Q', 45: 'R', 46: 'S', 47: 'T', 48: 'U', 49: 'V', 50: 'W', 51: 'X', 52: 'Y', 53: 'Z', 54: '\\\\', 55: ']', 56: 'a', 57: 'b', 58: 'c', 59: 'd', 60: 'e', 61: 'f', 62: 'g', 63: 'h', 64: 'i', 65: 'j', 66: 'k', 67: 'l', 68: 'm', 69: 'n', 70: 'o', 71: 'p', 72: 'q', 73: 'r', 74: 's', 75: 't', 76: 'u', 77: 'v', 78: 'w', 79: 'x', 80: 'y', 81: 'z', 82: '|', 83: '~', 84: '—', 85: '‘', 86: '’', 87: '“', 88: '”', 89: '•', 90: '■', 91: '□'}\n",
      "Encoded Text:  [39, 60, 62, 60, 69, 59, 56, 73, 80]\n",
      "Decoded Text:  Legendary\n"
     ]
    }
   ],
   "source": [
    "stoi = {character:index for index, character in enumerate(characters)}\n",
    "itos = {index:character for index, character in enumerate(characters)}\n",
    "encode = lambda string: [stoi[character] for character in string] # Token Encoder that takes in a string as an input, and outputs a list of integers\n",
    "decode = lambda list: ''.join([itos[index] for index in list]) # Token Decoder that takes in the encoded list of integers and outputs the decoded string\n",
    "\n",
    "print(\"STOI:\", stoi)\n",
    "print(\"ITOS:\", itos)\n",
    "print(\"Encoded Text: \", encode(\"Legendary\"))\n",
    "print(\"Decoded Text: \", decode(encode(\"Legendary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8b2f0-a80c-49e9-97a9-5965c01139b6",
   "metadata": {},
   "source": [
    "The `Token Encoder` here takes in a `string` or a `sequence of characters` and encodes it into a `list of integers` based on `stoi` mapping. And the `Token Decoder` takes in the encoded `list of integers` and decodes it based on `itos` mapping to get back the exact same string...\n",
    "\n",
    "In other words, it is more like a translation of `characters` into `integers` and `integers` into `characters`, because our model is going to be a character level language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3be15a-b6d6-4607-9bfa-eab54fa8dee6",
   "metadata": {},
   "source": [
    "Now this is only one of many possible `encodings` or `tokenizers` that are out there in the world right now...\n",
    "\n",
    "And people have come up with many such `tokenizers`, for example, Google uses <a href=\"https://github.com/google/sentencepiece\">`sentencepiece`</a>, OpenAI uses <a href=\"https://github.com/openai/tiktoken\">`tiktoken`</a>...\n",
    "\n",
    "And these `tokenizers` which are out there are more like `sub-word` tokenizers, which are **not** encoding `entire words` and also **not** encoding `individual characters`, and more like a `sub-word` unit level `tokenizers` which is usually what's adapted in practice...\n",
    "\n",
    "As an example let's take `tiktoken` vocabulary which uses `Byte-Pair Encoding (BPE)` to encode these `tokens`:\\\n",
    "![Tiktoken Vocabulary](ExplanationMedia/Images/Tiktoken_Vocabulary.png)\n",
    "\n",
    "We see that `tiktoken` has a vocabulary of roughly `50257` which for us is just `92`.\n",
    "\n",
    "And when we try to encode a sample string in `tiktoken`, we get:\\\n",
    "![Tiktoken Example](ExplanationMedia/Images/TikToken_Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b5404-df32-49ae-a752-ff97a9aeb981",
   "metadata": {},
   "source": [
    "We see that we only get `3` outputs for and entire string of `9` characters...\n",
    "\n",
    "Which means that we can *trade-off* `sequences of integers` and `vocabularies`...\n",
    "\n",
    "In other words, we can have a very long `sequences of integers` and very short `vocabularies` or we can have very short `sequences of integers` and very long `vocabularies`...\n",
    "\n",
    "But for now I'd like to keep our `tokenizer` extremely simple using our own character-level tokenizer (meaning we have very small `vocabulary`) and very simple `encode` and `decode` functions, but we do get very long `sequences of integers` as a result...\n",
    "\n",
    "Don't worry, if you'd like I will build a `tokenizer` in the future...\n",
    "\n",
    "So let's now move forward..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27569cc5-193d-4d54-b2e4-798f630390db",
   "metadata": {},
   "source": [
    "Now that we have a `token encoder` and a `token decoder` or effectively a `tokenizer` we can move forward and encode our entire `Harry Potter` dataset...\n",
    "\n",
    "And we will use <a href=\"https://pytorch.org/\">PyTorch</a> library for that:\n",
    "![PyTorch Logo](ExplanationMedia/Images/PyTorchLogo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0b5d2-4800-437b-a6c7-754ce0492e28",
   "metadata": {},
   "source": [
    "So we can now wrap our `text` data after `encoding` it into a `tensor` of datatype `long` because we want *floating-point numbers* to do mathematical transformations on this data later like this:\n",
    "```python\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "783af38b-46c0-4477-ab9f-927fe13527ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff76a2-4be5-4eda-ae6b-5688a1104c94",
   "metadata": {},
   "source": [
    "And then we can check the `shape` and `type` of this data and print out the first `100` characters, just like we did before (without decoding it) like this:\n",
    "```python\n",
    "print(data.shape, data.dtype)\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "torch.Size([6765190]) torch.int64\n",
    "```\n",
    "And:\n",
    "```python\n",
    "print(data[:100])\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "tensor([13,  1,  0,  0,  0,  0,  0, 47, 35, 32,  1, 29, 42, 52,  1, 50, 35, 42,\r\n",
    "         1, 39, 36, 49, 32, 31,  1,  0,  0, 40, 73, 12,  1, 56, 69, 59,  1, 40,\r\n",
    "        73, 74, 12,  1, 31, 76, 73, 74, 67, 60, 80, 10,  1, 70, 61,  1, 69, 76,\r\n",
    "        68, 57, 60, 73,  1, 61, 70, 76, 73, 10,  1, 43, 73, 64, 77, 60, 75,  1,\r\n",
    "        31, 73, 64, 77, 60, 10,  1,  0, 78, 60, 73, 60,  1, 71, 73, 70, 76, 59,\r\n",
    "         1, 75, 70,  1, 74, 56, 80,  1, 75, 63])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bda6f2-cf98-47ee-a401-875c08ad876a",
   "metadata": {},
   "source": [
    "And we see that we have a massive list of integers and is an identical translation of the first `100` characters exactly in the `text` file...\n",
    "\n",
    "And the entire dataset is just stretched out into a very large `sequence of integers`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fbe11-71fd-4baf-8fcf-a06d55d37a0d",
   "metadata": {},
   "source": [
    "Now before we move on with our progress, we would like to do one more thing that is we'd like to split our dataset into a `Train` and `Validation` split...\n",
    "\n",
    "So let's do that..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83671c-4c1f-426e-8a38-6bac731d79c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Splitting the Dataset into `Training` and `Validation` splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59615b21-02d3-4231-9872-8ecf8c038948",
   "metadata": {},
   "source": [
    "Now I'd like to split our data into a split of:\n",
    "1. First 90% into `Training` Split\n",
    "2. Last 10% into `Validation` Split\n",
    "\n",
    "And we are doing this to understand, to what extent our model is `overfitting`...\n",
    "\n",
    "Because we don't want our model to copy and create the exact book of `Harry Potter` instead, we want a model that will create `Harry Potter` like text...\n",
    "\n",
    "And here's how I do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40503ff9-d386-4899-ad3b-3de99801faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintyPercentOfDatasetLength = int((0.9 * len(data)))\n",
    "trainingData = data[:nintyPercentOfDatasetLength] # Data up till 90% of the length\n",
    "validationData = data[nintyPercentOfDatasetLength:] # Data from 90% of the length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5531f0-f95f-4e0e-86dc-447955cfd57f",
   "metadata": {},
   "source": [
    "We can now move on to the next part..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e41fc-c973-498c-a30d-cfb9f8c047e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Data Into Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6399b-5ed0-4b4e-b27e-69c535c78b06",
   "metadata": {},
   "source": [
    "We would like to now proceed to feed these integer sequences into the neural network so that it can train and learn those patterns...\n",
    "\n",
    "**BUT**\n",
    "\n",
    "We need to realise that we are not going to feed in the entire dataset into the neural network because that is going to be extremely computationally heavy, and rather we would load the data into small batches or *chunks* of data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b9b6c-fba0-49f5-9317-5ea8afe9e6c3",
   "metadata": {},
   "source": [
    "Now I typically use the term `block size` and specify a length to it, but this *chunk* of data can be recognized as different terminologies as well, for example `context length`.\n",
    "\n",
    "Let's start with a `blockSize` of just `8`...\n",
    "```python\n",
    "blockSize = 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f5140-2513-4d95-b747-399e00482dfd",
   "metadata": {},
   "source": [
    "Now let's look at the first *chunk* of training data (`blockSize` of `8` + `1`)...\n",
    "\n",
    "I'll explain why this `+1` is there in a bit...\n",
    "\n",
    "So we have:\n",
    "```python\n",
    "trainingData[:blockSize + 1]\n",
    "```\n",
    "For which we get the sequence:\n",
    "```python\n",
    "tensor([13,  1,  0,  0,  0,  0,  0, 47, 35])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e865ff-8b5d-4f85-8fc0-49fce59df9c1",
   "metadata": {},
   "source": [
    "Which is the first `9` characters in a `sequence` in the `training-set`...\n",
    "\n",
    "Now, I'd like to point out that when we take out a *chunk* of data like this, we actually have **multiple examples** packed within it, because all of these characters **follow** each other...\n",
    "\n",
    "And we are going to simultaneously train it at every one of these positions...\n",
    "\n",
    "And in a chunk of `9` characters, there's actually `8` individual examples packed in there...\n",
    "\n",
    "How so?\n",
    "\n",
    "Let's look at it this way...\n",
    "\n",
    "For our example:\n",
    "```python\n",
    "[13,  1,  0,  0,  0,  0,  0, 47, 35]\n",
    "```\n",
    "1. In the context of `[13]` → `1` is likely to come next,\n",
    "2. In the context of `[13,  1]` → `0` is likely to come next,\n",
    "3. In the context of `[13,  1,  0]` → `0` is likely to come next,\n",
    "4. In the context of `[13,  1,  0,  0]` → `0` is likely to come next,\n",
    "5. In the context of `[13,  1,  0,  0,  0]` → `0` is likely to come next,\n",
    "6. In the context of `[13,  1,  0,  0,  0,  0]` → `0` is likely to come next,\n",
    "7. In the context of `[13,  1,  0,  0,  0,  0,  0]` → `47` is likely to come next,\n",
    "8. In the context of `[13,  1,  0,  0,  0,  0,  0, 47]` → `35` is likely to come next.\n",
    "\n",
    "Summing upto `8` individual examples in our case, which is the `blockSize` and we take the `+1` to get the desired `label` for training..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844f6f1-c31e-4b74-bc7d-430536c6de0b",
   "metadata": {},
   "source": [
    "Let's see how we can achieve the same output in a code snippet:\n",
    "```python\n",
    "inputs = trainingData[:blockSize] # First Chuck of Characters\n",
    "outputs = trainingData[1:blockSize + 1] # First Chunk of Characters offset by 1\n",
    "for i in range(blockSize):\n",
    "    context = inputs[:i+1] # Context is inputs upto the offset\n",
    "    label = outputs[i] # Label is the offset\n",
    "    print(f\"When input example is {context}, then the label is: {label}\")\n",
    "```\n",
    "\n",
    "For which we get:\n",
    "```python\r\n",
    "When input example is tensor([13]), then the label is: 1\r\n",
    "When input example is tensor([13,  1]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0,  0,  0]), then the label is: 0\r\n",
    "When input example is tensor([13,  1,  0,  0,  0,  0,  0]), then the label is: 47\r\n",
    "When input example is tensor([13,  1,  0,  0,  0,  0,  0, 47]), then the label is: 35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7ff83-47a3-4c18-8904-3cbab831730e",
   "metadata": {},
   "source": [
    "One more thing to mention is that we not only train these examples all the way to the context of `blockSize` just for efficiency.\n",
    "\n",
    "**We also want our network to get *\"used to\"* seeing these examples for context of as small as `1` all the way upto the `blockSize` and everything in between.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65986360-6e76-4764-9fc1-2828740745ec",
   "metadata": {},
   "source": [
    "So, during `inference` we can start sampling from as little as `1` character of context. And once it starts sampling it can go all the way upto `blockSize` and after the context of `blockSize` we can start `truncating`, because the neural network will receive more than `blockSize` inputs when its trying to predict the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56596f7-828c-4e0c-8967-ed9b7c059206",
   "metadata": {},
   "source": [
    "And these input examples that we just looked are nothing but the `Time` dimension...\n",
    "\n",
    "But we need to care about the `Batch` dimension now... And that is because, everytime we feed these *chunks* of texts into a `Transformer`, we are going to have **mini-batches** of multiple chunks of texts that are all **stacked-up** in a single tensor (this is done for efficiency, such that we could keep the `GPU`'s busy, because they are very good at parallel processing of data, and we want to process multiple *chunks* of text all at the same time, but they are processed completely independently and they don't \"talk\" to each other)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f12c1-a099-407d-9329-35ba0c3b766b",
   "metadata": {},
   "source": [
    "We will also set up a `seed` so that whatever numbers I see here in my system, you are going to see the same numbers in your system later as well...\n",
    "\n",
    "Let's now generalize our discussion into code and I will discuss what is happening in the code one by one..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5871cb-da5c-4a93-b818-7cea3baa75ae",
   "metadata": {},
   "source": [
    "More specifically let's define a `getBatch()` method try to pick out batches of `batchSize`...\n",
    "\n",
    "Now don't get confused between `blockSize` and `batchSize`...\n",
    "\n",
    "$$\n",
    "\\displaylines{\n",
    "\\begin{align}\n",
    "blockSize \\rightarrow \\text{The number of independent sequences of characters we want to process in parallel} \\\\\n",
    "batchSize \\rightarrow \\text{The maximum context length of predictions}\n",
    "\\end{align}\n",
    "}\n",
    "$$\n",
    "\n",
    "We will now use our older *example* code get batches of examples, but now in context of `batchSize` now, and we will pick random indeces from the entire dataset, which will then be used to process all the possible examples in sequence and their corresponding labels in a batch using `torch.stack` which essentially concatenates a sequence of tensors along a new dimension. Which makes the `inputBatch` and `outputBatch` a `(4, 8)` tensor, where each row in an `inputBatch` is a *chunk* of the training set, and `outputBatch` will be used all the way at the end during **loss-function**...\n",
    "\n",
    "So, we can spell each of these `examples in a sequence` can be spelled out just like we did before to get their corresponding `labels` for each of these examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3535142b-a9e1-4d18-98c8-16ae1e486be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: torch.Size([4, 8])  Values: tensor([[70, 70, 67,  1, 70, 61,  1, 75],\n",
      "        [59,  1,  0, 56, 75,  1, 75, 63],\n",
      "        [59,  1,  0, 71, 67, 60, 69, 75],\n",
      "        [75, 63, 60,  1, 68, 56, 81, 60]])\n",
      "Outputs: torch.Size([4, 8])  Values: tensor([[70, 67,  1, 70, 61,  1, 75, 63],\n",
      "        [ 1,  0, 56, 75,  1, 75, 63, 60],\n",
      "        [ 1,  0, 71, 67, 60, 69, 75, 80],\n",
      "        [63, 60,  1, 68, 56, 81, 60, 10]])\n",
      "---------------------------------------------\n",
      "When input example is [70], then the label is: 70\n",
      "When input example is [70, 70], then the label is: 67\n",
      "When input example is [70, 70, 67], then the label is: 1\n",
      "When input example is [70, 70, 67, 1], then the label is: 70\n",
      "When input example is [70, 70, 67, 1, 70], then the label is: 61\n",
      "When input example is [70, 70, 67, 1, 70, 61], then the label is: 1\n",
      "When input example is [70, 70, 67, 1, 70, 61, 1], then the label is: 75\n",
      "When input example is [70, 70, 67, 1, 70, 61, 1, 75], then the label is: 63\n",
      "When input example is [59], then the label is: 1\n",
      "When input example is [59, 1], then the label is: 0\n",
      "When input example is [59, 1, 0], then the label is: 56\n",
      "When input example is [59, 1, 0, 56], then the label is: 75\n",
      "When input example is [59, 1, 0, 56, 75], then the label is: 1\n",
      "When input example is [59, 1, 0, 56, 75, 1], then the label is: 75\n",
      "When input example is [59, 1, 0, 56, 75, 1, 75], then the label is: 63\n",
      "When input example is [59, 1, 0, 56, 75, 1, 75, 63], then the label is: 60\n",
      "When input example is [59], then the label is: 1\n",
      "When input example is [59, 1], then the label is: 0\n",
      "When input example is [59, 1, 0], then the label is: 71\n",
      "When input example is [59, 1, 0, 71], then the label is: 67\n",
      "When input example is [59, 1, 0, 71, 67], then the label is: 60\n",
      "When input example is [59, 1, 0, 71, 67, 60], then the label is: 69\n",
      "When input example is [59, 1, 0, 71, 67, 60, 69], then the label is: 75\n",
      "When input example is [59, 1, 0, 71, 67, 60, 69, 75], then the label is: 80\n",
      "When input example is [75], then the label is: 63\n",
      "When input example is [75, 63], then the label is: 60\n",
      "When input example is [75, 63, 60], then the label is: 1\n",
      "When input example is [75, 63, 60, 1], then the label is: 68\n",
      "When input example is [75, 63, 60, 1, 68], then the label is: 56\n",
      "When input example is [75, 63, 60, 1, 68, 56], then the label is: 81\n",
      "When input example is [75, 63, 60, 1, 68, 56, 81], then the label is: 60\n",
      "When input example is [75, 63, 60, 1, 68, 56, 81, 60], then the label is: 10\n"
     ]
    }
   ],
   "source": [
    "# We define a manual seed such that you see the same numbers I see in my machine\n",
    "torch.manual_seed(69420)\n",
    "batchSize = 4 # Number of independent sequences of characters we want to process in parallel\n",
    "blockSize = 8 # Maximum context length of predictions\n",
    "\n",
    "def getBatch(split):\n",
    "    # Take the trainingData if the split is 'train' otherwise take the validationData\n",
    "    data = trainingData if split=='train' else validationData\n",
    "    # Generates random integers of batchSize between 0 and len(data) - blockSize\n",
    "    indexes = torch.randint(high=len(data) - blockSize, size=(batchSize,))\n",
    "    # Takes the inputs and outputs after stacking them up in a single tensor\n",
    "    inputs = torch.stack([data[i:i+blockSize] for i in indexes])\n",
    "    outputs = torch.stack([data[i+1:i+blockSize+1] for i in indexes])\n",
    "    return inputs, outputs\n",
    "\n",
    "# We call the method to initialize inputBatch and outputBatch\n",
    "inputBatch, outputBatch = getBatch('train')\n",
    "\n",
    "print(\"Inputs:\", inputBatch.shape, \" Values:\" , inputBatch)\n",
    "print(\"Outputs:\", outputBatch.shape, \" Values:\" , outputBatch)\n",
    "\n",
    "print('---------------------------------------------')\n",
    "\n",
    "for batchIndex in range(batchSize):\n",
    "    for blockIndex in range(blockSize):\n",
    "        context = inputBatch[batchIndex, : blockIndex+1]\n",
    "        label = outputBatch[batchIndex, blockIndex]\n",
    "        print(f\"When input example is {context.tolist()}, then the label is: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93ef1e-66a2-4e1c-a482-05699ec6e9fe",
   "metadata": {},
   "source": [
    "Now that we have our `inputBatch` and `outputBatch` we can start feeding these batches into a neural network and start getting predictions....\n",
    "\n",
    "Now we are going to start off with the simplest possible neural network, which in my opinion is a `Bigram Language Model` which was already covered in our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a> notebook in a lot of depth, and we will rather go faster and implement a `PyTorch Module` directly that implements the `Bigram Language Model`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc651c6-296a-45c2-8b59-4de0387293f3",
   "metadata": {},
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb728173-527c-42c4-879c-035a9adf9861",
   "metadata": {},
   "source": [
    "Before we implement the bigram model, I'd like to discuss this syntax in python:\n",
    "```python\n",
    "class A:\n",
    "    x = 10\n",
    "\n",
    "class B(A):\n",
    "    def show(self):\n",
    "        print(self.x)\n",
    "\n",
    "y = B()\n",
    "y.show()\n",
    "```\n",
    "For which we get the output:\n",
    "```python\n",
    "10\n",
    "```\n",
    "Which essentially means that this syntax (`B(A)`) is used for `inheritence`, which is much more helpful now to implement `Torch Modules` in our own implementations... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ba41b-335a-4e5f-a795-fa33dc2a4318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
