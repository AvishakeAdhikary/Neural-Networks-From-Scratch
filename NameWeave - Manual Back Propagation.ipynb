{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1897cf24-7d59-4d06-b81e-ab319dd19410",
   "metadata": {},
   "source": [
    "# Welcome to NameWeave - Manual Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36561d-d700-4ef6-8d5c-ea90ef096a18",
   "metadata": {},
   "source": [
    "Let's rearrange our last code from <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a> file, to come back to the part where we were at the end of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470658f1-cfa2-4716-ad53-af00abf6f968",
   "metadata": {},
   "source": [
    "In this lecture, we will not try to achieve a very low loss (We would get around the same loss that we had before) or dive into any complexifying architecture to improve our performance in the names prediction, rather we would be diving into the manual back propagation...\n",
    "\n",
    "Why would we do this you ask?\n",
    "\n",
    "Because, back in the days of around 2012, everybody used to write the backward pass manually, and you can *shoot yourself in the foot* if you don't know how to manually back propagate through, while debugging a neural network when you run into optimization issues and much more...\n",
    "\n",
    "Also, back propagation is a <a href=\"https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b#:~:text=%3E%20The%20problem%20with%20Backpropagation%20is,them%20work%E2%80%9D%20on%20your%20data.\">leaky abstraction</a>. Try to look into Andrej's beautiful medium post about back propagation.\n",
    "\n",
    "In this notebook, we would divide the excercises into 4 parts:\n",
    "1. Manual Back Propagation for each and every expression\n",
    "2. Manual Back Propagation through fast cross_entropy (Combined Back Propagation)\n",
    "3. Manual Back Propagation through batch normalization (Combined Back Propagation)\n",
    "4. Putting all the code together for manual `loss.backward()`\n",
    "\n",
    "**Typically we would use the general `loss.backward()` nowadays (which is PyTorch's auto-grad engine), and not everybody knows how to backpropagate through the neural networks manually, but we would learn to manually do it, just like we used to, back in the days...**\n",
    "\n",
    "Such that, this meme becomes relatable:\n",
    "![Manual Back Propagation Meme](ExplanationMedia/Images/ManualBackPropMeme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df483d-ae8e-4200-adec-5fef9cad48c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68e7f28-bce0-42b1-ae45-7ce879a2f3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9fb181-2119-448a-b76b-ce59f188e331",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13aef46b-3c44-47fa-b4d4-b40d1ad81555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avhis\\AppData\\Local\\Temp\\ipykernel_10444\\3082457058.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c388295-b4ed-4a6c-b943-64dbe0609c78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2a9c38-6d71-4cce-9e0e-2c7ee4c3dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"Datasets/Indian_Names.txt\").read().splitlines()\n",
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abfa64a7-68e2-4af0-99e9-a43e3a4e515e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53982"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7df46-38e7-4bb8-b908-8923f999a37d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d5c81f-ca2e-4ba7-b5f5-b6a606af3bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "STOI: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "ITOS {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "Vocabulary Size: 27\n"
     ]
    }
   ],
   "source": [
    "# Remember we need our starting and ending tokens as well in these mappings,\n",
    "characters = sorted(list(set(''.join(words)))) # Gives us all the characters in the english alphabet, hopefully our dataset has all of them\n",
    "stoi = {s:i+1 for i,s in enumerate(characters)} # Enumerate returns the tuples of number and string, which can then be mapped to string:index\n",
    "# We manually add these tokens for convenience\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()} # After we have the string:index mapping, we can easily iterate over their items to map index:string\n",
    "print(\"Characters:\", characters)\n",
    "print(\"STOI:\", stoi)\n",
    "print(\"ITOS\", itos)\n",
    "# We define a common vocabulary size\n",
    "vocabularySize = len(stoi)\n",
    "print(\"Vocabulary Size:\", vocabularySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42482f-a241-4858-80a8-c24c47d58e0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8472854-0573-44c1-9ece-0ca06baca8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Examples: 53982\n",
      "Training Examples: 43185\n",
      "Validation Examples: 5398\n",
      "Test Examples: 5399\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 3\n",
    "# Will build the dataset on only the words we take as input\n",
    "def buildDataset(words):\n",
    "    # We define two lists, inputs & outputs, where inputs are our blocks of the block size mentioned above and outputs are the label indexes\n",
    "    inputs , outputs = [], []\n",
    "    # We iterate over each word\n",
    "    for word in words:\n",
    "        # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "        block = [0] * blockSize\n",
    "        # We run another loop for each word's character, here word also needs the ending token '.'\n",
    "        for character in word + '.':\n",
    "            # We take out the index from our look-up table\n",
    "            index = stoi[character]\n",
    "            # We append the input with our block\n",
    "            inputs.append(block)\n",
    "            # We append the output label with out index of the character\n",
    "            outputs.append([index])\n",
    "            # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "            block = block[1:] + [index]\n",
    "    # We also convert these inputs and outputs to tensors for neural network processing\n",
    "    inputs = torch.tensor(inputs)\n",
    "    outputs = torch.flatten(torch.tensor(outputs))\n",
    "    # We return the inputs and outputs\n",
    "    return inputs, outputs\n",
    "\n",
    "# We define a manual seed to random\n",
    "random.seed(69)\n",
    "# We shuffle all the words, so that the model receives all kinds of data\n",
    "random.shuffle(words)\n",
    "# We define two number of inputs\n",
    "# We take the number of examples to 80% in the first variable\n",
    "numberOfInputs1 = int(0.8*len(words))\n",
    "# We take the number of examples to 90% in the first variable\n",
    "numberOfInputs2 = int(0.9*len(words))\n",
    "# Inputs and outputs that go till 80% of the examples\n",
    "trainingInputs, trainingOutputs = buildDataset(words[:numberOfInputs1])\n",
    "# Inputs and outputs that start at 80% of the examples and go till 90% of the examples\n",
    "validationInputs, validationOutputs = buildDataset(words[numberOfInputs1:numberOfInputs2])\n",
    "# Inputs and outputs that start at 90% of the examples\n",
    "testInputs, testOutputs = buildDataset(words[numberOfInputs2:])\n",
    "\n",
    "# We can check the numbers\n",
    "print(\"Total Examples:\",len(words)) # 100%\n",
    "print(\"Training Examples:\",len(words[:numberOfInputs1])) # 80%\n",
    "print(\"Validation Examples:\",len(words[numberOfInputs1:numberOfInputs2])) # 10%\n",
    "print(\"Test Examples:\",len(words[numberOfInputs2:])) # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf43b14-1c57-4b70-860e-1cecf8956be1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Utility Function to Compare Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a84d8-08ab-41ab-aea2-d956a6ddb833",
   "metadata": {},
   "source": [
    "We would also use a new utility function in this lecture to compare the gradients in this lecture.\\\n",
    "The utility function tells us if we have an exact match, an approximate match and the maximum difference we have in our gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf2efe2-97b6-422f-b30c-bb5f7c802699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gradients(description, manual_gradient, torch_tensor):\n",
    "    \"\"\"\n",
    "    Compare manual gradients with PyTorch gradients.\n",
    "\n",
    "    Parameters:\n",
    "    - description: A string describing the comparison.\n",
    "    - manual_gradient: The manually computed gradient.\n",
    "    - torch_tensor: The PyTorch tensor for which gradients are computed.\n",
    "\n",
    "    Prints the comparison results including exact match, approximate match, and maximum difference.\n",
    "    \"\"\"\n",
    "    exact_match = torch.all(manual_gradient == torch_tensor.grad).item()\n",
    "    approximate_match = torch.allclose(manual_gradient, torch_tensor.grad)\n",
    "    max_difference = (manual_gradient - torch_tensor.grad).abs().max().item()\n",
    "    print(f'{description:15s} | Exact: {str(exact_match):5s} | Approximate: {str(approximate_match):5s} | MaxDiff: {max_difference}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ef4bb-0405-4df5-9847-a74dfcdbb9bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Neural Network Initialization - Weights, Biases & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48230c3b-2415-470a-beaa-e007f5be7489",
   "metadata": {},
   "source": [
    "You will see that I changed the initialization for the parameters to be small numbers...\\\n",
    "Normally you would set the biases to be all zeros, but I did set the biases to be small numbers...\\\n",
    "Sometimes if your variables are initialized at exactly zero, it can mask an incorrect implementation of a gradient...\n",
    "\n",
    "You will also notice, unlike last time, we are still using the biases for the First Hidden Layer, despite of the batch normalization which is just for fun (we are going to have a gradient with respect to it and we can check wheter we are calculating it correctly or not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b884d09d-a98d-4bac-8193-b9b6f21f60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a generator to give the same result on your machine, as of my machine\n",
    "generator = torch.Generator().manual_seed(6942069420)\n",
    "# Embedding Matrix (Input Layer)\n",
    "embeddingFeatureSpaceLength = 10 \n",
    "embeddingLookUpMatrix = torch.randn((vocabularySize, embeddingFeatureSpaceLength), generator=generator)\n",
    "# Hidden Layer\n",
    "numberOfHiddenLayerNeurons = 64\n",
    "weightsOfHiddenLayer = torch.randn((blockSize*embeddingFeatureSpaceLength), numberOfHiddenLayerNeurons, generator=generator) * (5/3) / ((embeddingFeatureSpaceLength*blockSize)**0.5)\n",
    "biasesOfHiddenLayer = torch.randn(numberOfHiddenLayerNeurons, generator=generator) * 0.1\n",
    "# Output Layer / Final Layer\n",
    "weightsOfFinalLayer = torch.randn(numberOfHiddenLayerNeurons, vocabularySize, generator=generator) * 0.1\n",
    "biasesOfFinalLayer = torch.randn(vocabularySize, generator=generator) * 0.1\n",
    "# Batch Normalization Layer\n",
    "batchGains = torch.randn((1, numberOfHiddenLayerNeurons)) * 0.1 + 1.0\n",
    "batchBiases = torch.randn((1, numberOfHiddenLayerNeurons)) * 0.1\n",
    "# Parameters\n",
    "parameters = [embeddingLookUpMatrix, weightsOfHiddenLayer, biasesOfHiddenLayer, weightsOfFinalLayer, biasesOfFinalLayer, batchGains, batchBiases]\n",
    "# We set all the requires gradient to True\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12d8ab-8deb-47e0-bd2d-ac0ac86fb210",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c850d-d46c-4526-8e09-f030f8325de4",
   "metadata": {},
   "source": [
    "You will see that we will significantly expand the forward pass because we have some lines of code like the \n",
    "```python\n",
    "loss = F.cross_entropy(logits, trainingOutputs[indexes])\n",
    "```\n",
    "which is very much not understandable of what works under the hood, so I tried to bring back some of the explicit implementation of the loss function that we have inside the PyTorch's `cross_entropy` method.\n",
    "\n",
    "Also, we will be running the forward pass for only 1 epoch for the time being, until we figure out all the back propagation elements, and we will be prepending a 'd' in front of the variables to make them understandable as backward pass variables in the context of $\\frac{d}{dx}y$ equations.\n",
    "\n",
    "Note: This back propagation is going to be way different than what we did back in our original <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/Neural%20Network%20with%20Derivatives.ipynb\">Neural Network with Derivatives</a> notebook, because in that notebook we had single (individual) scalars, but here we have entire layers of neurons and we want to backpropagate through the entirity of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f23a90-634a-414e-96a4-612e2fa35974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(3.4479, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batchSize = 32\n",
    "indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(batchSize,))\n",
    "inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "# Forward Pass Start\n",
    "embedding = embeddingLookUpMatrix[inputBatch]\n",
    "concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "# Linear Layer - 1\n",
    "hiddenLayerPreBatchNormStates = concatenatedEmbedding @ weightsOfHiddenLayer + biasesOfHiddenLayer\n",
    "# Batch Normalization Layer\n",
    "batchMeanAtIteration = 1/batchSize*hiddenLayerPreBatchNormStates.sum(0, keepdim=True)\n",
    "batchDifference = hiddenLayerPreBatchNormStates - batchMeanAtIteration\n",
    "batchDifferencePow2 = batchDifference**2\n",
    "batchVariance = 1/(batchSize - 1)*(batchDifferencePow2).sum(0, keepdim=True) # Note: Bessel's Correction (which is division by n-1 instead of n)\n",
    "batchVarianceInverse = (batchVariance + 1e-5)**-0.5\n",
    "batchRawValue = batchDifference * batchVarianceInverse\n",
    "hiddenLayerPreActivationStates = batchGains * batchRawValue + batchBiases\n",
    "# Non-Linearity Layer\n",
    "hiddenLayerPostActivationStates = torch.tanh(hiddenLayerPreActivationStates)\n",
    "# Linear Layer - 2\n",
    "logits = hiddenLayerPostActivationStates @ weightsOfFinalLayer + biasesOfFinalLayer\n",
    "# Cross Entropy Loss (Same as F.cross_entropy() method)\n",
    "maxLogits = logits.max(1, keepdim=True).values\n",
    "normalizedLogits = logits - maxLogits # Subtracting it for numerical stability\n",
    "counts = normalizedLogits.exp()\n",
    "sumCounts = counts.sum(1, keepdims=True)\n",
    "sumCountsInverse = sumCounts**-1 # For some reason (1/sumCounts) does not output with exact back propagation\n",
    "probabilities = counts * sumCountsInverse\n",
    "logProbabilities = probabilities.log()\n",
    "loss = -logProbabilities[range(batchSize), outputBatch].mean()\n",
    "\n",
    "# Backward Pass (Mini-Batch)\n",
    "for parameter in parameters:\n",
    "    parameter.grad = None\n",
    "# Rataining Gradient For Checking It Later\n",
    "for item in [logProbabilities, probabilities, sumCountsInverse, sumCounts, counts, normalizedLogits, maxLogits, logits, \n",
    "             hiddenLayerPostActivationStates, hiddenLayerPreActivationStates, batchRawValue, batchVarianceInverse,\n",
    "             batchVariance, batchDifferencePow2, batchDifference, batchMeanAtIteration, hiddenLayerPreBatchNormStates,\n",
    "             concatenatedEmbedding, embedding]: # I am sorry, but I did not find a much cleaner way to do this XD\n",
    "    item.retain_grad()\n",
    "loss.backward()\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf37d8b-abd0-420d-b1e6-1a1d06c30811",
   "metadata": {},
   "source": [
    "# Manual Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47338e-1023-4203-ba17-2d6ba4b46c58",
   "metadata": {},
   "source": [
    "In this exercise we will back propagate through the entire forward pass for all the small expressions, one-by-one...\n",
    "\n",
    "So let's start..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c9f09-09cd-4024-9723-116a6360bfde",
   "metadata": {},
   "source": [
    "Let's understand what we are trying to achieve in the first place...\n",
    "\n",
    "At the very beginning we are trying to calcualate the gradients of the loss...\n",
    "\n",
    "In the context of sensitivity analysis, **gradients represent how sensitive one variable (output) is to small changes in another variable (input)**.\n",
    "\n",
    "So, we are trying to know that **how sensitive the loss is in small changes to `logProbabilities`**..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eeeaff-4212-451b-9a79-3ffe9ee2edc8",
   "metadata": {},
   "source": [
    "Let's understand what $\\log{}$ is first...\n",
    "\n",
    "$\\log{}\\rightarrow\\text{Inverse Function to Exponentiation}$\n",
    "\n",
    "For example:\\\n",
    "$1000 = 10^3 \\rightarrow \\log_{10}(1000) \\rightarrow 3$\\\n",
    "Or\\\n",
    "$x = b^y \\rightarrow \\log_{10}(x) \\rightarrow y$\n",
    "\n",
    "And we have natural logarithms as well... Also represented as $\\ln{}$...\\\n",
    "Here we do $y=e^x\\rightarrow\\log_e{x}\\rightarrow\\ln{x}$.\n",
    "\n",
    "\n",
    "Here we see that `e` is the base, and `e` is also known as the Euler's Number\n",
    "\n",
    "$$ e = \\sum\\limits_{n=0}^\\infty\\frac{1}{n!} = \\frac{1}{0!} + \\frac{1}{1!} + \\frac{1}{2!} + \\frac{1}{3!} +...$$\n",
    "\n",
    "So what is $\\frac{d}{dx}(\\log{(x)})$?\n",
    "\n",
    "Well, $\\frac{d}{dx}(\\log{(x)}) = \\frac{1}{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a91f9-ad7d-47b6-b8c6-be34dabc5810",
   "metadata": {},
   "source": [
    "Also if we remember $\\log{}$ from our school days of mathematics, we know that $\\log{(a.b.c)}=\\log(a)+\\log(b)+\\log(c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff1c7d-70e3-4eaa-9c13-7caac0b45512",
   "metadata": {},
   "source": [
    "Let's first understand what are we doing in this line:\n",
    "```python\n",
    "loss = -logProbabilities[range(batchSize), outputBatch].mean()\n",
    "```\n",
    "\n",
    "`range(batchSize)` gives us an array of `32` numbers from `0` to `31`, when we index into `logProbabilities` using `range()` and `outputBatch` range is essentially using all the `32` rows of the output and *plucking-out* all the **`logProbabilities` of the correct next character**, then it is averaging them using `mean()` and we are taking the `negetive` of that number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ca075-d771-49e0-bf9e-92017bf7f71b",
   "metadata": {},
   "source": [
    "So from all the above explanations we come to a simpler way of trying to evaluate something complex...\\\n",
    "Here in this line `loss = -logProbabilities[range(batchSize), outputBatch].mean()` we are doing something like:\\\n",
    "$$\\text{loss} = -(a + b + c)/3$$\n",
    "So if we do $\\frac{dLoss}{da}$ we get:\n",
    "$$loss = -\\frac{1}{3}a + -\\frac{1}{3}b + -\\frac{1}{3}c$$\\\n",
    "Or $\\frac{dLoss}{da} = -\\frac{1}{3}$\\\n",
    "Or we can say that it is $-\\frac{1}{n}$ of only those indexes who participate, otherwise all the other gradients remain 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071fb77-167b-4cac-a85d-1962e8cdee48",
   "metadata": {},
   "source": [
    "But now we have to stay careful, because we understand that we don't have an individual scalar here...\\\n",
    "If we check the size of the `logProbabilities` using:\n",
    "```python\n",
    "print(logProbabilities.shape)\n",
    "```\n",
    "we get a shape like:\n",
    "```python\n",
    "torch.Size([32, 27])\n",
    "```\n",
    "\n",
    "And by this time, it should not surprise us, that the shape of the tensor remain the same because we are doing element-wise operations and assigning gradients to the same locations as the original tensor.\n",
    "\n",
    "So now what we can do is, we can create a tensor of zeros and only fill the indexes with equation $-1/n$ where the indeces participate...\n",
    "\n",
    "(For us `n` is the `batchSize`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b58e4dde-1652-4aa9-800a-c757d7567041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLogProbabilities | Exact: True  | Approximate: True  | MaxDiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dLogProbabilities = torch.zeros_like(logProbabilities)\n",
    "dLogProbabilities[range(batchSize), outputBatch] = -1.0/batchSize\n",
    "\n",
    "# Now we can compare our backpropagation implementation\n",
    "compare_gradients('dLogProbabilities', dLogProbabilities, logProbabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2ad4b-0901-433f-b2a8-de1252d18f7a",
   "metadata": {},
   "source": [
    "Congratulations...\\\n",
    "We did our very first step..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850188f-4d0f-4054-b93c-a2cf72836a1b",
   "metadata": {},
   "source": [
    "Let's do the next step now...\n",
    "\n",
    "For \n",
    "```python\n",
    "logProbabilities = probabilities.log()\n",
    "```\n",
    "\n",
    "Once again,\n",
    "$$\\frac{d}{dx}\\log{(x)} = \\frac{1}{x}$$\n",
    "\n",
    "But now, because this is an intermediate node, we have to be careful for local and global derivatives as well...\n",
    "\n",
    "So, we will have $1/\\text{Probailities}$ as the **local derivative** and our already caculated `dLogProbabilities` as our global derivative..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c002fc-0a98-4050-b36b-beba2ae4c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLogProbabilities | Exact: True  | Approximate: True  | MaxDiff: 0.0\n",
      "dProbabilities  | Exact: True  | Approximate: True  | MaxDiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dLogProbabilities = torch.zeros_like(logProbabilities)\n",
    "dLogProbabilities[range(batchSize), outputBatch] = -1.0/batchSize\n",
    "dProbabilities = (1/probabilities) * dLogProbabilities\n",
    "\n",
    "\n",
    "# Now we can compare our backpropagation implementation\n",
    "compare_gradients('dLogProbabilities', dLogProbabilities, logProbabilities)\n",
    "compare_gradients('dProbabilities', dProbabilities, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b64d7-b8a3-48ef-b19b-0d605da318f5",
   "metadata": {},
   "source": [
    "Seems like, we are very well on track... 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bfdf8f7-da3e-405e-91e7-65d80046e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n",
      "torch.Size([32, 27])\n",
      "torch.Size([32, 27])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 27])\n",
      "torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "print(maxLogits.shape)\n",
    "print(normalizedLogits.shape)\n",
    "print(counts.shape)\n",
    "print(sumCounts.shape)\n",
    "print(sumCountsInverse.shape)\n",
    "print(probabilities.shape)\n",
    "print(logProbabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6500e-3395-4640-9c79-ee2bff904647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4901d40-bfb4-4e38-845d-86b2bc0f3809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a90de-87ac-4a97-8d64-b5e3668ae63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa67a6-a46d-4443-ad74-7288c063964d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221d7e5-8fcb-4e2d-a3bf-af96551a3368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
