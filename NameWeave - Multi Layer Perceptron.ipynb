{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908f6d13-f8ae-4cc6-b33c-ccc67424b11f",
   "metadata": {},
   "source": [
    "# Welcome to NameWeave - Multi Layer Perceptron Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abcd4f7-a35a-42e6-96e4-061a8ec1d627",
   "metadata": {},
   "source": [
    "Like our original <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a>,\\\n",
    "We will try to create a **Multi Layer Perceptron** to build a character level language model and predict names based on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22300d7d-798b-4837-9c4b-4264461cdb89",
   "metadata": {},
   "source": [
    "To Approach this model, we will follow an approach based on the paper,\\\n",
    "<a href=\"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\">A Neural Probabilistic Language Model</a>,\\\n",
    "Which is a **word level language model** but solves the similar problem of predicting words...\\\n",
    "This paper is 19 pages long, and we don't have time to read the entire paper,\\\n",
    "But I invite you to read it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e0abc-ea74-4034-be72-2a30002f1368",
   "metadata": {},
   "source": [
    "In this paper,\\\n",
    "They used a word vocabulary of 17000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a68db0-b3b3-4403-ad33-14e359b7fdab",
   "metadata": {},
   "source": [
    "![Word Vocabulary](ExplanationMedia/Images/Vocabulary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33363f-56fc-4dc4-833c-d3b5f00a0a0b",
   "metadata": {},
   "source": [
    "They then converted this vocabulary into a 30 dimensional feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8913c-97a4-4a7c-90d7-c2955bfeab19",
   "metadata": {},
   "source": [
    "![Vocabulary to Feature Space](ExplanationMedia/Images/VocabularytoFeatureSpace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484bf79-78db-4c4b-9316-5a6cc0dcd172",
   "metadata": {},
   "source": [
    "This is a very small space for a very large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c5d75-0090-491f-ae03-69248b1d7b01",
   "metadata": {},
   "source": [
    "The approach of this paper is also very similar because,\\\n",
    "They used a **multilayer neural network** to **predict the next word given the previous ones**,\\\n",
    "& they **maximize the log-likelihood** of the training data or a regularized criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c148c6-4349-4a99-834c-af86cf5bb319",
   "metadata": {},
   "source": [
    "#### Why does this approach work? Let's take a concrete example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5c121-0e85-45c2-9ed2-2097ec1d555b",
   "metadata": {},
   "source": [
    "We have a phrase: *A dog was running in a room*, *The cat is walking in the bedroom*\n",
    "\n",
    "During the training of the network, the words move around to a similar corner of the space based on their features\\\n",
    "So, even if the model goes *out of distribution* during test, making predictions,\\\n",
    "The similar words which have never occured before may occur here.\n",
    "\n",
    "Resulting in the phrase: *A dog is walking in a bedroom*, *The cat is running in a room*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dfa77-e029-4a49-a726-501a666a5f8f",
   "metadata": {},
   "source": [
    "If we knew that dog and cat played similar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), (is,was), we could naturally transfer probability mass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad7ecb-f523-4645-a6ac-803520992a4b",
   "metadata": {},
   "source": [
    "Let's now look at the Neural Network for this Approach\n",
    "\n",
    "![A Neural Probabilistic Language Model - Neural Network](https://miro.medium.com/v2/resize:fit:1200/1*EqKiy4-6tuLSoPP_kub33Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d53d7-4ee8-4724-ace1-15d922c91ed3",
   "metadata": {},
   "source": [
    "In this network,\\\n",
    "They are taking *3 previous words* and are trying to *predict the 4-th word in a sequence*.\n",
    "\n",
    "Now because they had the vocabulary of 17000 words (**'w'**),\\\n",
    "These previous words are the indexes ranging from 0-16999.\n",
    "\n",
    "There is also a lookup-table which they call **'C'** \\\n",
    "This is their lookup-embedding-matrix, which is shared among all the words\\\n",
    "This C is a matrix of say 17000x30. (So, number of words in vocabulary by number of dimensions in the feature space).\n",
    "\n",
    "So what this is essentially doing is,\\\n",
    "They are trying to pick out the row based on the index of the word from vocabulary\\\n",
    "And the row represents the 1x30 vector of the word's embedding.\\\n",
    "So they are using the same matrix over and over to look for their own vector of embedding.\n",
    "\n",
    "So, because they are taking *3 previous words*, and each vector uses 1x30 dimensions,\\\n",
    "They have 3x30 dimensions making up 90 dimensions in total.\n",
    "\n",
    "Next up is the hidden layer (tanh non-linearity layer).\\\n",
    "This is layer has the *hyper-parameter* (*hyper-parameter* is a parameter of the neural network, that is the designer' choice of the neural network).\\\n",
    "So, this layer's size can be as large as we'd like or as we'd like.\\\n",
    "So, we are going to go over multiple choices, and we are going to evaluage how good they work.\\\n",
    "Note: This layer will be fully connected to all the vector embeddings of the previous layer (90 dimensions).\n",
    "\n",
    "Next, they have a output layer of logits (you can refer to the original <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a>  for reference).\\\n",
    "Now, because they had a vocabulary of **17000 words**, this layer has **17000 neurons** which is **fully connected to the hidden layer**.\\\n",
    "Resulting in the *maximum computation between the hidden layer and output layer*.\n",
    "\n",
    "This output layer is then having a softmax activation layer, which exponentiates the logits and normalized to sum to 1.\\\n",
    "Which results in a nice probability distribution for the next *4-th word in a sequence*.\n",
    "\n",
    "<hr>\n",
    "\n",
    "During training we have the label (identity of the next word in a sequence).\n",
    "That word's index is used to choose the probability of that word,\\\n",
    "And then they maximize the probability of that word, with respect to the parameters of this neural network.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Parameters:\n",
    "1. The *weights and biases* of the *output layer*\n",
    "2. The *weights and biases* of the *hidden layer*\n",
    "3. The *embedding look-up table 'C'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9e483-f162-4ca7-8ee5-70aecc3b0dd3",
   "metadata": {},
   "source": [
    "So, Let's implement our own neural network, based on the above approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea5c56-a624-4206-8e00-c719c229ac73",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8ad30a-3f8e-4c74-babc-f77e59c79180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bd709-ec2a-48b6-bbaf-5193824e1426",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa7700f-aa4a-47af-8a12-23edb9a6083e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avhis\\AppData\\Local\\Temp\\ipykernel_8220\\3959944622.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F # This is required for one-hot encoding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f5d1b-c5be-4871-aed3-d349329f287f",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8277572-79a6-4ac7-878e-6235589d4f19",
   "metadata": {},
   "source": [
    "Once again, you can refer to the original <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a> for reference, as to why we chose to load the dataset in the following way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a0ea9b-821b-4492-a37d-f6f7a4e774c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"Datasets/Indian_Names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f181a3d-a0e0-43fc-9a9e-3014b0be05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8ce030-e90f-4527-82c4-aefcebe96242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53982"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931037f-8025-4be4-969f-5c72eecffdef",
   "metadata": {},
   "source": [
    "# Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74bfacc-e51a-4448-9f12-772f4487f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "STOI: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "ITOS {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Remember we need our starting and ending tokens as well in these mappings,\n",
    "characters = sorted(list(set(''.join(words)))) # Gives us all the characters in the english alphabet, hopefully our dataset has all of them\n",
    "stoi = {s:i+1 for i,s in enumerate(characters)} # Enumerate returns the tuples of number and string, which can then be mapped to string:index\n",
    "# We manually add these tokens for convenience\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()} # After we have the string:index mapping, we can easily iterate over their items to map index:string\n",
    "print(\"Characters:\",characters)\n",
    "print(\"STOI:\",stoi)\n",
    "print(\"ITOS\",itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd7c8d-b5a0-4f3b-8b61-316578d5d1df",
   "metadata": {},
   "source": [
    "# Building Dataset for Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa3a85-f68d-4e2b-8f53-0b3d94f2dd39",
   "metadata": {},
   "source": [
    "Now, we can't just feed in names to our Neural Network.\\\n",
    "Rather, we need to build a dataset which will be able to feed into our neural network.\n",
    "\n",
    "Let's visualize how we are going to feed in to the neural network first...\n",
    "\n",
    "![Multi Layer Perceptron Approach](ExplanationMedia/Images/NameWeaveMultiLayerPerceptronApproach.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64248f4-8188-431d-a613-6a5a2b4fc9cd",
   "metadata": {},
   "source": [
    "Let's now try to make this dataset...\n",
    "\n",
    "Remeber, this is **not bigram anymore**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dcc320c-f396-4bfd-b70f-229a2f235c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: aaban\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> a\n",
      "aba ---> n\n",
      "ban ---> .\n",
      "Name: aabharan\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> h\n",
      "abh ---> a\n",
      "bha ---> r\n",
      "har ---> a\n",
      "ara ---> n\n",
      "ran ---> .\n",
      "Name: aabhas\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> h\n",
      "abh ---> a\n",
      "bha ---> s\n",
      "has ---> .\n",
      "Name: aabhat\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> h\n",
      "abh ---> a\n",
      "bha ---> t\n",
      "hat ---> .\n",
      "Name: aabheer\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> h\n",
      "abh ---> e\n",
      "bhe ---> e\n",
      "hee ---> r\n",
      "eer ---> .\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "inputBlockSize = 3\n",
    "\n",
    "# We define two lists, inputs & outputs, where inputs are our blocks of the block size mentioned above and outputs are the label indexes\n",
    "inputs , outputs = [], []\n",
    "\n",
    "# We run a loop for each word in the original dataset\n",
    "for word in words[:5]:\n",
    "    # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "    block = [0] * inputBlockSize # This is also known as the context of the network\n",
    "    # We print each word\n",
    "    print(\"Name:\", word)\n",
    "    # We run another loop for each word's character, here word also needs the ending token '.'\n",
    "    for character in word + '.':\n",
    "        # We take out the index from our look-up table\n",
    "        index = stoi[character]\n",
    "        # We append the input with our block\n",
    "        inputs.append(block)\n",
    "        # We append the output label with out index of the character\n",
    "        outputs.append([index])\n",
    "        # We can check our inputs and thier corresponsing outputs\n",
    "        print(''.join(itos[i] for i in block), '--->', itos[index])\n",
    "        # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "        block = block[1:] + [index]\n",
    "# We also convert these inputs and outputs to tensors for neural network processing\n",
    "inputs = torch.tensor(inputs)\n",
    "outputs = torch.tensor(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e2afd8-a1d4-4319-909b-3e1edcf4d3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs Shape: torch.Size([37, 3]) , Datatype: torch.int64\n",
      "Outputs Shape: torch.Size([37, 1]) , Datatype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# We can now check the shape of inputs and outputs and their corresponding datatypes\n",
    "print(\"Inputs Shape:\",inputs.shape,\", Datatype:\",inputs.dtype)\n",
    "print(\"Outputs Shape:\",outputs.shape,\", Datatype:\",outputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b99fb60d-ba43-4f72-acd2-6e68f1e3cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  1],\n",
      "        [ 2,  1, 14],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  8],\n",
      "        [ 2,  8,  1],\n",
      "        [ 8,  1, 18],\n",
      "        [ 1, 18,  1],\n",
      "        [18,  1, 14],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  8],\n",
      "        [ 2,  8,  1],\n",
      "        [ 8,  1, 19],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  8],\n",
      "        [ 2,  8,  1],\n",
      "        [ 8,  1, 20],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  8],\n",
      "        [ 2,  8,  5],\n",
      "        [ 8,  5,  5],\n",
      "        [ 5,  5, 18]])\n"
     ]
    }
   ],
   "source": [
    "# We can also check how the inputs look like\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039c646f-5531-4d21-a100-03d1261646c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 1],\n",
      "        [14],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 8],\n",
      "        [ 1],\n",
      "        [18],\n",
      "        [ 1],\n",
      "        [14],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 8],\n",
      "        [ 1],\n",
      "        [19],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 8],\n",
      "        [ 1],\n",
      "        [20],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 8],\n",
      "        [ 5],\n",
      "        [ 5],\n",
      "        [18],\n",
      "        [ 0]])\n"
     ]
    }
   ],
   "source": [
    "# We can also check how the outputs look like\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d94a863c-3a0d-40fa-88b4-94c473cb3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want our outputs to be single elements in a list and not add another dimension to the list\n",
    "# So we use a flatten method available in PyTorch to flatten these outputs\n",
    "outputs = torch.flatten(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6013cf4b-0813-4747-8a99-a6ffaf44b5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1,  2,  1, 14,  0,  1,  1,  2,  8,  1, 18,  1, 14,  0,  1,  1,  2,\n",
      "         8,  1, 19,  0,  1,  1,  2,  8,  1, 20,  0,  1,  1,  2,  8,  5,  5, 18,\n",
      "         0])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45600e7-ea13-4f48-b536-75198d7feaf0",
   "metadata": {},
   "source": [
    "Now that we have our inputs and outputs configured, let's build our **embeddingLookUpMatrix 'C'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbd4a0-4539-4566-b2ee-f4faebc60296",
   "metadata": {},
   "source": [
    "# Building Embedding Look-up Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba1800-88f7-4510-b73b-98adf215d80b",
   "metadata": {},
   "source": [
    "In the paper the researchers had a big vocabulary of 17000 words,\\\n",
    "They used a very small 30 dimensional feature space.\n",
    "\n",
    "Because we have a vocabulary of only 27 characters,\\\n",
    "Let's use a very small 2 dimensional feature space for our embedding look-up matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45b25701-bc49-483b-8c10-93302fe5427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5820, -0.2370],\n",
      "        [ 1.1564, -1.5507],\n",
      "        [ 0.3370,  0.9905],\n",
      "        [ 2.0201, -1.3263],\n",
      "        [ 1.7319,  0.8092],\n",
      "        [-1.1567, -0.1717],\n",
      "        [ 1.4870,  0.3647],\n",
      "        [ 1.4763, -0.1208],\n",
      "        [ 1.4331, -0.6318],\n",
      "        [ 0.8025, -0.2571],\n",
      "        [-1.5145,  0.6996],\n",
      "        [ 0.5932,  1.5696],\n",
      "        [ 0.9620, -0.2745],\n",
      "        [ 0.7202, -0.7890],\n",
      "        [ 1.9879, -0.6847],\n",
      "        [ 1.8194,  0.2373],\n",
      "        [ 0.3454,  1.3520],\n",
      "        [-0.0533, -0.4030],\n",
      "        [ 0.6345, -1.5687],\n",
      "        [ 1.2893,  1.3174],\n",
      "        [-0.3376, -0.0277],\n",
      "        [ 1.3857, -2.1648],\n",
      "        [ 0.1235,  0.8132],\n",
      "        [ 1.2909, -0.0200],\n",
      "        [-0.7188, -0.9777],\n",
      "        [-0.0405,  0.3629]])\n"
     ]
    }
   ],
   "source": [
    "# We decide to build a embeddingLookUpMatrix with 27x2 because we have a vocabulary of 27 characters and we want to fit them in a 2 dimensional space\n",
    "# In the beginning we initialize it randomly\n",
    "embeddingFeatureSpaceLength = 2\n",
    "embeddingLookUpMatrix = torch.randn((len(characters),embeddingFeatureSpaceLength))\n",
    "# So each one of our 27 characters will have a 2 dimensional embedding\n",
    "print(embeddingLookUpMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c15f30-c740-4123-b11b-454ee80a4b2b",
   "metadata": {},
   "source": [
    "Now that we have a embedding-look-up matrix,\n",
    "\n",
    "For example,\n",
    "\n",
    "We can easily do:\n",
    "```python\n",
    "embeddingLookUpMatrix[6]\n",
    "```\n",
    "\n",
    "To get the embedding:\n",
    "```python\n",
    "tensor([-0.2483, -0.3909])\n",
    "```\n",
    "\n",
    "But there is a more similar way to do the exact same thing based on one-hot encoding....\n",
    "\n",
    "We can do:\n",
    "```python\n",
    "F.one_hot(torch.tensor(6), num_classes=27)\n",
    "```\n",
    "\n",
    "To get the one-hot embedding\n",
    "```python\n",
    "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "```\n",
    "\n",
    "Then convert this to float and multiply it with our original *embeddingLookUpMatrix*:\n",
    "```python\n",
    "F.one_hot(torch.tensor(6), num_classes=27).float() @ embeddingLookUpMatrix\n",
    "```\n",
    "\n",
    "Which results in:\n",
    "```python\n",
    "tensor([-0.2483, -0.3909])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b373dae-4ec4-45cd-99b9-2d1b53f1e8c9",
   "metadata": {},
   "source": [
    "This works because of the property of matrix multiplication,\n",
    "\n",
    "The 0's in our one-hot encoded vector discards all the zeros,\\\n",
    "And only multiplies the 1 to the corresponding column of the embeddingLookUpMatrix.\n",
    "\n",
    "So we can consider this matrix multiplication to be the first layer of our neural network,\\\n",
    "Giving us our corresponding embedding for the index. *(1x2 embedding vector for our case)*\n",
    "\n",
    "But we will simply index into our look-up table and discard the way of one-hot encoding for the time being"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d96f2-29d6-4d21-91e6-7f93856cff24",
   "metadata": {},
   "source": [
    "But, now that we know that we want to index into our look-up embedding matrix,\\\n",
    "How do we do that simultaneously for all the inputs?\n",
    "\n",
    "PyTorch has got you covered.\n",
    "\n",
    "In PyTorch we can, very flexibly pick out rows...\n",
    "\n",
    "For example,\\\n",
    "We can do indexing with lists of indexes:\n",
    "```python\n",
    "embeddingLookUpMatrix[[1,2,3]]\n",
    "```\n",
    "\n",
    "Which gives out the rows of the corresponding indexes:\n",
    "```python\n",
    "tensor([[ 0.2118,  1.0454],\n",
    "        [ 0.1876,  0.8921],\n",
    "        [ 0.9759, -0.2606]])\n",
    "```\n",
    "\n",
    "We can also do:\n",
    "```python\n",
    "embeddingLookUpMatrix[torch.tensor([1,2,3])]\n",
    "```\n",
    "\n",
    "Which gives out the rows of the corresponding indexes:\n",
    "```python\n",
    "tensor([[ 0.2118,  1.0454],\n",
    "        [ 0.1876,  0.8921],\n",
    "        [ 0.9759, -0.2606]])\n",
    "```\n",
    "\n",
    "We can similarly pick out the same rows again and again:\n",
    "```python\n",
    "embeddingLookUpMatrix[[1,2,3,3,3,3]]\n",
    "```\n",
    "\n",
    "Which gives us the same row again and again:\n",
    "```python\n",
    "tensor([[ 0.2118,  1.0454],\n",
    "        [ 0.1876,  0.8921],\n",
    "        [ 0.9759, -0.2606],\n",
    "        [ 0.9759, -0.2606],\n",
    "        [ 0.9759, -0.266],\n",
    "        [ 0.9759, -0.2606]])\n",
    "```\n",
    "\n",
    "Lastly, the magic happens when we try to do the same with multi dimensional lists as well:\n",
    "```python\n",
    "embeddingLookUpMatrix[[1, 0], [1, 1]]\n",
    "```\n",
    "\n",
    "Which results in:\n",
    "```python\n",
    "tensor([ 1.0454, -0.9078])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ec17e65-4e17-442c-a9c8-c8d0db40377e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 0.3370,  0.9905],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 1.9879, -0.6847]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905],\n",
       "         [ 1.4331, -0.6318]],\n",
       "\n",
       "        [[ 0.3370,  0.9905],\n",
       "         [ 1.4331, -0.6318],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 1.4331, -0.6318],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 0.6345, -1.5687]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 0.6345, -1.5687],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 0.6345, -1.5687],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 1.9879, -0.6847]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905],\n",
       "         [ 1.4331, -0.6318]],\n",
       "\n",
       "        [[ 0.3370,  0.9905],\n",
       "         [ 1.4331, -0.6318],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 1.4331, -0.6318],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 1.2893,  1.3174]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905],\n",
       "         [ 1.4331, -0.6318]],\n",
       "\n",
       "        [[ 0.3370,  0.9905],\n",
       "         [ 1.4331, -0.6318],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 1.4331, -0.6318],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [-0.3376, -0.0277]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 0.5820, -0.2370],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905]],\n",
       "\n",
       "        [[ 1.1564, -1.5507],\n",
       "         [ 0.3370,  0.9905],\n",
       "         [ 1.4331, -0.6318]],\n",
       "\n",
       "        [[ 0.3370,  0.9905],\n",
       "         [ 1.4331, -0.6318],\n",
       "         [-1.1567, -0.1717]],\n",
       "\n",
       "        [[ 1.4331, -0.6318],\n",
       "         [-1.1567, -0.1717],\n",
       "         [-1.1567, -0.1717]],\n",
       "\n",
       "        [[-1.1567, -0.1717],\n",
       "         [-1.1567, -0.1717],\n",
       "         [ 0.6345, -1.5687]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we can now easily do\n",
    "embeddingLookUpMatrix[inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3edb92e9-8f70-444a-8129-3764415a5519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 3, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also check the shape of this\n",
    "embeddingLookUpMatrix[inputs].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d0c59-9e9f-40e2-a166-1e2dcd7a37dc",
   "metadata": {},
   "source": [
    "We see that the size of this index is the shape of the original size of the dataset with a 2-dimensional embedding vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc906b2-34d9-485b-88a6-b97ec2493cd4",
   "metadata": {},
   "source": [
    "So if we do:\n",
    "```python\n",
    "# Input of 5th block and 3rd index of the block\n",
    "inputs[5,2]\n",
    "```\n",
    "\n",
    "It gives us:\n",
    "```python\n",
    "tensor(14)\n",
    "```\n",
    "\n",
    "We can look that vector up by doing:\n",
    "```python\n",
    "embeddingLookUpMatrix[inputs][5,2]\n",
    "```\n",
    "\n",
    "Which gives us the corresponding vector of the item specified:\n",
    "```python\n",
    "tensor([ 1.7724, -0.9331])\n",
    "```\n",
    "\n",
    "We can verify the same by doing:\n",
    "```python\n",
    "embeddingLookUpMatrix[14]\n",
    "```\n",
    "\n",
    "Gives the same output:\n",
    "```python\n",
    "tensor([ 1.7724, -0.9331])\r",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a963c5f-13f5-47b7-84d3-33eae2eb22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can now define our embedding into a variable\n",
    "embedding = embeddingLookUpMatrix[inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d716f8-7e83-4b80-a994-5e8781195eef",
   "metadata": {},
   "source": [
    "# Constructing Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273ea35-ab3b-4dec-ab40-7c4883f7b323",
   "metadata": {},
   "source": [
    "Let's understand what we will initially have in the hidden layer.\n",
    "\n",
    "1. The hidden layer will have it's own weights & biases\n",
    "2. The hidden layer will have it's own neurons which will act as a hyper-parameter to set the number of neurons we want in this layer\n",
    "\n",
    "So let's initialize weights and biases for now...\n",
    "\n",
    "Note: The size of the weights will be based on the block size of the inputs and its corresponding vector embedding.\\\n",
    "Thus,\n",
    "\n",
    "$$\\text{Hidden Layer Size} = [(\\text{Block Size} * \\text{Vector Embedding Dimensions}), \\text{Number of Neurons(Hyperparameter)}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f11e60b6-9b9b-40c0-b854-0d4028481fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can initialize the number of neurons we want in the hidden layer\n",
    "numberOfHiddenLayerNeurons = 100\n",
    "# Then we can randomly initialize the weights of the hidden layer\n",
    "weightsOfHiddenLayer = torch.randn((inputBlockSize*embeddingFeatureSpaceLength), numberOfHiddenLayerNeurons)\n",
    "# Then we can initialize the corresponding biases as well\n",
    "biasesOfHiddenLayer = torch.randn(numberOfHiddenLayerNeurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "379233f4-d2dd-4cfe-b5d5-9723663d38dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Weights of Hidden Layer: torch.Size([6, 100])\n",
      "Shape of Biases of Hidden Layer: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# We can check the shapes of our hidden layer weights and hidden layer biases\n",
    "print(\"Shape of Weights of Hidden Layer:\", weightsOfHiddenLayer.shape)\n",
    "print(\"Shape of Biases of Hidden Layer:\", biasesOfHiddenLayer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb8329-506e-4690-aa0b-76626363adac",
   "metadata": {},
   "source": [
    "Now that we have the weights and biases initialized for our hidden layer.\n",
    "\n",
    "By convention we would like to do something like:\n",
    "$$\\text{Layer Computation} = \\text{Embeddings} * \\text{Weights} + \\text{Biases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1dd49-7470-434c-bff9-747c1ff7017a",
   "metadata": {},
   "source": [
    "But for our case:\n",
    "\n",
    "Embeddings are in the shape of the [number of blocks in all the names, block size, vector dimension size]\\\n",
    "for example, [37, 3, 2]\\\n",
    "& Weights are in the shape of [(block size * vector dimension size), number of neurons (hyperparameter)]\\\n",
    "for example, [6, 100]\n",
    "\n",
    "And thus, we cannot just simply multiply these matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f37a10-b884-47a9-8725-ac8a2cc46cca",
   "metadata": {},
   "source": [
    "There are a numerous ways to do this,\\\n",
    "Either we can convert [37, 3, 2] ---> [37, 6]\\\n",
    "Or we can convert [6, 100] ---> [3, 2, 100]\n",
    "\n",
    "We will stick with the first one, because it is fairly simpler and would reduce the complexity to understand the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386218c-e002-4531-a422-314e2b0da870",
   "metadata": {},
   "source": [
    "I invite you to also look into the documentation of <a href=\"https://pytorch.org/docs/stable/index.html\">PyTorch</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210401e5-8d41-419c-8bde-48a891b28a22",
   "metadata": {},
   "source": [
    "According to the official documentation,\n",
    "\n",
    "**TORCH.CAT**\n",
    "Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
    "\n",
    "```python\n",
    "torch.cat(tensors, dim=0, *, out=None) → Tensor\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "- tensors (sequence of Tensors) – any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.\n",
    "- dim (int, optional) – the dimension over which the tensors are concatenated\n",
    "\n",
    "Keyword Arguments:\n",
    "- out (Tensor, optional) – the output tensor.\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> x = torch.randn(2, 3)\n",
    ">>> x\n",
    "tensor([[ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497]])\n",
    ">>> torch.cat((x, x, x), 0)\n",
    "tensor([[ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497],\n",
    "        [ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497],\n",
    "        [ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497]])\n",
    ">>> torch.cat((x, x, x), 1)\n",
    "tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n",
    "         -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n",
    "         -0.5790,  0.1497]])\n",
    "```\n",
    "\n",
    "So we can do:\n",
    "```python\n",
    "# Pickout the Embedding along the dimension 0, 1 & 2 and concatenate them along dimension 1\n",
    "# Each embedding[:, n, :] gives us the 3x2 embeddings\n",
    "torch.cat([embedding[:, 0, :], embedding[:, 1, :], embedding[:, 2, :]], dim=1)\n",
    "```\n",
    "\n",
    "And its shape turns out to be:\n",
    "```python\n",
    "torch.Size([37, 6])\n",
    "```\n",
    "\n",
    "But this is kind of ugly and we have another method...\n",
    "\n",
    "**TORCH.UNBIND**\n",
    "Removes a tensor dimension.\n",
    "\n",
    "Returns a tuple of all slices along a given dimension, already without it.\n",
    "\n",
    "```python\n",
    "torch.unbind(input, dim=0) → seq\n",
    "```\n",
    "Parameters:\n",
    "- input (Tensor) – the tensor to unbind\n",
    "- dim (int) – dimension to remove\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> torch.unbind(torch.tensor([[1, 2, 3],\n",
    ">>>                            [4, 5, 6],\n",
    ">>>                            [7, 8, 9]]))\n",
    "(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\n",
    "```\n",
    "\n",
    "So now we can do:\n",
    "```python\n",
    "torch.cat(torch.unbind(embedding, dim=1), dim=1)\n",
    "```\n",
    "\n",
    "Whose shape also turns out to be:\n",
    "```python\n",
    "torch.Size([37, 6])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0fbba-7b41-4e8b-9570-3a14aeb51038",
   "metadata": {},
   "source": [
    "We have a third way of doing the same thing.\n",
    "\n",
    "Its called:\\\n",
    "**TORCH.TENSOR.VIEW**\n",
    "\n",
    "Which gives me the paternity to explain some of the features of the internals of the PyTorch Library.\n",
    "\n",
    "We have an interesting blog post <a href=\"http://blog.ezyang.com/2019/05/pytorch-internals/\">here</a> by Edward Z. Yang which you can go through to understand more about this.\n",
    "\n",
    "To explain *TORCH.TENSOR.VIEW*.\\\n",
    "Let's take an example and explain each step one by one...\n",
    "\n",
    "For example,\\\n",
    "If we do:\n",
    "```python\n",
    "torch.arange(0,18)\n",
    "```\n",
    "\n",
    "It gives us:\n",
    "```python\n",
    "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
    "```\n",
    "\n",
    "This can also be viewed as:\n",
    "```python\n",
    "torch.arange(0,18).view(9,2)\n",
    "```\n",
    "\n",
    "Which gives us:\n",
    "```python\n",
    "tensor([[ 0,  1],\n",
    "        [ 2,  3],\n",
    "        [ 4,  5],\n",
    "        [ 6,  7],\n",
    "        [ 8,  9],\n",
    "        [10, 11],\n",
    "        [12, 13],\n",
    "        [14, 15],\n",
    "        [16, 17]])\n",
    "```\n",
    "\n",
    "This can also be written as:\n",
    "```python\n",
    "torch.arange(0,18).view(3,3,2)\n",
    "```\n",
    "\n",
    "Which gives us:\n",
    "```python\n",
    "tensor([[[ 0,  1],\n",
    "         [ 2,  3],\n",
    "         [ 4,  5]],\n",
    "\n",
    "        [[ 6,  7],\n",
    "         [ 8,  9],\n",
    "         [10, 11]],\n",
    "\n",
    "        [[12, 13],\n",
    "         [14, 15],\n",
    "         [16, 17]]])\n",
    "```\n",
    "\n",
    "So,\\\n",
    "If we have an embedding of size say: [37, 3, 2]\\\n",
    "We can essentially do:\n",
    "```python\n",
    "embedding.view(37,6)\n",
    "```\n",
    "\n",
    "We can also verify the result to be the same by doing\n",
    "```python\n",
    "embedding.view(37,6) == torch.cat(torch.unbind(embedding, dim=1), dim=1)\n",
    "```\n",
    "\n",
    "Resulting in all True values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00327478-b784-4b4b-a801-66610be1a353",
   "metadata": {},
   "source": [
    "So, to get the *hidden-states*, we can simply run:\n",
    "\n",
    "```python\n",
    "embedding.view(37,6)\n",
    "```\n",
    "\n",
    "to get all the hidden layer states as:\n",
    "\n",
    "```python\n",
    "hiddenLayerStates = embedding.view(37,6) @ weightsOfHiddenLayer + biasesOfHiddenLayer\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "Before we use this, you see how we are using 37 and 6 as a number which is hard-coded and does not make our model very flexible?\\\n",
    "Let's fix it by using *-1* instead of *37* to specify that it should take all the inputs, and use *inputBlockSize\\*embeddingFeatureSpaceLength* instead of *6*.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Also, remembering our original multi-layer perceptron approach, we had something called tanh() non-linearity.\n",
    "\n",
    "So, instead we would want to know what tanh() is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b1f17-80b3-4c0d-b619-f672b0f8b991",
   "metadata": {},
   "source": [
    "So **what is tanh()**?\n",
    "\n",
    "In order to answer that we have to understand a few more things,\n",
    "\n",
    "In mathematics, the trigonometric functions are real functions which relate an angle of a right-angled triangle to ratios of two side lengths.\n",
    "![Trigonometry](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/Sinus_und_Kosinus_am_Einheitskreis_1.svg/250px-Sinus_und_Kosinus_am_Einheitskreis_1.svg.png)\n",
    "\n",
    "In mathematics, hyperbolic functions are analogues of the ordinary trigonometric functions, but defined using the hyperbola rather than the circle.\n",
    "![Hyperbola v/s Parabola](ExplanationMedia/Images/Hyperbola-vs-Parabola.png)\n",
    "\n",
    "Here are some of the most used hyperbolic functions:\n",
    "![sinhcoshtanh](https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Sinh_cosh_tanh.svg/300px-Sinh_cosh_tanh.svg.png)\n",
    "\n",
    "So to answer the question,\\\n",
    "**tanh() is a hyperbolic function**, or a non-linearity we use to get a number between **-1 and 1** and ranges between $$-\\infty\\text{ and }\\infty$$\n",
    "\n",
    "This is the formula for tanh():\\\n",
    "$$\\tanh x = \\frac{\\sinh x}{\\cosh x} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = \\frac{e^{2x} - 1}{e^{2x} + 1}$$\n",
    "\n",
    "![tanh](https://miro.medium.com/v2/resize:fit:443/1*WeuJzmlt3iNVWsUsvf24Eg.png)\n",
    "\n",
    "So, we can now do:\n",
    "```python\n",
    "hiddenLayerStates = torch.tanh(embedding.view(37,6) @ weightsOfHiddenLayer + biasesOfHiddenLayer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3317bb5-8771-4051-ae04-e8656ad0e08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8461, -0.1966, -0.7330,  ...,  0.9426, -0.9570,  0.2508],\n",
      "        [ 0.9832,  0.7964, -0.6979,  ...,  0.9806, -0.9932, -0.1872],\n",
      "        [ 1.0000, -0.1505,  0.4600,  ...,  0.9999, -0.9906,  0.9022],\n",
      "        ...,\n",
      "        [ 0.9981, -0.9832,  0.6876,  ..., -0.2359, -0.9823,  0.9468],\n",
      "        [-0.1683,  0.9544,  0.7570,  ...,  0.9999, -0.9999,  0.9696],\n",
      "        [ 0.9985,  0.8951,  0.9982,  ...,  0.8906, -0.5284,  0.9913]])\n"
     ]
    }
   ],
   "source": [
    "# So putting all the above things we learnt together, we get\n",
    "hiddenLayerStates = torch.tanh(embedding.view(-1, inputBlockSize*embeddingFeatureSpaceLength) @ weightsOfHiddenLayer + biasesOfHiddenLayer)\n",
    "print(hiddenLayerStates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cec60d4e-79f7-426e-9d64-f1318f79b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 100])\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the shape of the hidden layer states look like\n",
    "print(hiddenLayerStates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e9161-2cd8-40a3-8249-d927d9f8dbb5",
   "metadata": {},
   "source": [
    "Keep in mind that we need to be careful with the broadcasting rules of the '+' of:\n",
    "```python\n",
    "hiddenLayerStates = torch.tanh(embedding.view(37,6) @ weightsOfHiddenLayer + biasesOfHiddenLayer)\n",
    "```\n",
    "\n",
    "I will move on to the next section, but to check the broadcasting rules you can refer to <a href=\"https://pytorch.org/docs/stable/notes/broadcasting.html\">original documentation</a> or refer to my <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave Notebook</a> for a more simpler explanation.\n",
    "\n",
    "So let's create our final layer next..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d5f57-87be-43e9-a953-f51e8f62f80a",
   "metadata": {},
   "source": [
    "# Constructing Final Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515161be-f757-4ba5-b9df-5595c180c243",
   "metadata": {},
   "source": [
    "Looking at the shape that we have right now :\n",
    "\n",
    "```python\n",
    "torch.Size([37, 100])\n",
    "```\n",
    "\n",
    "We see that we have a *100* neurons, taking *37* inputs...\n",
    "\n",
    "We understand that each of these neurons would be the inputs to our final layer,\\\n",
    "Thus, we have to take a layer where it takes *100* inputs and produces the output of *27*.\n",
    "\n",
    "Why **27**?\n",
    "\n",
    "Because, we would be interested in the index of the output now.\n",
    "\n",
    "So we will do:\n",
    "```python\n",
    "# We can initialize the number of neurons we have in the final layer\n",
    "numberOfFinalLayerOutputs = 27\n",
    "# Then we can randomly initialize the weights of the final layer\n",
    "weightsOfFinalLayer = torch.randn(numberOfHiddenLayerNeurons, numberOfFinalLayerOutputs)\n",
    "# Then we can initialize the corresponding biases as well\n",
    "biasesOfFinalLayer = torch.randn(numberOfFinalLayerOutputs)\n",
    "```\n",
    "\n",
    "Therefore,\\\n",
    "The **logits** our final layer will produce would be:\n",
    "$$\\text{Logits} = \\text{hiddenLayerStates} * \\text{weightsOfFinalLayer} + \\text{biasesOfFinalLayer}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99c85ff5-f44f-4eb7-b977-3f9a942b9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's construct our final layer\n",
    "# We can initialize the number of neurons we have in the final layer\n",
    "numberOfFinalLayerOutputs = 27\n",
    "# Then we can randomly initialize the weights of the final layer\n",
    "weightsOfFinalLayer = torch.randn(numberOfHiddenLayerNeurons, numberOfFinalLayerOutputs)\n",
    "# Then we can initialize the corresponding biases as well\n",
    "biasesOfFinalLayer = torch.randn(numberOfFinalLayerOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e95707b2-dfc5-4589-ad73-0eeb68c81bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute logits now\n",
    "logits = hiddenLayerStates @ weightsOfFinalLayer + biasesOfFinalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baa299fb-d459-48dd-94d7-e15b585dd6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.2723e+00,  6.7407e+00,  5.4457e+00,  1.1902e+00,  8.3090e+00,\n",
      "         -1.1993e+01,  1.1749e+01,  1.0155e+01, -1.2423e+01,  6.8171e+00,\n",
      "         -1.0491e+01,  1.2763e+01, -3.8437e+00, -3.7703e+00,  9.8887e+00,\n",
      "          5.3221e+00,  1.5256e+00, -1.1038e+01, -8.4957e+00, -8.4986e+00,\n",
      "          6.8459e+00, -4.4593e+00,  7.1014e-01, -8.8109e+00,  1.8097e+00,\n",
      "         -1.3912e+01, -1.2222e+01],\n",
      "        [-6.5779e+00,  1.1229e+01,  1.1490e+01, -5.1242e-01,  2.7115e+00,\n",
      "         -1.4522e+01,  3.8286e+00,  1.0287e+01, -2.5238e+01,  8.7356e+00,\n",
      "         -6.7601e+00,  9.6300e+00, -5.4677e+00, -7.5854e+00,  7.6966e+00,\n",
      "         -9.0475e+00, -6.8988e+00, -1.5325e+01, -2.3821e+00, -1.6014e+01,\n",
      "          1.6096e+00,  2.9128e-01,  8.5502e+00, -1.7395e+01,  4.0671e+00,\n",
      "         -1.4717e+01, -3.6569e+00],\n",
      "        [-9.8583e+00,  1.1538e+01,  5.3546e+00,  8.9338e-02,  3.4122e+00,\n",
      "         -1.6844e+01,  6.7176e+00,  7.0386e+00, -1.4026e+01,  7.0354e+00,\n",
      "         -9.2277e+00,  4.9584e+00, -4.7631e+00, -8.8140e+00,  2.6388e-01,\n",
      "         -3.2911e+00, -6.8332e+00, -1.1655e+01,  2.0546e+00, -1.2694e+01,\n",
      "          6.2232e+00, -1.2514e+01,  1.2552e+00, -1.5673e+01,  6.3891e+00,\n",
      "         -1.5508e+01, -5.4430e+00],\n",
      "        [ 2.3982e+00,  1.2692e+01, -4.2725e+00,  6.4750e+00,  8.3266e+00,\n",
      "         -1.2346e+01,  1.2022e+01,  8.3476e+00, -3.2687e+00, -4.4843e-02,\n",
      "         -4.0846e+00,  1.5591e+00,  2.4473e+00, -6.2936e+00,  4.5483e+00,\n",
      "          1.8995e+01,  9.1765e+00, -6.3726e+00, -5.3111e+00, -2.8632e+00,\n",
      "          8.0766e+00, -1.3979e+01, -1.3819e+01,  1.5449e-01, -4.8694e+00,\n",
      "         -1.0656e+01, -5.9974e+00],\n",
      "        [-6.0434e+00,  1.4242e+01,  2.3259e+01,  2.4139e-01,  1.3459e+01,\n",
      "         -3.6342e+00,  6.2080e+00,  1.6726e+01, -1.2331e+01,  5.2402e+00,\n",
      "          1.3827e+00,  1.0291e+01,  4.8508e+00, -2.4218e-01,  1.5965e+01,\n",
      "         -3.1015e+00,  3.3401e+00, -1.6789e+01, -1.1596e+01, -9.9958e+00,\n",
      "          1.6559e+00,  2.2025e-01,  9.9445e+00, -4.1774e+00, -2.6991e+00,\n",
      "         -2.1451e+01, -1.8959e+00],\n",
      "        [-2.8921e-01,  2.8238e+00,  3.0579e+00,  1.5992e-01,  1.3183e+00,\n",
      "         -1.8573e+01,  9.5364e+00,  3.5543e+00, -1.5904e+01,  1.3713e+01,\n",
      "         -1.4933e+01, -2.2650e+00, -1.5791e+01, -6.2251e+00,  4.4290e+00,\n",
      "         -3.0591e+00, -7.0640e+00, -9.9654e+00,  4.4596e+00, -1.0693e+01,\n",
      "          1.0015e+00, -9.1677e-01,  1.2112e+00, -1.1996e+01,  7.5221e+00,\n",
      "         -1.1449e+01, -1.3367e+01],\n",
      "        [-5.2723e+00,  6.7407e+00,  5.4457e+00,  1.1902e+00,  8.3090e+00,\n",
      "         -1.1993e+01,  1.1749e+01,  1.0155e+01, -1.2423e+01,  6.8171e+00,\n",
      "         -1.0491e+01,  1.2763e+01, -3.8437e+00, -3.7703e+00,  9.8887e+00,\n",
      "          5.3221e+00,  1.5256e+00, -1.1038e+01, -8.4957e+00, -8.4986e+00,\n",
      "          6.8459e+00, -4.4593e+00,  7.1014e-01, -8.8109e+00,  1.8097e+00,\n",
      "         -1.3912e+01, -1.2222e+01],\n",
      "        [-6.5779e+00,  1.1229e+01,  1.1490e+01, -5.1242e-01,  2.7115e+00,\n",
      "         -1.4522e+01,  3.8286e+00,  1.0287e+01, -2.5238e+01,  8.7356e+00,\n",
      "         -6.7601e+00,  9.6300e+00, -5.4677e+00, -7.5854e+00,  7.6966e+00,\n",
      "         -9.0475e+00, -6.8988e+00, -1.5325e+01, -2.3821e+00, -1.6014e+01,\n",
      "          1.6096e+00,  2.9128e-01,  8.5502e+00, -1.7395e+01,  4.0671e+00,\n",
      "         -1.4717e+01, -3.6569e+00],\n",
      "        [-9.8583e+00,  1.1538e+01,  5.3546e+00,  8.9338e-02,  3.4122e+00,\n",
      "         -1.6844e+01,  6.7176e+00,  7.0386e+00, -1.4026e+01,  7.0354e+00,\n",
      "         -9.2277e+00,  4.9584e+00, -4.7631e+00, -8.8140e+00,  2.6388e-01,\n",
      "         -3.2911e+00, -6.8332e+00, -1.1655e+01,  2.0546e+00, -1.2694e+01,\n",
      "          6.2232e+00, -1.2514e+01,  1.2552e+00, -1.5673e+01,  6.3891e+00,\n",
      "         -1.5508e+01, -5.4430e+00],\n",
      "        [ 2.3982e+00,  1.2692e+01, -4.2725e+00,  6.4750e+00,  8.3266e+00,\n",
      "         -1.2346e+01,  1.2022e+01,  8.3476e+00, -3.2687e+00, -4.4843e-02,\n",
      "         -4.0846e+00,  1.5591e+00,  2.4473e+00, -6.2936e+00,  4.5483e+00,\n",
      "          1.8995e+01,  9.1765e+00, -6.3726e+00, -5.3111e+00, -2.8632e+00,\n",
      "          8.0766e+00, -1.3979e+01, -1.3819e+01,  1.5449e-01, -4.8694e+00,\n",
      "         -1.0656e+01, -5.9974e+00],\n",
      "        [-4.3415e+00,  1.2153e+01,  2.4982e+01,  2.8371e+00,  1.9159e+01,\n",
      "         -2.5395e+00,  1.1269e+01,  1.4452e+01, -6.6369e+00,  8.1628e+00,\n",
      "         -3.3838e-01,  1.3409e+01,  5.6499e+00, -1.1889e+00,  2.4045e+01,\n",
      "          4.4576e+00,  2.5913e+00, -1.4064e+01, -1.5232e+01, -8.3151e+00,\n",
      "          2.5219e+00,  3.9748e+00,  3.0204e+00, -4.9768e+00, -3.2549e+00,\n",
      "         -2.1279e+01, -4.3785e+00],\n",
      "        [-1.6119e+00,  7.4458e+00,  6.4428e+00, -1.7951e+00, -6.4009e+00,\n",
      "         -1.8052e+01,  3.5060e+00,  5.9239e+00, -2.1811e+01,  1.2242e+01,\n",
      "         -5.0494e+00,  4.6133e-01, -1.5742e+01, -9.3395e+00,  6.0920e+00,\n",
      "         -9.5717e+00, -1.2515e+01, -9.9566e+00, -9.9832e-01, -4.4899e+00,\n",
      "          4.0753e+00, -1.0573e+01,  9.0842e+00, -1.7317e+01,  2.3325e+00,\n",
      "         -9.7250e+00, -1.3683e+01],\n",
      "        [-7.6448e+00,  1.4344e+01,  2.4916e+00, -1.4438e+00,  1.6135e-01,\n",
      "         -1.3858e+01, -1.3324e+00,  8.6855e+00, -8.5439e+00,  9.1472e-01,\n",
      "         -1.9827e+00,  7.1787e+00, -2.4519e+00, -7.6212e+00,  2.6437e+00,\n",
      "         -3.2885e+00, -5.2963e-01, -8.5383e+00,  1.9405e+00, -7.1237e+00,\n",
      "          9.0301e+00, -1.5004e+01,  3.4363e+00, -1.0304e+01,  2.4604e+00,\n",
      "         -2.0095e+01,  1.1196e-01],\n",
      "        [-1.1481e+01,  1.6868e+01,  6.6207e+00,  1.0836e+00,  6.5637e+00,\n",
      "         -1.5603e+01,  3.0601e+00,  9.8628e+00, -1.5298e+01,  2.0608e+00,\n",
      "         -8.2240e+00,  9.2101e+00,  1.6978e-01, -6.8305e+00, -3.0868e-02,\n",
      "         -1.8299e+00, -2.0762e+00, -1.3312e+01,  1.3099e-02, -1.5867e+01,\n",
      "          5.2605e+00, -1.4979e+01, -1.2973e+00, -1.3639e+01,  9.4908e+00,\n",
      "         -1.8718e+01, -1.1147e+00],\n",
      "        [-5.6631e+00,  1.6826e+01,  9.7327e+00,  2.2904e+00,  8.0594e+00,\n",
      "         -1.4363e+01,  8.7838e+00,  7.0024e+00, -2.0684e+01,  7.3115e+00,\n",
      "         -1.1041e+01,  8.8000e+00, -5.4906e+00, -1.1932e+01,  1.1786e+00,\n",
      "          5.3139e+00, -3.8438e+00, -1.6942e+01, -4.0669e+00, -1.7144e+01,\n",
      "          6.1956e+00, -9.3802e+00, -2.8974e+00, -1.0830e+01,  9.5636e+00,\n",
      "         -1.8198e+01, -1.8136e+00],\n",
      "        [-5.2723e+00,  6.7407e+00,  5.4457e+00,  1.1902e+00,  8.3090e+00,\n",
      "         -1.1993e+01,  1.1749e+01,  1.0155e+01, -1.2423e+01,  6.8171e+00,\n",
      "         -1.0491e+01,  1.2763e+01, -3.8437e+00, -3.7703e+00,  9.8887e+00,\n",
      "          5.3221e+00,  1.5256e+00, -1.1038e+01, -8.4957e+00, -8.4986e+00,\n",
      "          6.8459e+00, -4.4593e+00,  7.1014e-01, -8.8109e+00,  1.8097e+00,\n",
      "         -1.3912e+01, -1.2222e+01],\n",
      "        [-6.5779e+00,  1.1229e+01,  1.1490e+01, -5.1242e-01,  2.7115e+00,\n",
      "         -1.4522e+01,  3.8286e+00,  1.0287e+01, -2.5238e+01,  8.7356e+00,\n",
      "         -6.7601e+00,  9.6300e+00, -5.4677e+00, -7.5854e+00,  7.6966e+00,\n",
      "         -9.0475e+00, -6.8988e+00, -1.5325e+01, -2.3821e+00, -1.6014e+01,\n",
      "          1.6096e+00,  2.9128e-01,  8.5502e+00, -1.7395e+01,  4.0671e+00,\n",
      "         -1.4717e+01, -3.6569e+00],\n",
      "        [-9.8583e+00,  1.1538e+01,  5.3546e+00,  8.9338e-02,  3.4122e+00,\n",
      "         -1.6844e+01,  6.7176e+00,  7.0386e+00, -1.4026e+01,  7.0354e+00,\n",
      "         -9.2277e+00,  4.9584e+00, -4.7631e+00, -8.8140e+00,  2.6388e-01,\n",
      "         -3.2911e+00, -6.8332e+00, -1.1655e+01,  2.0546e+00, -1.2694e+01,\n",
      "          6.2232e+00, -1.2514e+01,  1.2552e+00, -1.5673e+01,  6.3891e+00,\n",
      "         -1.5508e+01, -5.4430e+00],\n",
      "        [ 2.3982e+00,  1.2692e+01, -4.2725e+00,  6.4750e+00,  8.3266e+00,\n",
      "         -1.2346e+01,  1.2022e+01,  8.3476e+00, -3.2687e+00, -4.4843e-02,\n",
      "         -4.0846e+00,  1.5591e+00,  2.4473e+00, -6.2936e+00,  4.5483e+00,\n",
      "          1.8995e+01,  9.1765e+00, -6.3726e+00, -5.3111e+00, -2.8632e+00,\n",
      "          8.0766e+00, -1.3979e+01, -1.3819e+01,  1.5449e-01, -4.8694e+00,\n",
      "         -1.0656e+01, -5.9974e+00],\n",
      "        [-4.3415e+00,  1.2153e+01,  2.4982e+01,  2.8371e+00,  1.9159e+01,\n",
      "         -2.5395e+00,  1.1269e+01,  1.4452e+01, -6.6369e+00,  8.1628e+00,\n",
      "         -3.3838e-01,  1.3409e+01,  5.6499e+00, -1.1889e+00,  2.4045e+01,\n",
      "          4.4576e+00,  2.5913e+00, -1.4064e+01, -1.5232e+01, -8.3151e+00,\n",
      "          2.5219e+00,  3.9748e+00,  3.0204e+00, -4.9768e+00, -3.2549e+00,\n",
      "         -2.1279e+01, -4.3785e+00],\n",
      "        [-1.6119e+00,  7.4458e+00,  6.4428e+00, -1.7951e+00, -6.4009e+00,\n",
      "         -1.8052e+01,  3.5060e+00,  5.9239e+00, -2.1811e+01,  1.2242e+01,\n",
      "         -5.0494e+00,  4.6133e-01, -1.5742e+01, -9.3395e+00,  6.0920e+00,\n",
      "         -9.5717e+00, -1.2515e+01, -9.9566e+00, -9.9832e-01, -4.4899e+00,\n",
      "          4.0753e+00, -1.0573e+01,  9.0842e+00, -1.7317e+01,  2.3325e+00,\n",
      "         -9.7250e+00, -1.3683e+01],\n",
      "        [ 1.0845e+00,  1.1706e+01, -8.4931e+00,  8.7850e+00,  8.8514e+00,\n",
      "         -8.3435e+00,  1.5320e+01,  2.2116e+00, -5.0549e+00,  1.8879e+00,\n",
      "         -6.1558e+00,  6.2258e+00,  2.0407e+00, -7.5081e+00,  7.1991e+00,\n",
      "          1.7666e+01,  8.5011e+00, -5.4354e+00, -6.1884e+00, -7.0214e+00,\n",
      "          1.0770e+01, -8.3026e+00, -1.1844e+01,  7.6494e+00, -3.6138e+00,\n",
      "         -1.5321e+01, -3.0585e+00],\n",
      "        [-5.2723e+00,  6.7407e+00,  5.4457e+00,  1.1902e+00,  8.3090e+00,\n",
      "         -1.1993e+01,  1.1749e+01,  1.0155e+01, -1.2423e+01,  6.8171e+00,\n",
      "         -1.0491e+01,  1.2763e+01, -3.8437e+00, -3.7703e+00,  9.8887e+00,\n",
      "          5.3221e+00,  1.5256e+00, -1.1038e+01, -8.4957e+00, -8.4986e+00,\n",
      "          6.8459e+00, -4.4593e+00,  7.1014e-01, -8.8109e+00,  1.8097e+00,\n",
      "         -1.3912e+01, -1.2222e+01],\n",
      "        [-6.5779e+00,  1.1229e+01,  1.1490e+01, -5.1242e-01,  2.7115e+00,\n",
      "         -1.4522e+01,  3.8286e+00,  1.0287e+01, -2.5238e+01,  8.7356e+00,\n",
      "         -6.7601e+00,  9.6300e+00, -5.4677e+00, -7.5854e+00,  7.6966e+00,\n",
      "         -9.0475e+00, -6.8988e+00, -1.5325e+01, -2.3821e+00, -1.6014e+01,\n",
      "          1.6096e+00,  2.9128e-01,  8.5502e+00, -1.7395e+01,  4.0671e+00,\n",
      "         -1.4717e+01, -3.6569e+00],\n",
      "        [-9.8583e+00,  1.1538e+01,  5.3546e+00,  8.9338e-02,  3.4122e+00,\n",
      "         -1.6844e+01,  6.7176e+00,  7.0386e+00, -1.4026e+01,  7.0354e+00,\n",
      "         -9.2277e+00,  4.9584e+00, -4.7631e+00, -8.8140e+00,  2.6388e-01,\n",
      "         -3.2911e+00, -6.8332e+00, -1.1655e+01,  2.0546e+00, -1.2694e+01,\n",
      "          6.2232e+00, -1.2514e+01,  1.2552e+00, -1.5673e+01,  6.3891e+00,\n",
      "         -1.5508e+01, -5.4430e+00],\n",
      "        [ 2.3982e+00,  1.2692e+01, -4.2725e+00,  6.4750e+00,  8.3266e+00,\n",
      "         -1.2346e+01,  1.2022e+01,  8.3476e+00, -3.2687e+00, -4.4843e-02,\n",
      "         -4.0846e+00,  1.5591e+00,  2.4473e+00, -6.2936e+00,  4.5483e+00,\n",
      "          1.8995e+01,  9.1765e+00, -6.3726e+00, -5.3111e+00, -2.8632e+00,\n",
      "          8.0766e+00, -1.3979e+01, -1.3819e+01,  1.5449e-01, -4.8694e+00,\n",
      "         -1.0656e+01, -5.9974e+00],\n",
      "        [-4.3415e+00,  1.2153e+01,  2.4982e+01,  2.8371e+00,  1.9159e+01,\n",
      "         -2.5395e+00,  1.1269e+01,  1.4452e+01, -6.6369e+00,  8.1628e+00,\n",
      "         -3.3838e-01,  1.3409e+01,  5.6499e+00, -1.1889e+00,  2.4045e+01,\n",
      "          4.4576e+00,  2.5913e+00, -1.4064e+01, -1.5232e+01, -8.3151e+00,\n",
      "          2.5219e+00,  3.9748e+00,  3.0204e+00, -4.9768e+00, -3.2549e+00,\n",
      "         -2.1279e+01, -4.3785e+00],\n",
      "        [-1.6119e+00,  7.4458e+00,  6.4428e+00, -1.7951e+00, -6.4009e+00,\n",
      "         -1.8052e+01,  3.5060e+00,  5.9239e+00, -2.1811e+01,  1.2242e+01,\n",
      "         -5.0494e+00,  4.6133e-01, -1.5742e+01, -9.3395e+00,  6.0920e+00,\n",
      "         -9.5717e+00, -1.2515e+01, -9.9566e+00, -9.9832e-01, -4.4899e+00,\n",
      "          4.0753e+00, -1.0573e+01,  9.0842e+00, -1.7317e+01,  2.3325e+00,\n",
      "         -9.7250e+00, -1.3683e+01],\n",
      "        [ 4.8502e-01,  1.1282e+01, -1.4905e+00,  5.7769e-02, -2.6766e+00,\n",
      "         -7.3648e+00,  1.1753e+01,  7.5614e+00, -3.3403e+00,  7.1020e+00,\n",
      "         -2.5297e+00,  1.1349e+01, -1.9670e+00, -8.9373e+00,  6.6234e+00,\n",
      "          5.9593e+00,  4.9979e+00, -2.6979e+00,  3.8400e+00,  2.5735e+00,\n",
      "          7.3181e+00, -1.1986e+01, -7.2783e+00, -3.5606e+00, -1.6153e+00,\n",
      "         -1.3882e+01, -1.7317e+00],\n",
      "        [-5.2723e+00,  6.7407e+00,  5.4457e+00,  1.1902e+00,  8.3090e+00,\n",
      "         -1.1993e+01,  1.1749e+01,  1.0155e+01, -1.2423e+01,  6.8171e+00,\n",
      "         -1.0491e+01,  1.2763e+01, -3.8437e+00, -3.7703e+00,  9.8887e+00,\n",
      "          5.3221e+00,  1.5256e+00, -1.1038e+01, -8.4957e+00, -8.4986e+00,\n",
      "          6.8459e+00, -4.4593e+00,  7.1014e-01, -8.8109e+00,  1.8097e+00,\n",
      "         -1.3912e+01, -1.2222e+01],\n",
      "        [-6.5779e+00,  1.1229e+01,  1.1490e+01, -5.1242e-01,  2.7115e+00,\n",
      "         -1.4522e+01,  3.8286e+00,  1.0287e+01, -2.5238e+01,  8.7356e+00,\n",
      "         -6.7601e+00,  9.6300e+00, -5.4677e+00, -7.5854e+00,  7.6966e+00,\n",
      "         -9.0475e+00, -6.8988e+00, -1.5325e+01, -2.3821e+00, -1.6014e+01,\n",
      "          1.6096e+00,  2.9128e-01,  8.5502e+00, -1.7395e+01,  4.0671e+00,\n",
      "         -1.4717e+01, -3.6569e+00],\n",
      "        [-9.8583e+00,  1.1538e+01,  5.3546e+00,  8.9338e-02,  3.4122e+00,\n",
      "         -1.6844e+01,  6.7176e+00,  7.0386e+00, -1.4026e+01,  7.0354e+00,\n",
      "         -9.2277e+00,  4.9584e+00, -4.7631e+00, -8.8140e+00,  2.6388e-01,\n",
      "         -3.2911e+00, -6.8332e+00, -1.1655e+01,  2.0546e+00, -1.2694e+01,\n",
      "          6.2232e+00, -1.2514e+01,  1.2552e+00, -1.5673e+01,  6.3891e+00,\n",
      "         -1.5508e+01, -5.4430e+00],\n",
      "        [ 2.3982e+00,  1.2692e+01, -4.2725e+00,  6.4750e+00,  8.3266e+00,\n",
      "         -1.2346e+01,  1.2022e+01,  8.3476e+00, -3.2687e+00, -4.4843e-02,\n",
      "         -4.0846e+00,  1.5591e+00,  2.4473e+00, -6.2936e+00,  4.5483e+00,\n",
      "          1.8995e+01,  9.1765e+00, -6.3726e+00, -5.3111e+00, -2.8632e+00,\n",
      "          8.0766e+00, -1.3979e+01, -1.3819e+01,  1.5449e-01, -4.8694e+00,\n",
      "         -1.0656e+01, -5.9974e+00],\n",
      "        [-4.3415e+00,  1.2153e+01,  2.4982e+01,  2.8371e+00,  1.9159e+01,\n",
      "         -2.5395e+00,  1.1269e+01,  1.4452e+01, -6.6369e+00,  8.1628e+00,\n",
      "         -3.3838e-01,  1.3409e+01,  5.6499e+00, -1.1889e+00,  2.4045e+01,\n",
      "          4.4576e+00,  2.5913e+00, -1.4064e+01, -1.5232e+01, -8.3151e+00,\n",
      "          2.5219e+00,  3.9748e+00,  3.0204e+00, -4.9768e+00, -3.2549e+00,\n",
      "         -2.1279e+01, -4.3785e+00],\n",
      "        [-3.3183e+00,  2.1208e-01, -1.1587e+01, -7.1090e+00, -1.9799e+01,\n",
      "         -6.6065e+00,  4.7775e+00,  6.3306e+00, -4.3303e+00,  2.4838e+00,\n",
      "         -5.4478e+00,  1.4184e+01, -2.3101e+00, -9.1937e+00, -4.1193e+00,\n",
      "         -2.5640e+00, -1.0815e-01,  7.8967e-01,  8.0462e+00,  4.6252e+00,\n",
      "          4.4331e+00, -1.7424e+01,  2.3205e-01, -8.4581e+00, -3.9285e+00,\n",
      "         -2.1822e+00, -1.8367e+01],\n",
      "        [ 3.0278e+00,  4.0367e+00, -2.6195e+00, -1.2753e+00,  1.7043e+01,\n",
      "         -7.5156e+00,  1.3776e+01,  6.8684e+00,  1.2315e+01, -1.3623e+01,\n",
      "         -5.6516e+00,  5.6646e+00,  1.3305e+01,  5.3846e+00, -3.5566e+00,\n",
      "         -5.9279e+00,  1.0600e+01, -6.3075e+00, -8.8162e+00, -3.7465e+00,\n",
      "          3.2790e+00,  1.9855e+00, -6.6138e+00,  6.4170e+00, -1.0343e+01,\n",
      "         -1.1742e+00,  5.9592e+00],\n",
      "        [-1.0130e+01, -1.1708e+01,  1.8715e+01, -1.5099e+00,  5.6002e+00,\n",
      "         -1.7556e+01,  1.3594e+00,  9.5154e+00, -1.1950e+01,  5.2771e+00,\n",
      "         -8.7494e+00, -5.1836e+00, -4.9143e+00, -1.9623e+00,  6.7797e-01,\n",
      "         -2.6698e+01, -2.0013e+00, -9.5458e+00,  2.2121e-01, -1.7908e+01,\n",
      "         -6.0400e+00,  1.2068e+01, -1.7329e+00, -6.2205e+00,  2.4880e+01,\n",
      "          8.9436e+00, -3.6737e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Let's check the output\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e6d24c7-3ffb-4a64-85f1-67863ac426f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 27])\n"
     ]
    }
   ],
   "source": [
    "#Let's check the shape as well\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244beae-4a1a-493f-ae51-e59ffc59162a",
   "metadata": {},
   "source": [
    "So, we want to do exactly now what we did in our previous <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave Notebook</a>.\\\n",
    "We want to:\n",
    "1. Take the logits\n",
    "2. Exponentiate them\n",
    "3. Normalize them into a probability that sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54483c0b-b2d1-4820-9622-c1f6d884056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's get our probabilities back\n",
    "# Calculating counts from logits\n",
    "counts = logits.exp()\n",
    "# Normalizing counts to probabilities that sum to 1\n",
    "probabilities = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf21e938-b523-4920-9649-3a9252f9ab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.7155e-09, 1.6019e-03, 4.3874e-04, 6.2239e-06, 7.6865e-03, 1.1713e-11,\n",
      "         2.3968e-01, 4.8713e-02, 7.6167e-12, 1.7290e-03, 5.2588e-11, 6.6065e-01,\n",
      "         4.0539e-08, 4.3627e-08, 3.7305e-02, 3.8775e-04, 8.7040e-06, 3.0449e-11,\n",
      "         3.8685e-10, 3.8574e-10, 1.7796e-03, 2.1903e-08, 3.8511e-06, 2.8226e-10,\n",
      "         1.1564e-05, 1.7189e-12, 9.3201e-12],\n",
      "        [6.0114e-09, 3.2555e-01, 4.2259e-01, 2.5893e-06, 6.5058e-05, 2.1321e-12,\n",
      "         1.9881e-04, 1.2682e-01, 4.7295e-17, 2.6886e-02, 5.0098e-09, 6.5764e-02,\n",
      "         1.8244e-08, 2.1949e-09, 9.5128e-03, 5.0866e-10, 4.3611e-09, 9.5495e-13,\n",
      "         3.9920e-07, 4.7977e-13, 2.1615e-05, 5.7838e-06, 2.2336e-02, 1.2052e-13,\n",
      "         2.5236e-04, 1.7556e-12, 1.1156e-07],\n",
      "        [4.8819e-10, 9.5707e-01, 1.9745e-03, 1.0205e-05, 2.8307e-04, 4.5179e-13,\n",
      "         7.7166e-03, 1.0638e-02, 7.5594e-12, 1.0603e-02, 9.1720e-10, 1.3286e-03,\n",
      "         7.9691e-08, 1.3872e-09, 1.2151e-05, 3.4728e-07, 1.0055e-08, 8.0994e-11,\n",
      "         7.2830e-05, 2.8637e-11, 4.7064e-03, 3.4307e-11, 3.2743e-05, 1.4570e-12,\n",
      "         5.5556e-03, 1.7172e-12, 4.0378e-08],\n",
      "        [6.1796e-08, 1.8258e-03, 7.8325e-11, 3.6432e-06, 2.3206e-05, 2.4423e-14,\n",
      "         9.3456e-04, 2.3700e-05, 2.1372e-10, 5.3696e-09, 9.4509e-11, 2.6701e-08,\n",
      "         6.4904e-08, 1.0379e-11, 5.3052e-07, 9.9712e-01, 5.4291e-05, 9.5900e-12,\n",
      "         2.7723e-11, 3.2058e-10, 1.8073e-05, 4.7708e-15, 5.5939e-15, 6.5540e-09,\n",
      "         4.3119e-11, 1.3234e-13, 1.3956e-11],\n",
      "        [1.8750e-13, 1.2102e-04, 9.9769e-01, 1.0057e-10, 5.5327e-05, 2.0860e-12,\n",
      "         3.9240e-08, 1.4504e-03, 3.4856e-16, 1.4908e-08, 3.1485e-10, 2.3283e-06,\n",
      "         1.0099e-08, 6.2006e-11, 6.7753e-04, 3.5533e-12, 2.2295e-09, 4.0387e-18,\n",
      "         7.2694e-16, 3.6016e-15, 4.1376e-10, 9.8461e-11, 1.6461e-06, 1.2117e-12,\n",
      "         5.3141e-12, 3.8156e-20, 1.1864e-11],\n",
      "        [8.1519e-07, 1.8332e-05, 2.3168e-05, 1.2774e-06, 4.0683e-06, 9.3440e-15,\n",
      "         1.5083e-02, 3.8062e-05, 1.3481e-13, 9.8263e-01, 3.5591e-13, 1.1303e-07,\n",
      "         1.5097e-13, 2.1545e-09, 9.1274e-05, 5.1088e-08, 9.3110e-10, 5.1159e-11,\n",
      "         9.4113e-05, 2.4713e-11, 2.9635e-06, 4.3522e-07, 3.6550e-06, 6.7161e-12,\n",
      "         2.0122e-03, 1.1599e-11, 1.7051e-12],\n",
      "        [9.7155e-09, 1.6019e-03, 4.3874e-04, 6.2239e-06, 7.6865e-03, 1.1713e-11,\n",
      "         2.3968e-01, 4.8713e-02, 7.6167e-12, 1.7290e-03, 5.2588e-11, 6.6065e-01,\n",
      "         4.0539e-08, 4.3627e-08, 3.7305e-02, 3.8775e-04, 8.7040e-06, 3.0449e-11,\n",
      "         3.8685e-10, 3.8574e-10, 1.7796e-03, 2.1903e-08, 3.8511e-06, 2.8226e-10,\n",
      "         1.1564e-05, 1.7189e-12, 9.3201e-12],\n",
      "        [6.0114e-09, 3.2555e-01, 4.2259e-01, 2.5892e-06, 6.5058e-05, 2.1321e-12,\n",
      "         1.9881e-04, 1.2682e-01, 4.7295e-17, 2.6886e-02, 5.0098e-09, 6.5764e-02,\n",
      "         1.8244e-08, 2.1949e-09, 9.5128e-03, 5.0866e-10, 4.3610e-09, 9.5495e-13,\n",
      "         3.9920e-07, 4.7977e-13, 2.1615e-05, 5.7838e-06, 2.2336e-02, 1.2052e-13,\n",
      "         2.5236e-04, 1.7556e-12, 1.1156e-07],\n",
      "        [4.8819e-10, 9.5707e-01, 1.9745e-03, 1.0205e-05, 2.8307e-04, 4.5179e-13,\n",
      "         7.7166e-03, 1.0638e-02, 7.5594e-12, 1.0603e-02, 9.1720e-10, 1.3286e-03,\n",
      "         7.9691e-08, 1.3872e-09, 1.2151e-05, 3.4728e-07, 1.0055e-08, 8.0994e-11,\n",
      "         7.2830e-05, 2.8638e-11, 4.7064e-03, 3.4307e-11, 3.2743e-05, 1.4570e-12,\n",
      "         5.5556e-03, 1.7172e-12, 4.0378e-08],\n",
      "        [6.1796e-08, 1.8258e-03, 7.8325e-11, 3.6432e-06, 2.3206e-05, 2.4423e-14,\n",
      "         9.3456e-04, 2.3700e-05, 2.1372e-10, 5.3696e-09, 9.4509e-11, 2.6701e-08,\n",
      "         6.4904e-08, 1.0379e-11, 5.3052e-07, 9.9712e-01, 5.4291e-05, 9.5900e-12,\n",
      "         2.7723e-11, 3.2058e-10, 1.8073e-05, 4.7708e-15, 5.5939e-15, 6.5540e-09,\n",
      "         4.3119e-11, 1.3234e-13, 1.3956e-11],\n",
      "        [1.3200e-13, 1.9223e-06, 7.1693e-01, 1.7305e-10, 2.1226e-03, 8.0010e-13,\n",
      "         7.9483e-07, 1.9168e-05, 1.3294e-14, 3.5571e-08, 7.2291e-12, 6.7503e-06,\n",
      "         2.8823e-09, 3.0883e-12, 2.8092e-01, 8.7492e-10, 1.3533e-10, 7.9062e-18,\n",
      "         2.4594e-18, 2.4822e-15, 1.2626e-10, 5.3987e-10, 2.0785e-10, 6.9926e-14,\n",
      "         3.9123e-13, 5.8175e-21, 1.2719e-13],\n",
      "        [9.0941e-07, 7.8065e-03, 2.8632e-03, 7.5715e-07, 7.5664e-09, 6.5871e-14,\n",
      "         1.5185e-04, 1.7041e-03, 1.5355e-15, 9.4495e-01, 2.9231e-08, 7.2299e-06,\n",
      "         6.6421e-13, 4.0059e-10, 2.0160e-03, 3.1756e-10, 1.6739e-11, 2.1611e-10,\n",
      "         1.6796e-06, 5.1147e-08, 2.6831e-04, 1.1668e-10, 4.0178e-02, 1.3739e-13,\n",
      "         4.6964e-05, 2.7244e-10, 5.2016e-12],\n",
      "        [2.7963e-10, 9.9085e-01, 7.0592e-06, 1.3792e-07, 6.8666e-07, 5.5995e-13,\n",
      "         1.5418e-07, 3.4574e-03, 1.1379e-10, 1.4586e-06, 8.0463e-08, 7.6618e-04,\n",
      "         5.0328e-08, 2.8629e-10, 8.2188e-06, 2.1801e-08, 3.4407e-07, 1.1442e-10,\n",
      "         4.0682e-06, 4.7086e-10, 4.8799e-03, 1.7802e-13, 1.8156e-05, 1.9566e-11,\n",
      "         6.8426e-06, 1.0955e-15, 6.5357e-07],\n",
      "        [4.8636e-13, 9.9792e-01, 3.5364e-05, 1.3926e-07, 3.3405e-05, 7.8891e-15,\n",
      "         1.0051e-06, 9.0481e-04, 1.0703e-14, 3.7000e-07, 1.2635e-11, 4.7111e-04,\n",
      "         5.5840e-08, 5.0908e-11, 4.5689e-08, 7.5592e-09, 5.9091e-09, 7.7985e-14,\n",
      "         4.7742e-08, 6.0566e-15, 9.0742e-06, 1.4719e-14, 1.2877e-08, 5.6224e-14,\n",
      "         6.2378e-04, 3.4997e-16, 1.5456e-08],\n",
      "        [1.7059e-10, 9.9751e-01, 8.2842e-04, 4.8541e-07, 1.5543e-04, 2.8407e-14,\n",
      "         3.2074e-04, 5.4009e-05, 5.1096e-17, 7.3576e-05, 7.8745e-13, 3.2597e-04,\n",
      "         2.0269e-10, 3.2298e-13, 1.5968e-07, 9.9813e-06, 1.0520e-09, 2.1565e-15,\n",
      "         8.4169e-10, 1.7621e-15, 2.4105e-05, 4.1458e-12, 2.7107e-09, 9.7224e-13,\n",
      "         6.9952e-04, 6.1363e-16, 8.0119e-09],\n",
      "        [9.7155e-09, 1.6019e-03, 4.3874e-04, 6.2239e-06, 7.6865e-03, 1.1713e-11,\n",
      "         2.3968e-01, 4.8713e-02, 7.6167e-12, 1.7290e-03, 5.2588e-11, 6.6065e-01,\n",
      "         4.0539e-08, 4.3627e-08, 3.7305e-02, 3.8775e-04, 8.7040e-06, 3.0449e-11,\n",
      "         3.8685e-10, 3.8574e-10, 1.7796e-03, 2.1903e-08, 3.8511e-06, 2.8226e-10,\n",
      "         1.1564e-05, 1.7189e-12, 9.3201e-12],\n",
      "        [6.0114e-09, 3.2555e-01, 4.2259e-01, 2.5893e-06, 6.5058e-05, 2.1321e-12,\n",
      "         1.9881e-04, 1.2682e-01, 4.7295e-17, 2.6886e-02, 5.0098e-09, 6.5764e-02,\n",
      "         1.8244e-08, 2.1949e-09, 9.5128e-03, 5.0866e-10, 4.3611e-09, 9.5495e-13,\n",
      "         3.9920e-07, 4.7977e-13, 2.1615e-05, 5.7838e-06, 2.2336e-02, 1.2052e-13,\n",
      "         2.5236e-04, 1.7556e-12, 1.1156e-07],\n",
      "        [4.8819e-10, 9.5707e-01, 1.9745e-03, 1.0205e-05, 2.8307e-04, 4.5179e-13,\n",
      "         7.7166e-03, 1.0638e-02, 7.5594e-12, 1.0603e-02, 9.1720e-10, 1.3286e-03,\n",
      "         7.9691e-08, 1.3872e-09, 1.2151e-05, 3.4728e-07, 1.0055e-08, 8.0994e-11,\n",
      "         7.2830e-05, 2.8638e-11, 4.7064e-03, 3.4307e-11, 3.2743e-05, 1.4570e-12,\n",
      "         5.5556e-03, 1.7172e-12, 4.0378e-08],\n",
      "        [6.1796e-08, 1.8258e-03, 7.8325e-11, 3.6432e-06, 2.3206e-05, 2.4423e-14,\n",
      "         9.3456e-04, 2.3700e-05, 2.1372e-10, 5.3696e-09, 9.4509e-11, 2.6701e-08,\n",
      "         6.4904e-08, 1.0379e-11, 5.3052e-07, 9.9712e-01, 5.4291e-05, 9.5900e-12,\n",
      "         2.7723e-11, 3.2058e-10, 1.8073e-05, 4.7708e-15, 5.5939e-15, 6.5540e-09,\n",
      "         4.3119e-11, 1.3234e-13, 1.3956e-11],\n",
      "        [1.3200e-13, 1.9223e-06, 7.1693e-01, 1.7305e-10, 2.1226e-03, 8.0010e-13,\n",
      "         7.9483e-07, 1.9168e-05, 1.3294e-14, 3.5571e-08, 7.2291e-12, 6.7503e-06,\n",
      "         2.8823e-09, 3.0883e-12, 2.8092e-01, 8.7492e-10, 1.3533e-10, 7.9062e-18,\n",
      "         2.4594e-18, 2.4822e-15, 1.2626e-10, 5.3987e-10, 2.0786e-10, 6.9926e-14,\n",
      "         3.9123e-13, 5.8175e-21, 1.2719e-13],\n",
      "        [9.0941e-07, 7.8065e-03, 2.8632e-03, 7.5715e-07, 7.5664e-09, 6.5871e-14,\n",
      "         1.5185e-04, 1.7041e-03, 1.5355e-15, 9.4495e-01, 2.9231e-08, 7.2299e-06,\n",
      "         6.6421e-13, 4.0059e-10, 2.0160e-03, 3.1756e-10, 1.6739e-11, 2.1611e-10,\n",
      "         1.6796e-06, 5.1147e-08, 2.6831e-04, 1.1668e-10, 4.0178e-02, 1.3739e-13,\n",
      "         4.6964e-05, 2.7244e-10, 5.2016e-12],\n",
      "        [5.7199e-08, 2.3466e-03, 3.9616e-12, 1.2638e-04, 1.3505e-04, 4.6008e-12,\n",
      "         8.7009e-02, 1.7655e-07, 1.2333e-10, 1.2773e-07, 4.1014e-11, 9.7767e-06,\n",
      "         1.4881e-07, 1.0609e-11, 2.5878e-05, 9.0929e-01, 9.5141e-05, 8.4298e-11,\n",
      "         3.9702e-11, 1.7259e-11, 9.2022e-04, 4.7929e-12, 1.3886e-13, 4.0595e-05,\n",
      "         5.2113e-10, 4.2909e-15, 9.0800e-10],\n",
      "        [9.7155e-09, 1.6019e-03, 4.3874e-04, 6.2239e-06, 7.6865e-03, 1.1713e-11,\n",
      "         2.3968e-01, 4.8713e-02, 7.6167e-12, 1.7290e-03, 5.2588e-11, 6.6065e-01,\n",
      "         4.0539e-08, 4.3627e-08, 3.7305e-02, 3.8775e-04, 8.7040e-06, 3.0449e-11,\n",
      "         3.8685e-10, 3.8574e-10, 1.7796e-03, 2.1903e-08, 3.8511e-06, 2.8226e-10,\n",
      "         1.1564e-05, 1.7189e-12, 9.3201e-12],\n",
      "        [6.0114e-09, 3.2555e-01, 4.2259e-01, 2.5892e-06, 6.5058e-05, 2.1321e-12,\n",
      "         1.9881e-04, 1.2682e-01, 4.7295e-17, 2.6886e-02, 5.0098e-09, 6.5764e-02,\n",
      "         1.8244e-08, 2.1949e-09, 9.5128e-03, 5.0866e-10, 4.3610e-09, 9.5495e-13,\n",
      "         3.9920e-07, 4.7977e-13, 2.1615e-05, 5.7838e-06, 2.2336e-02, 1.2052e-13,\n",
      "         2.5236e-04, 1.7556e-12, 1.1156e-07],\n",
      "        [4.8819e-10, 9.5707e-01, 1.9745e-03, 1.0205e-05, 2.8307e-04, 4.5179e-13,\n",
      "         7.7166e-03, 1.0638e-02, 7.5594e-12, 1.0603e-02, 9.1720e-10, 1.3286e-03,\n",
      "         7.9691e-08, 1.3872e-09, 1.2151e-05, 3.4728e-07, 1.0055e-08, 8.0994e-11,\n",
      "         7.2830e-05, 2.8638e-11, 4.7064e-03, 3.4307e-11, 3.2743e-05, 1.4570e-12,\n",
      "         5.5556e-03, 1.7172e-12, 4.0378e-08],\n",
      "        [6.1796e-08, 1.8258e-03, 7.8325e-11, 3.6432e-06, 2.3206e-05, 2.4423e-14,\n",
      "         9.3456e-04, 2.3700e-05, 2.1372e-10, 5.3696e-09, 9.4509e-11, 2.6701e-08,\n",
      "         6.4904e-08, 1.0379e-11, 5.3052e-07, 9.9712e-01, 5.4291e-05, 9.5900e-12,\n",
      "         2.7723e-11, 3.2058e-10, 1.8073e-05, 4.7708e-15, 5.5939e-15, 6.5540e-09,\n",
      "         4.3119e-11, 1.3234e-13, 1.3956e-11],\n",
      "        [1.3200e-13, 1.9223e-06, 7.1693e-01, 1.7305e-10, 2.1226e-03, 8.0010e-13,\n",
      "         7.9483e-07, 1.9168e-05, 1.3294e-14, 3.5571e-08, 7.2291e-12, 6.7503e-06,\n",
      "         2.8823e-09, 3.0883e-12, 2.8092e-01, 8.7492e-10, 1.3533e-10, 7.9062e-18,\n",
      "         2.4594e-18, 2.4822e-15, 1.2626e-10, 5.3987e-10, 2.0785e-10, 6.9926e-14,\n",
      "         3.9123e-13, 5.8175e-21, 1.2719e-13],\n",
      "        [9.0941e-07, 7.8065e-03, 2.8632e-03, 7.5715e-07, 7.5664e-09, 6.5871e-14,\n",
      "         1.5185e-04, 1.7041e-03, 1.5355e-15, 9.4495e-01, 2.9231e-08, 7.2299e-06,\n",
      "         6.6421e-13, 4.0059e-10, 2.0160e-03, 3.1756e-10, 1.6739e-11, 2.1611e-10,\n",
      "         1.6796e-06, 5.1147e-08, 2.6831e-04, 1.1668e-10, 4.0178e-02, 1.3739e-13,\n",
      "         4.6964e-05, 2.7244e-10, 5.2016e-12],\n",
      "        [5.4617e-06, 2.6692e-01, 7.5751e-07, 3.5626e-06, 2.3133e-07, 2.1291e-09,\n",
      "         4.2749e-01, 6.4647e-03, 1.1912e-07, 4.0834e-03, 2.6794e-07, 2.8544e-01,\n",
      "         4.7036e-07, 4.4185e-10, 2.5303e-03, 1.3025e-03, 4.9801e-04, 2.2647e-07,\n",
      "         1.5644e-04, 4.4092e-05, 5.0688e-03, 2.0949e-11, 2.3214e-09, 9.5570e-08,\n",
      "         6.6861e-07, 3.1464e-12, 5.9513e-07],\n",
      "        [9.7155e-09, 1.6019e-03, 4.3874e-04, 6.2239e-06, 7.6865e-03, 1.1713e-11,\n",
      "         2.3968e-01, 4.8713e-02, 7.6167e-12, 1.7290e-03, 5.2588e-11, 6.6065e-01,\n",
      "         4.0539e-08, 4.3627e-08, 3.7305e-02, 3.8775e-04, 8.7040e-06, 3.0449e-11,\n",
      "         3.8685e-10, 3.8574e-10, 1.7796e-03, 2.1903e-08, 3.8511e-06, 2.8226e-10,\n",
      "         1.1564e-05, 1.7189e-12, 9.3201e-12],\n",
      "        [6.0114e-09, 3.2555e-01, 4.2259e-01, 2.5893e-06, 6.5058e-05, 2.1321e-12,\n",
      "         1.9881e-04, 1.2682e-01, 4.7295e-17, 2.6886e-02, 5.0098e-09, 6.5764e-02,\n",
      "         1.8244e-08, 2.1949e-09, 9.5128e-03, 5.0866e-10, 4.3611e-09, 9.5495e-13,\n",
      "         3.9920e-07, 4.7977e-13, 2.1615e-05, 5.7838e-06, 2.2336e-02, 1.2052e-13,\n",
      "         2.5236e-04, 1.7556e-12, 1.1156e-07],\n",
      "        [4.8819e-10, 9.5707e-01, 1.9745e-03, 1.0205e-05, 2.8307e-04, 4.5179e-13,\n",
      "         7.7166e-03, 1.0638e-02, 7.5594e-12, 1.0603e-02, 9.1720e-10, 1.3286e-03,\n",
      "         7.9691e-08, 1.3872e-09, 1.2151e-05, 3.4728e-07, 1.0055e-08, 8.0994e-11,\n",
      "         7.2830e-05, 2.8637e-11, 4.7064e-03, 3.4307e-11, 3.2743e-05, 1.4570e-12,\n",
      "         5.5556e-03, 1.7172e-12, 4.0378e-08],\n",
      "        [6.1796e-08, 1.8258e-03, 7.8325e-11, 3.6432e-06, 2.3206e-05, 2.4423e-14,\n",
      "         9.3456e-04, 2.3700e-05, 2.1372e-10, 5.3696e-09, 9.4509e-11, 2.6701e-08,\n",
      "         6.4904e-08, 1.0379e-11, 5.3052e-07, 9.9712e-01, 5.4291e-05, 9.5900e-12,\n",
      "         2.7723e-11, 3.2058e-10, 1.8073e-05, 4.7708e-15, 5.5939e-15, 6.5540e-09,\n",
      "         4.3119e-11, 1.3234e-13, 1.3956e-11],\n",
      "        [1.3200e-13, 1.9223e-06, 7.1693e-01, 1.7305e-10, 2.1226e-03, 8.0010e-13,\n",
      "         7.9483e-07, 1.9168e-05, 1.3294e-14, 3.5571e-08, 7.2291e-12, 6.7503e-06,\n",
      "         2.8823e-09, 3.0883e-12, 2.8092e-01, 8.7492e-10, 1.3533e-10, 7.9062e-18,\n",
      "         2.4594e-18, 2.4822e-15, 1.2626e-10, 5.3987e-10, 2.0785e-10, 6.9926e-14,\n",
      "         3.9123e-13, 5.8175e-21, 1.2719e-13],\n",
      "        [2.4976e-08, 8.5258e-07, 6.4014e-12, 5.6395e-10, 1.7387e-15, 9.3211e-10,\n",
      "         8.1940e-05, 3.8722e-04, 9.0780e-09, 8.2668e-06, 2.9696e-09, 9.9724e-01,\n",
      "         6.8448e-08, 7.0124e-11, 1.1211e-08, 5.3103e-08, 6.1896e-07, 1.5191e-06,\n",
      "         2.1530e-03, 7.0360e-05, 5.8065e-05, 1.8691e-14, 8.6978e-07, 1.4633e-10,\n",
      "         1.3568e-08, 7.7785e-08, 7.2762e-15],\n",
      "        [7.6364e-07, 2.0943e-06, 2.6933e-09, 1.0329e-08, 9.3244e-01, 2.0135e-11,\n",
      "         3.5556e-02, 3.5551e-05, 8.2460e-03, 4.4806e-14, 1.2986e-10, 1.0666e-05,\n",
      "         2.2183e-02, 8.0620e-06, 1.0551e-09, 9.8510e-11, 1.4843e-03, 6.7394e-11,\n",
      "         5.4842e-12, 8.7267e-10, 9.8171e-07, 2.6930e-07, 4.9614e-11, 2.2635e-05,\n",
      "         1.1916e-12, 1.1428e-08, 1.4320e-05],\n",
      "        [6.2292e-16, 1.2859e-16, 2.0984e-03, 3.4526e-12, 4.2269e-09, 3.7092e-19,\n",
      "         6.0851e-11, 2.1201e-07, 1.0094e-16, 3.0598e-09, 2.4779e-15, 8.7634e-14,\n",
      "         1.1471e-13, 2.1962e-12, 3.0783e-11, 3.9722e-23, 2.1121e-12, 1.1173e-15,\n",
      "         1.9496e-11, 2.6106e-19, 3.7216e-14, 2.7213e-06, 2.7625e-12, 3.1072e-14,\n",
      "         9.9790e-01, 1.1968e-07, 1.0823e-11]])\n"
     ]
    }
   ],
   "source": [
    "# Let's see the probabilities in action\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb74251-8863-49f7-b238-60ba73e65f39",
   "metadata": {},
   "source": [
    "Now that we have our probabilities,\\\n",
    "We also want to:\n",
    "1. Calculate Loss\n",
    "2. Tune the particular weights depending on the gradients\n",
    "\n",
    "So just like our older <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave Notebook</a>, we will calculate loss such that,\n",
    "1. We will take the log likelihood of the probabilities\n",
    "2. Take the average of the log likelihood\n",
    "3. Convert it to negetive average log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "839393e7-6ca3-4c89-aca1-ee6dd18c467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.9243)\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate our own average negetive log likelihood straight from this tensor\n",
    "# This is the vectorized form of that expression\n",
    "loss = -probabilities[torch.arange(len(inputs)), outputs].log().mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29a6fe-f1bf-483c-a4f0-5e1bebe4d44b",
   "metadata": {},
   "source": [
    "Let's make our code a little more respectable...\n",
    "\n",
    "What is **entropy**?\n",
    "\n",
    "Entropy is the measurement of disorder or impurities in the information processed in machine learning.\n",
    "\n",
    "$$ H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log p(x) $$\n",
    "\n",
    "**When entropy becomes 0, then the dataset has no impurity.**\n",
    "\n",
    "![LowHighEntropy](https://static.javatpoint.com/tutorial/machine-learning/images/entropy-in-machine-learning3.png)\n",
    "\n",
    "What is the **Information Gain in Entropy**?\n",
    "\n",
    "Information gain is defined as the pattern observed in the dataset by calculating the reduction in entropy or surprise by splitting a dataset according to a given value of a random variable.\n",
    "\n",
    "$$ \\text{Information Gain} = 1-\\text{Entropy} $$\n",
    "\n",
    "![LowHighInformationGain](https://miro.medium.com/v2/resize:fit:1400/1*DsjX_bHYWn21Z0VIPjxnbw.png)\n",
    "\n",
    "What is **Cross-Entropy**?\n",
    "\n",
    "Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events.\n",
    "\n",
    "What is **Cross-Entropy Loss**?\n",
    "\n",
    "Cross-entropy loss refers to the contrast between two random variables. It measures the variables to extract the difference in the information they contain, showcasing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf303df1-2d03-409e-b519-8e48a24174bc",
   "metadata": {},
   "source": [
    "So,\\\n",
    "We now understand that our lines of code:\n",
    "```python\n",
    "# Calculating counts from logits\n",
    "counts = logits.exp()\n",
    "# Normalizing counts to probabilities that sum to 1\n",
    "probabilities = counts / counts.sum(1, keepdims=True)\n",
    "# Calculating the negetive average log likelihood as loss\n",
    "loss = -probabilities[torch.arange(len(inputs)), outputs].log().mean()\n",
    "```\n",
    "\n",
    "Can now be replaced by a ready-made function:\n",
    "```python\n",
    "loss = F.cross_entropy(logits, outputs)\n",
    "```\n",
    "\n",
    "Since we are doing classification..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "874bd94c-9f96-4bbc-bcd9-589eef109b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.9243)\n"
     ]
    }
   ],
   "source": [
    "# Calculating the negetive average log likelihood as loss (cross entropy loss)\n",
    "loss = F.cross_entropy(logits, outputs)\n",
    "# We get the same loss now\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f25c4-b6c4-40f9-9184-ab452bba55bd",
   "metadata": {},
   "source": [
    "Now, why are we using *F.cross_entropy()* might be the next question coming to your mind...\n",
    "\n",
    "There are a number of reasons, as to why,\n",
    "1. Its efficient and does not require to use new memory\n",
    "2. We practically don't use the three lines of code\n",
    "\n",
    "Now you might be wondering why my last point is valid.\\\n",
    "Let me explain...\n",
    "\n",
    "Suppose we write the same code:\n",
    "```python\n",
    "#Defining logits (example)\n",
    "logits = torch.tensor([-100, -3, 0, 100])\n",
    "# Calculating counts from logits\n",
    "counts = logits.exp()\n",
    "# Normalizing counts to probabilities that sum to 1\n",
    "probabilities = counts / counts.sum(1, keepdims=True)\n",
    "# Calculating the negetive average log likelihood as loss\n",
    "loss = -probabilities[torch.arange(len(inputs)), outputs].log().mean()\n",
    "```\n",
    "\n",
    "Even if the code seems simple, we run into problems...\\\n",
    "Because when we represent counts by performing an exponential function,\\\n",
    "Any large negetive number works fine(it represents a very tiny number),\\\n",
    "But the moment we associate a very positive number along with it(it tries to represent a very large number) and it goes out of memory and count turns out to be inifinity and probabilities remain undefined as well.\n",
    "\n",
    "So summarising the reasons:\n",
    "1. The forward pass would be much more efficient\n",
    "2. The backward pass would be much more efficient\n",
    "3. The calculations would be mathematically well behaved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbeb27-c45a-48eb-9ae4-bf5ae0dffbb4",
   "metadata": {},
   "source": [
    "##### Now that we have all the layers, let's put all the parameters in a single variable to access all of them way faster\n",
    "\n",
    "Let's recall what those were:\n",
    "1. The *weights and biases* of the *output layer*\n",
    "2. The *weights and biases* of the *hidden layer*\n",
    "3. The *embedding look-up table 'C'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84f04d1e-a826-454b-84a8-16388a4698c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.5820, -0.2370],\n",
      "        [ 1.1564, -1.5507],\n",
      "        [ 0.3370,  0.9905],\n",
      "        [ 2.0201, -1.3263],\n",
      "        [ 1.7319,  0.8092],\n",
      "        [-1.1567, -0.1717],\n",
      "        [ 1.4870,  0.3647],\n",
      "        [ 1.4763, -0.1208],\n",
      "        [ 1.4331, -0.6318],\n",
      "        [ 0.8025, -0.2571],\n",
      "        [-1.5145,  0.6996],\n",
      "        [ 0.5932,  1.5696],\n",
      "        [ 0.9620, -0.2745],\n",
      "        [ 0.7202, -0.7890],\n",
      "        [ 1.9879, -0.6847],\n",
      "        [ 1.8194,  0.2373],\n",
      "        [ 0.3454,  1.3520],\n",
      "        [-0.0533, -0.4030],\n",
      "        [ 0.6345, -1.5687],\n",
      "        [ 1.2893,  1.3174],\n",
      "        [-0.3376, -0.0277],\n",
      "        [ 1.3857, -2.1648],\n",
      "        [ 0.1235,  0.8132],\n",
      "        [ 1.2909, -0.0200],\n",
      "        [-0.7188, -0.9777],\n",
      "        [-0.0405,  0.3629]]), tensor([[-1.3309e+00,  5.3760e-01, -8.8002e-01, -1.6027e+00, -8.7127e-01,\n",
      "          1.5733e+00,  1.5703e+00, -5.7903e-02, -6.7040e-01,  1.5406e+00,\n",
      "         -6.2869e-02, -7.6818e-01,  3.9260e-01, -2.2779e+00, -1.8487e-01,\n",
      "          7.3894e-03, -2.6233e-01,  1.8654e+00, -1.5064e+00,  8.4365e-02,\n",
      "         -2.5354e-01,  1.4439e-01,  1.0357e+00, -1.1222e+00,  1.5599e+00,\n",
      "          9.9688e-01, -3.8515e-01,  5.9393e-01, -9.7277e-02, -3.6606e-01,\n",
      "          4.8837e-01, -1.3940e+00, -2.5506e-01, -1.1282e+00, -1.8727e-01,\n",
      "         -1.1176e+00,  2.4430e-01,  3.9079e-01, -3.2704e-01, -7.7329e-02,\n",
      "          8.0980e-01,  2.5340e-01, -7.0271e-01, -1.6802e+00, -9.2598e-01,\n",
      "          5.0657e-01,  5.3528e-01,  1.4070e+00, -1.7344e+00, -2.0002e+00,\n",
      "          2.6695e-01, -9.4600e-01,  1.4118e+00,  1.2434e+00,  2.3755e-01,\n",
      "         -1.3315e+00, -4.8819e-01,  5.8057e-01, -4.0031e-01,  4.4854e-01,\n",
      "         -1.1016e+00,  3.9298e-01, -2.5692e-01,  7.4658e-01, -1.1525e+00,\n",
      "         -2.6733e+00,  1.1521e-02,  1.0843e+00,  7.3623e-01,  4.0915e-01,\n",
      "          2.0959e-01, -3.4613e-01,  9.9326e-01,  6.0410e-01,  9.0647e-01,\n",
      "         -1.5039e+00, -5.5806e-01, -1.3786e+00,  9.9444e-01,  1.9750e+00,\n",
      "         -2.2914e-03, -8.8557e-03, -7.5970e-01, -4.3659e-01,  3.8519e-02,\n",
      "         -7.4881e-01, -1.6952e-01,  5.8114e-01,  8.4919e-01, -6.2851e-01,\n",
      "         -9.6541e-02, -8.5209e-01, -4.0941e-03, -1.3501e-02,  1.3899e+00,\n",
      "          2.6859e-01, -1.3055e+00,  1.6258e+00, -1.8733e+00, -5.8859e-01],\n",
      "        [-4.7777e-01, -8.9962e-01,  1.2990e+00,  8.6798e-01,  3.2813e-01,\n",
      "         -6.8859e-01, -6.9989e-01,  2.3148e-02, -4.7191e-01, -2.2842e+00,\n",
      "         -2.3208e-01, -9.7641e-02,  2.2132e-01,  2.1101e+00,  4.7468e-02,\n",
      "          2.6959e-01,  1.9122e-01, -1.3851e+00, -8.4684e-01,  6.0138e-01,\n",
      "         -1.1100e+00,  1.6966e-01, -3.7490e-01, -3.4588e-01,  1.9954e-01,\n",
      "          3.5620e-01, -6.9985e-02, -4.5143e-01,  4.3894e-01,  4.5460e-01,\n",
      "          1.2859e-01, -1.4358e+00, -7.1921e-01,  1.3497e+00, -1.5864e+00,\n",
      "          5.9658e-01,  1.7555e-01, -1.4003e+00, -1.0750e+00,  1.0854e+00,\n",
      "          4.4751e-02,  8.3217e-01,  4.7636e-01,  3.5159e-01,  2.2629e-01,\n",
      "         -7.5892e-02, -4.0880e-01,  1.6193e+00,  6.5181e-01,  1.1079e-01,\n",
      "          2.0800e-01,  1.0691e+00,  5.1968e-01, -2.9245e-01,  6.7402e-01,\n",
      "         -9.4632e-01,  6.9604e-01,  9.5261e-01, -3.7395e-01,  3.9874e-01,\n",
      "         -1.0413e+00, -3.0102e-02,  2.5658e-01, -5.4915e-01, -2.0302e-01,\n",
      "         -2.3100e-01, -1.0203e+00, -1.4377e+00, -1.1299e+00,  9.1278e-01,\n",
      "          3.0766e-01,  2.9088e-01, -1.0209e+00, -1.1914e+00,  4.6379e-01,\n",
      "          2.0850e+00, -1.3668e+00,  2.8093e+00,  5.2998e-02, -5.3804e-01,\n",
      "         -7.0440e-01,  3.5564e-01,  1.6849e+00, -3.7820e-01,  1.1178e+00,\n",
      "         -6.2676e-01, -6.7081e-01,  7.1514e-01, -6.0051e-01, -2.8266e-01,\n",
      "         -2.3830e+00, -4.2640e-01, -9.1350e-01,  2.3289e-01,  1.1668e+00,\n",
      "          7.1544e-02,  1.4273e+00, -8.0557e-01, -3.6652e-01,  4.8889e-01],\n",
      "        [ 6.9220e-01, -7.4551e-01, -1.5467e+00, -1.0293e+00, -1.4992e+00,\n",
      "         -1.6583e-01, -1.1887e-02, -1.1118e+00, -3.8187e-01,  1.6545e+00,\n",
      "         -3.8502e-01, -1.0659e+00, -1.9049e-01, -1.0040e-01,  1.4531e+00,\n",
      "         -8.6961e-02, -2.9355e-01,  4.0490e-01, -6.2181e-01,  1.1647e+00,\n",
      "         -7.4661e-01,  1.0201e+00, -1.5348e+00,  9.1312e-01,  2.0153e-01,\n",
      "         -7.2702e-02, -2.7234e-01,  7.8289e-01, -8.5910e-01, -2.8716e-01,\n",
      "         -4.1418e-02,  1.0915e+00, -8.1097e-01, -2.0565e-01, -6.8447e-01,\n",
      "          1.6070e-01,  1.7329e+00, -5.8615e-01,  3.0751e-01,  1.1162e-01,\n",
      "          9.9849e-01, -4.4300e-01, -1.1258e+00, -8.3702e-01, -3.3572e-01,\n",
      "         -1.7013e-01, -1.0379e+00,  6.4179e-01,  4.7401e-01,  6.6802e-02,\n",
      "         -1.3061e+00,  7.7246e-01, -5.5010e-02,  7.2567e-01, -8.8693e-01,\n",
      "          1.0389e+00,  5.2864e-01, -1.5777e-01,  2.0811e-02, -9.8126e-01,\n",
      "          2.8939e-01, -1.1100e+00, -4.7640e-01,  1.7447e+00, -8.8621e-01,\n",
      "         -8.4154e-01, -5.3944e-02,  5.4638e-01, -1.8262e-01, -5.0275e-01,\n",
      "          3.8975e-01, -1.5514e+00, -3.5744e-01,  1.7626e+00, -1.5752e+00,\n",
      "          1.8317e-01, -2.0974e-01, -2.3136e+00, -9.0146e-02,  6.6591e-01,\n",
      "         -4.9796e-01,  3.6350e-01,  2.0833e-01, -4.9626e-01, -1.3789e+00,\n",
      "          5.2573e-01, -7.5205e-01, -1.4886e+00,  1.2920e+00, -4.4018e-01,\n",
      "         -1.3254e+00,  3.3856e-01, -5.8536e-02,  4.8590e-01, -1.1933e+00,\n",
      "         -7.0768e-01,  1.2708e-01, -1.3113e+00,  4.1251e-01, -9.6696e-01],\n",
      "        [-2.5321e+00,  6.1816e-01, -1.7118e+00,  9.7377e-01,  1.5213e+00,\n",
      "         -1.2141e+00, -6.7537e-01, -2.4627e-01,  1.7928e+00, -6.1528e-01,\n",
      "          4.9874e-01,  6.7715e-01, -5.2468e-01,  1.6879e+00, -9.3342e-01,\n",
      "         -4.5015e-01, -9.0794e-01, -1.3096e+00, -1.1231e+00, -1.1617e-01,\n",
      "         -2.7613e-01, -4.6792e-01,  5.9624e-02, -9.4865e-01, -6.9647e-01,\n",
      "         -7.0529e-01, -2.1063e-01,  3.5138e-01,  9.4279e-02, -1.8668e+00,\n",
      "         -1.4981e+00, -9.8331e-02, -1.1391e+00,  1.3043e+00,  1.3514e-01,\n",
      "         -4.8757e-01,  1.6272e+00,  1.1906e+00, -5.3820e-01,  1.5106e+00,\n",
      "         -7.8377e-01,  1.7152e+00, -1.4231e+00,  4.2123e-01, -4.6663e-02,\n",
      "         -4.9630e-01,  2.7418e-01, -2.6578e-01, -6.9756e-01, -2.0441e-01,\n",
      "         -3.7010e-01,  1.2488e+00,  7.4041e-01,  6.5922e-01,  1.0971e+00,\n",
      "         -2.1994e+00,  1.2059e+00,  1.0144e-01, -2.2854e-01,  3.3737e-01,\n",
      "         -4.3595e-01, -1.7102e-01,  2.4686e-01, -2.9084e-01,  1.3602e+00,\n",
      "          4.5163e-01,  1.1807e+00, -1.2195e+00, -3.8913e-01, -5.3004e-01,\n",
      "         -5.4938e-01,  6.1264e-02, -4.4102e-01,  6.9234e-01, -6.4825e-01,\n",
      "          1.3022e+00,  1.6458e-01,  5.5635e-01,  4.7097e-01, -4.7548e-01,\n",
      "          1.7844e+00, -2.2144e+00,  6.0142e-01,  2.7186e-01, -1.2835e+00,\n",
      "         -1.2490e+00,  1.3889e+00, -7.2127e-01,  1.0792e-01,  1.0628e+00,\n",
      "         -1.3888e+00, -2.3059e-01,  1.3858e+00,  1.7001e+00,  3.4070e-01,\n",
      "         -6.9204e-01, -1.4332e+00, -2.4478e+00,  6.0594e-02, -1.6965e+00],\n",
      "        [-5.5996e-01,  4.1000e-03, -3.5817e-01, -2.9499e-01, -7.6611e-01,\n",
      "         -7.4932e-01,  2.1830e-02,  6.5076e-01,  1.7884e+00, -3.9841e-01,\n",
      "         -8.1958e-01,  5.1770e-01, -6.1492e-01, -4.0957e-01, -6.2389e-01,\n",
      "          1.2343e+00, -1.3053e+00,  1.1335e+00,  1.4029e+00,  2.0002e+00,\n",
      "         -4.0797e-03, -3.8851e-01,  1.0701e+00, -1.2645e+00,  1.2159e+00,\n",
      "          3.0274e-01,  3.6809e-01, -2.2069e-01,  2.5497e-01,  1.5104e-01,\n",
      "         -6.9560e-01,  6.5380e-01,  8.5749e-01, -7.4288e-01, -7.8204e-01,\n",
      "          1.2505e+00, -6.4734e-02, -5.5513e-01,  1.3841e+00,  1.4493e+00,\n",
      "         -6.8750e-02,  2.1367e+00, -7.1010e-01, -6.5154e-01,  1.6285e+00,\n",
      "          6.1841e-01,  1.0930e+00, -4.4478e-01,  1.9059e-01, -7.5447e-01,\n",
      "          7.5021e-01,  2.3581e-01, -9.1898e-01,  3.9986e-02, -3.5221e-01,\n",
      "          9.6778e-01,  4.7191e-01,  4.2777e-01,  7.9255e-02,  1.1756e+00,\n",
      "         -6.9802e-01, -1.4787e+00,  1.1130e+00, -2.8132e-01, -6.6463e-01,\n",
      "         -7.6253e-01,  2.8872e-03,  1.4913e+00, -5.6016e-01, -3.9886e-01,\n",
      "          9.3733e-01,  9.1327e-01,  1.0175e+00, -3.6514e-01, -4.2121e-01,\n",
      "          3.3769e-01, -5.7859e-01,  3.2780e-01,  1.4998e+00,  1.1832e+00,\n",
      "          6.3789e-01, -3.9014e-01,  2.8262e-01,  1.3771e-01,  1.8117e-01,\n",
      "         -4.5273e-01, -6.8751e-01,  1.4941e+00,  5.8897e-01, -1.5144e+00,\n",
      "          1.4020e+00, -1.1111e+00,  1.5156e-03, -4.5771e-01, -1.9587e+00,\n",
      "          1.6494e+00,  1.2119e-01,  2.5594e-01,  4.8604e-01, -5.4420e-01],\n",
      "        [-1.1157e+00, -9.7852e-01, -2.1155e-01,  2.6812e-01,  1.9349e-01,\n",
      "         -2.1002e+00,  1.5977e+00,  1.0761e+00, -1.0781e+00, -1.3814e+00,\n",
      "          1.4186e+00, -7.9773e-02,  9.0538e-01, -6.0046e-01, -1.5921e+00,\n",
      "          6.6642e-01,  2.6560e+00, -1.3324e+00,  3.1435e-01, -1.0483e+00,\n",
      "          2.0953e-01, -1.1544e-02, -1.1061e+00,  1.6476e-02, -1.8924e-01,\n",
      "         -5.4662e-01,  4.3868e-01, -1.9348e+00, -9.3545e-01, -2.1302e+00,\n",
      "         -3.5785e-01, -8.0148e-01, -3.7003e-01,  1.0941e-01,  7.2654e-01,\n",
      "          1.2415e+00, -5.4700e-01, -4.7211e-01,  7.0744e-01, -4.8757e-01,\n",
      "         -2.8573e-02, -3.0398e-01,  1.6167e+00,  2.2136e+00, -2.4971e-01,\n",
      "         -1.8014e+00,  1.2367e+00, -1.1111e+00,  1.5685e+00, -1.0122e+00,\n",
      "          8.9835e-01,  5.6169e-01, -9.2679e-01,  3.7734e-01, -2.2296e-01,\n",
      "          2.1748e-01,  3.6686e-01,  1.2943e+00, -5.9963e-01,  2.9190e+00,\n",
      "         -1.6428e+00, -1.0009e+00,  5.2192e-01, -1.9367e+00, -1.3045e+00,\n",
      "         -9.9974e-01,  8.0031e-01,  2.4026e-01,  9.6005e-01,  2.6590e-01,\n",
      "         -6.3356e-01,  5.0051e-01, -1.1728e+00,  2.0543e+00,  1.5429e+00,\n",
      "          1.8593e+00,  1.1934e+00,  1.1988e-01, -1.0830e+00, -1.6480e-01,\n",
      "          9.1416e-01, -6.5953e-01,  2.3641e-01,  7.4476e-01,  1.7428e-01,\n",
      "         -1.7109e+00, -4.6254e-01, -1.7829e+00,  6.5442e-01, -6.8641e-01,\n",
      "          3.1828e-01, -1.6780e+00, -5.1776e-01,  1.3046e+00,  1.1035e+00,\n",
      "          1.4242e+00, -5.2186e-02, -3.0763e-01,  9.1911e-01,  1.0132e-01]]), tensor([ 0.9622, -0.3792,  0.5376, -0.5091,  0.3280, -0.3655,  1.9941,  0.4362,\n",
      "        -1.2566, -0.7842,  0.3160,  0.1405,  1.0169, -0.8422, -1.1315,  1.7779,\n",
      "        -0.2347,  2.2632,  0.4291, -0.8466,  1.2164, -0.2640, -0.4150, -0.7054,\n",
      "         0.2757, -0.5870,  1.4403, -0.3901, -0.5869, -0.6008,  1.3011,  0.8770,\n",
      "         0.7340, -0.2143,  0.3078, -1.2844, -0.0929,  0.8987,  1.1442,  0.1336,\n",
      "        -0.1886,  0.1648, -0.1727, -1.8630,  0.6894,  0.2181, -0.1693,  0.9267,\n",
      "        -0.5963,  0.1454,  1.5176,  1.2571, -0.9120, -1.0110,  0.0588,  0.9327,\n",
      "        -1.8595,  1.8576,  0.7003,  0.7321, -0.6484,  0.0135,  0.6905, -0.3973,\n",
      "         1.0300, -0.4844, -0.0172, -0.2736,  1.8609,  0.3862,  0.6344, -0.5987,\n",
      "         0.4915,  1.3887, -0.8216,  0.2504, -0.9351,  0.8111, -0.9134,  0.8837,\n",
      "        -1.1675, -0.0934,  0.8297,  0.2870,  0.5731, -1.4449, -0.2771, -1.4336,\n",
      "        -0.1230,  1.4740,  0.4356, -1.3052,  0.0898, -0.2187,  1.2329,  0.5914,\n",
      "         0.6534,  0.5853, -1.1967,  1.2161]), tensor([[ 1.0483, -1.1605,  0.3259,  ...,  0.6344, -0.5604, -0.0787],\n",
      "        [ 2.3069, -0.1839,  1.0545,  ..., -0.8273,  0.5470,  1.0350],\n",
      "        [-1.5347, -0.3304,  0.2654,  ..., -0.8080,  0.1312, -0.6703],\n",
      "        ...,\n",
      "        [-0.0058, -0.4602,  0.8973,  ..., -0.6056,  0.6224,  2.0355],\n",
      "        [ 0.9260, -0.5305,  0.6560,  ..., -0.9053,  0.8770,  0.6587],\n",
      "        [ 1.4616, -0.1537, -0.7097,  ...,  0.5023,  1.6726, -0.4561]]), tensor([-2.6924,  0.5154,  1.9185, -0.4052, -0.4127,  0.1742, -0.0759,  1.5747,\n",
      "         0.7362,  0.4856,  0.1652, -2.3270,  0.5496, -0.5393,  0.6733, -1.0305,\n",
      "        -0.2942,  1.3882, -1.7603,  0.7646, -1.0336, -1.3147,  0.2398,  1.6424,\n",
      "        -0.3005,  0.7137, -1.8108])]\n"
     ]
    }
   ],
   "source": [
    "# Let's define our parameters variable\n",
    "parameters = [embeddingLookUpMatrix, weightsOfHiddenLayer, biasesOfHiddenLayer, weightsOfFinalLayer, biasesOfFinalLayer]\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "281d509a-08f3-4344-93ea-e5098df77d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must set requires_grad to True in all the parameters to avoid any errors in the future\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37420ba-00f2-44f1-b1e4-03c84906af13",
   "metadata": {},
   "source": [
    "Let's put everything we have together for now with a respectable generator so that we all get the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc779344-bbd3-4efa-9cd4-89c64e012392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a generator to give the same result on your machine, as of my machine\n",
    "generator = torch.Generator().manual_seed(6942069420)\n",
    "# Embedding Matrix (Input Layer)\n",
    "embeddingFeatureSpaceLength = 2\n",
    "embeddingLookUpMatrix = torch.randn((len(characters),embeddingFeatureSpaceLength), generator=generator)\n",
    "# Hidden Layer\n",
    "numberOfHiddenLayerNeurons = 100\n",
    "weightsOfHiddenLayer = torch.randn((inputBlockSize*embeddingFeatureSpaceLength), numberOfHiddenLayerNeurons, generator=generator)\n",
    "biasesOfHiddenLayer = torch.randn(numberOfHiddenLayerNeurons, generator=generator)\n",
    "# Output Layer / Final Layer\n",
    "numberOfFinalLayerOutputs = 27\n",
    "weightsOfFinalLayer = torch.randn(numberOfHiddenLayerNeurons, numberOfFinalLayerOutputs, generator=generator)\n",
    "biasesOfFinalLayer = torch.randn(numberOfFinalLayerOutputs, generator=generator)\n",
    "# Parameters\n",
    "parameters = [embeddingLookUpMatrix, weightsOfHiddenLayer, biasesOfHiddenLayer, weightsOfFinalLayer, biasesOfFinalLayer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a007f79-ff1e-43ef-9179-71dd0fbb0ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3479"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check how many parameters we have\n",
    "sum(parameter.nelement() for parameter in parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e78cc-285d-490a-b711-81d14d94cbc0",
   "metadata": {},
   "source": [
    "Let's understand how neural network will train itself with forward pass, backward pass and updatation now...\n",
    "\n",
    "Now that we have trained two neural networks already...\n",
    "\n",
    "We can safely say that they work in the sequence:\n",
    "1. Forward Pass - Makes calculations and calculates loss\n",
    "2. Backward Pass - Resets all the gradients and back propagtes through the network\n",
    "3. Data Updation - Updates the data for all the parameters in the opposite direction of the gradient depending on the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a78f1bd9-3e12-4536-9dd0-d229744cd2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(9.8323, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "embedding = embeddingLookUpMatrix[inputs]\n",
    "hiddenLayerStates = torch.tanh(embedding.view(-1, inputBlockSize*embeddingFeatureSpaceLength) @ weightsOfHiddenLayer + biasesOfHiddenLayer)\n",
    "logits = hiddenLayerStates @ weightsOfFinalLayer + biasesOfFinalLayer\n",
    "loss = F.cross_entropy(logits, outputs)\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Backward Pass\n",
    "for parameter in parameters:\n",
    "    parameter.grad = None\n",
    "loss.backward()\n",
    "\n",
    "# Update Weights\n",
    "learning_rate = 0.1\n",
    "for parameter in parameters:\n",
    "    parameter.data += -learning_rate * parameter.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b2c4f2-e6bd-469b-8a65-1796825f0234",
   "metadata": {},
   "source": [
    "So we can take it in a loop to train the model over and over..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3b6943d-9fa1-48ae-96ac-27ed1702b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(7.7304, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(6.5795, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(5.4027, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.4660, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(4.5961, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.2872, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(3.2044, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.4061, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(2.0618, grad_fn=<NllLossBackward0>)\n",
      "Loss: tensor(1.8381, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We define the number of epochs\n",
    "epochs = 10\n",
    "for _ in range(epochs):\n",
    "    # Forward Pass\n",
    "    embedding = embeddingLookUpMatrix[inputs]\n",
    "    hiddenLayerStates = torch.tanh(embedding.view(-1, inputBlockSize*embeddingFeatureSpaceLength) @ weightsOfHiddenLayer + biasesOfHiddenLayer)\n",
    "    logits = hiddenLayerStates @ weightsOfFinalLayer + biasesOfFinalLayer\n",
    "    loss = F.cross_entropy(logits, outputs)\n",
    "    print(\"Loss:\", loss)\n",
    "    \n",
    "    # Backward Pass\n",
    "    for parameter in parameters:\n",
    "        parameter.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update Weights\n",
    "    learning_rate = 0.1\n",
    "    for parameter in parameters:\n",
    "        parameter.data += -learning_rate * parameter.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62729c-c33d-400f-8cd8-edb01b7ad11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
