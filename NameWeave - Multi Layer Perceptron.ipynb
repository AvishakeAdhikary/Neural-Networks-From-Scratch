{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908f6d13-f8ae-4cc6-b33c-ccc67424b11f",
   "metadata": {},
   "source": [
    "# Welcome to NameWeave - Multi Layer Perceptron Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abcd4f7-a35a-42e6-96e4-061a8ec1d627",
   "metadata": {},
   "source": [
    "Like our original <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a>,\\\n",
    "We will try to create a **Multi Layer Perceptron** to build a character level language model and predict names based on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22300d7d-798b-4837-9c4b-4264461cdb89",
   "metadata": {},
   "source": [
    "To Approach this model, we will follow an approach based on the paper,\\\n",
    "<a href=\"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\">A Neural Probabilistic Language Model</a>,\\\n",
    "Which is a **word level language model** but solves the similar problem of predicting words...\\\n",
    "This paper is 19 pages long, and we don't have time to read the entire paper,\\\n",
    "But I invite you to read it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e0abc-ea74-4034-be72-2a30002f1368",
   "metadata": {},
   "source": [
    "In this paper,\\\n",
    "They used a word vocabulary of 17000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a68db0-b3b3-4403-ad33-14e359b7fdab",
   "metadata": {},
   "source": [
    "![Word Vocabulary](ExplanationMedia/Images/Vocabulary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33363f-56fc-4dc4-833c-d3b5f00a0a0b",
   "metadata": {},
   "source": [
    "They then converted this vocabulary into a 30 dimensional feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8913c-97a4-4a7c-90d7-c2955bfeab19",
   "metadata": {},
   "source": [
    "![Vocabulary to Feature Space](ExplanationMedia/Images/VocabularytoFeatureSpace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484bf79-78db-4c4b-9316-5a6cc0dcd172",
   "metadata": {},
   "source": [
    "This is a very small space for a very large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c5d75-0090-491f-ae03-69248b1d7b01",
   "metadata": {},
   "source": [
    "The approach of this paper is also very similar because,\\\n",
    "They used a **multilayer neural network** to **predict the next word given the previous ones**,\\\n",
    "& they **maximize the log-likelihood** of the training data or a regularized criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c148c6-4349-4a99-834c-af86cf5bb319",
   "metadata": {},
   "source": [
    "#### Why does this approach work? Let's take a concrete example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5c121-0e85-45c2-9ed2-2097ec1d555b",
   "metadata": {},
   "source": [
    "We have a phrase: *A dog was running in a room*, *The cat is walking in the bedroom*\n",
    "\n",
    "During the training of the network, the words move around to a similar corner of the space based on their features\\\n",
    "So, even if the model goes *out of distribution* during test, making predictions,\\\n",
    "The similar words which have never occured before may occur here.\n",
    "\n",
    "Resulting in the phrase: *A dog is walking in a bedroom*, *The cat is running in a room*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dfa77-e029-4a49-a726-501a666a5f8f",
   "metadata": {},
   "source": [
    "If we knew that dog and cat played similar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), (is,was), we could naturally transfer probability mass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad7ecb-f523-4645-a6ac-803520992a4b",
   "metadata": {},
   "source": [
    "Let's now look at the Neural Network for this Approach\n",
    "\n",
    "![A Neural Probabilistic Language Model - Neural Network](https://miro.medium.com/v2/resize:fit:1200/1*EqKiy4-6tuLSoPP_kub33Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d53d7-4ee8-4724-ace1-15d922c91ed3",
   "metadata": {},
   "source": [
    "In this network,\\\n",
    "They are taking *3 previous words* and are trying to *predict the 4-th word in a sequence*.\n",
    "\n",
    "Now because they had the vocabulary of 17000 words (**'w'**),\\\n",
    "These previous words are the indexes ranging from 0-16999.\n",
    "\n",
    "There is also a lookup-table which they call **'C'** \\\n",
    "This is their lookup-embedding-matrix, which is shared among all the words\\\n",
    "This C is a matrix of say 17000x30. (So, number of words in vocabulary by number of dimensions in the feature space).\n",
    "\n",
    "So what this is essentially doing is,\\\n",
    "They are trying to pick out the row based on the index of the word from vocabulary\\\n",
    "And the row represents the 1x30 vector of the word's embedding.\\\n",
    "So they are using the same matrix over and over to look for their own vector of embedding.\n",
    "\n",
    "So, because they are taking *3 previous words*, and each vector uses 1x30 dimensions,\\\n",
    "They have 3x30 dimensions making up 90 dimensions in total.\n",
    "\n",
    "Next up is the hidden layer (tanh non-linearity layer).\\\n",
    "This is layer has the *hyper-parameter* (*hyper-parameter* is a parameter of the neural network, that is the designer' choice of the neural network).\\\n",
    "So, this layer's size can be as large as we'd like or as we'd like.\\\n",
    "So, we are going to go over multiple choices, and we are going to evaluage how good they work.\\\n",
    "Note: This layer will be fully connected to all the vector embeddings of the previous layer (90 dimensions).\n",
    "\n",
    "Next, they have a output layer of logits (you can refer to the original <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a>  for reference).\\\n",
    "Now, because they had a vocabulary of **17000 words**, this layer has **17000 neurons** which is **fully connected to the hidden layer**.\\\n",
    "Resulting in the *maximum computation between the hidden layer and output layer*.\n",
    "\n",
    "This output layer is then having a softmax activation layer, which exponentiates the logits and normalized to sum to 1.\\\n",
    "Which results in a nice probability distribution for the next *4-th word in a sequence*.\n",
    "\n",
    "<hr>\n",
    "\n",
    "During training we have the label (identity of the next word in a sequence).\n",
    "That word's index is used to choose the probability of that word,\\\n",
    "And then they maximize the probability of that word, with respect to the parameters of this neural network.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Parameters:\n",
    "1. The *weights and biases* of the *output layer*\n",
    "2. The *weights and biases* of the *hidden layer*\n",
    "3. The *embedding look-up table 'C'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9e483-f162-4ca7-8ee5-70aecc3b0dd3",
   "metadata": {},
   "source": [
    "So, Let's implement our own neural network, based on the above approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea5c56-a624-4206-8e00-c719c229ac73",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8ad30a-3f8e-4c74-babc-f77e59c79180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bd709-ec2a-48b6-bbaf-5193824e1426",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "efa7700f-aa4a-47af-8a12-23edb9a6083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F # This is required for one-hot encoding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f5d1b-c5be-4871-aed3-d349329f287f",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8277572-79a6-4ac7-878e-6235589d4f19",
   "metadata": {},
   "source": [
    "Once again, you can refer to the original <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave.ipynb\">NameWeave</a> for reference, as to why we chose to load the dataset in the following way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a0ea9b-821b-4492-a37d-f6f7a4e774c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"Datasets/Indian_Names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f181a3d-a0e0-43fc-9a9e-3014b0be05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8ce030-e90f-4527-82c4-aefcebe96242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53982"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931037f-8025-4be4-969f-5c72eecffdef",
   "metadata": {},
   "source": [
    "# Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74bfacc-e51a-4448-9f12-772f4487f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "STOI: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "ITOS {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Remember we need our starting and ending tokens as well in these mappings,\n",
    "characters = sorted(list(set(''.join(words)))) # Gives us all the characters in the english alphabet, hopefully our dataset has all of them\n",
    "stoi = {s:i+1 for i,s in enumerate(characters)} # Enumerate returns the tuples of number and string, which can then be mapped to string:index\n",
    "# We manually add these tokens for convenience\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()} # After we have the string:index mapping, we can easily iterate over their items to map index:string\n",
    "print(\"Characters:\",characters)\n",
    "print(\"STOI:\",stoi)\n",
    "print(\"ITOS\",itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd7c8d-b5a0-4f3b-8b61-316578d5d1df",
   "metadata": {},
   "source": [
    "# Building Dataset for Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa3a85-f68d-4e2b-8f53-0b3d94f2dd39",
   "metadata": {},
   "source": [
    "Now, we can't just feed in names to our Neural Network.\\\n",
    "Rather, we need to build a dataset which will be able to feed into our neural network.\n",
    "\n",
    "Let's visualize how we are going to feed in to the neural network first...\n",
    "\n",
    "![Multi Layer Perceptron Approach](ExplanationMedia/Images/NameWeaveMultiLayerPerceptronApproach.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64248f4-8188-431d-a613-6a5a2b4fc9cd",
   "metadata": {},
   "source": [
    "Let's now try to make this dataset...\n",
    "\n",
    "Remeber, this is **not bigram anymore**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dcc320c-f396-4bfd-b70f-229a2f235c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: aaban\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> a\n",
      "aba ---> n\n",
      "ban ---> .\n",
      "Name: aabharan\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> h\n",
      "abh ---> a\n",
      "bha ---> r\n",
      "har ---> a\n",
      "ara ---> n\n",
      "ran ---> .\n",
      "Name: aabhas\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> h\n",
      "abh ---> a\n",
      "bha ---> s\n",
      "has ---> .\n",
      "Name: aabhat\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> h\n",
      "abh ---> a\n",
      "bha ---> t\n",
      "hat ---> .\n",
      "Name: aabheer\n",
      "... ---> a\n",
      "..a ---> a\n",
      ".aa ---> b\n",
      "aab ---> h\n",
      "abh ---> e\n",
      "bhe ---> e\n",
      "hee ---> r\n",
      "eer ---> .\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "inputBlockSize = 3\n",
    "\n",
    "# We define two lists, inputs & outputs, where inputs are our blocks of the block size mentioned above and outputs are the label indexes\n",
    "inputs , outputs = [], []\n",
    "\n",
    "# We run a loop for each word in the original dataset\n",
    "for word in words[:5]:\n",
    "    # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "    block = [0] * inputBlockSize # This is also known as the context of the network\n",
    "    # We print each word\n",
    "    print(\"Name:\", word)\n",
    "    # We run another loop for each word's character, here word also needs the ending token '.'\n",
    "    for character in word + '.':\n",
    "        # We take out the index from our look-up table\n",
    "        index = stoi[character]\n",
    "        # We append the input with our block\n",
    "        inputs.append(block)\n",
    "        # We append the output label with out index of the character\n",
    "        outputs.append([index])\n",
    "        # We can check our inputs and thier corresponsing outputs\n",
    "        print(''.join(itos[i] for i in block), '--->', itos[index])\n",
    "        # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "        block = block[1:] + [index]\n",
    "# We also convert these inputs and outputs to tensors for neural network processing\n",
    "inputs = torch.tensor(inputs)\n",
    "outputs = torch.tensor(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6e2afd8-a1d4-4319-909b-3e1edcf4d3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs Shape: torch.Size([37, 3]) , Datatype: torch.int64\n",
      "Outputs Shape: torch.Size([37, 1]) , Datatype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# We can now check the shape of inputs and outputs and their corresponding datatypes\n",
    "print(\"Inputs Shape:\",inputs.shape,\", Datatype:\",inputs.dtype)\n",
    "print(\"Outputs Shape:\",outputs.shape,\", Datatype:\",outputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b99fb60d-ba43-4f72-acd2-6e68f1e3cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  1],\n",
      "        [ 2,  1, 14],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  8],\n",
      "        [ 2,  8,  1],\n",
      "        [ 8,  1, 18],\n",
      "        [ 1, 18,  1],\n",
      "        [18,  1, 14],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  8],\n",
      "        [ 2,  8,  1],\n",
      "        [ 8,  1, 19],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  8],\n",
      "        [ 2,  8,  1],\n",
      "        [ 8,  1, 20],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1,  1],\n",
      "        [ 1,  1,  2],\n",
      "        [ 1,  2,  8],\n",
      "        [ 2,  8,  5],\n",
      "        [ 8,  5,  5],\n",
      "        [ 5,  5, 18]])\n"
     ]
    }
   ],
   "source": [
    "# We can also check how the inputs look like\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "039c646f-5531-4d21-a100-03d1261646c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 1],\n",
      "        [14],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 8],\n",
      "        [ 1],\n",
      "        [18],\n",
      "        [ 1],\n",
      "        [14],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 8],\n",
      "        [ 1],\n",
      "        [19],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 8],\n",
      "        [ 1],\n",
      "        [20],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 2],\n",
      "        [ 8],\n",
      "        [ 5],\n",
      "        [ 5],\n",
      "        [18],\n",
      "        [ 0]])\n"
     ]
    }
   ],
   "source": [
    "# We can also check how the outputs look like\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45600e7-ea13-4f48-b536-75198d7feaf0",
   "metadata": {},
   "source": [
    "Now that we have our inputs and outputs configured, let's build our **embeddingLookUpMatrix 'C'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbd4a0-4539-4566-b2ee-f4faebc60296",
   "metadata": {},
   "source": [
    "# Building Embedding Look-up Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba1800-88f7-4510-b73b-98adf215d80b",
   "metadata": {},
   "source": [
    "In the paper the researchers had a big vocabulary of 17000 words,\\\n",
    "They used a very small 30 dimensional feature space.\n",
    "\n",
    "Because we have a vocabulary of only 27 characters,\\\n",
    "Let's use a very small 2 dimensional feature space for our embedding look-up matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "45b25701-bc49-483b-8c10-93302fe5427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3346,  0.6162],\n",
      "        [ 0.4039, -2.8433],\n",
      "        [ 0.9639, -0.1585],\n",
      "        [ 0.3347,  0.3767],\n",
      "        [ 0.0578,  0.9342],\n",
      "        [-1.0575, -1.4122],\n",
      "        [ 1.2202, -0.5792],\n",
      "        [ 0.5931, -0.3377],\n",
      "        [-1.5203, -0.3711],\n",
      "        [-0.0503, -0.4452],\n",
      "        [ 0.1173, -0.4054],\n",
      "        [ 1.7778,  0.7496],\n",
      "        [-0.7196, -2.4921],\n",
      "        [ 1.6031, -0.0101],\n",
      "        [-1.6452,  1.4524],\n",
      "        [ 1.3075, -0.2939],\n",
      "        [-0.3972,  0.1475],\n",
      "        [-0.5473, -0.1973],\n",
      "        [-0.4729, -0.1627],\n",
      "        [-0.8425,  1.7155],\n",
      "        [ 2.5917,  1.3175],\n",
      "        [-0.7879,  0.9584],\n",
      "        [-0.1324,  1.8445],\n",
      "        [ 1.8695, -0.0285],\n",
      "        [-0.5529,  0.4944],\n",
      "        [ 0.0266,  2.5908]])\n"
     ]
    }
   ],
   "source": [
    "# We decide to build a embeddingLookUpMatrix with 27x2 because we have a vocabulary of 27 characters and we want to fit them in a 2 dimensional space\n",
    "# In the beginning we initialize it randomly\n",
    "embeddingFeatureSpaceLength = 2\n",
    "embeddingLookUpMatrix = torch.randn((len(characters),embeddingFeatureSpaceLength))\n",
    "# So each one of our 27 characters will have a 2 dimensional embedding\n",
    "print(embeddingLookUpMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c15f30-c740-4123-b11b-454ee80a4b2b",
   "metadata": {},
   "source": [
    "Now that we have a embedding-look-up matrix,\n",
    "\n",
    "For example,\n",
    "\n",
    "We can easily do:\n",
    "```python\n",
    "embeddingLookUpMatrix[6]\n",
    "```\n",
    "\n",
    "To get the embedding:\n",
    "```python\n",
    "tensor([-0.2483, -0.3909])\n",
    "```\n",
    "\n",
    "But there is a more similar way to do the exact same thing based on one-hot encoding....\n",
    "\n",
    "We can do:\n",
    "```python\n",
    "F.one_hot(torch.tensor(6), num_classes=27)\n",
    "```\n",
    "\n",
    "To get the one-hot embedding\n",
    "```python\n",
    "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "```\n",
    "\n",
    "Then convert this to float and multiply it with our original *embeddingLookUpMatrix*:\n",
    "```python\n",
    "F.one_hot(torch.tensor(6), num_classes=27).float() @ embeddingLookUpMatrix\n",
    "```\n",
    "\n",
    "Which results in:\n",
    "```python\n",
    "tensor([-0.2483, -0.3909])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b373dae-4ec4-45cd-99b9-2d1b53f1e8c9",
   "metadata": {},
   "source": [
    "This works because of the property of matrix multiplication,\n",
    "\n",
    "The 0's in our one-hot encoded vector discards all the zeros,\\\n",
    "And only multiplies the 1 to the corresponding column of the embeddingLookUpMatrix.\n",
    "\n",
    "So we can consider this matrix multiplication to be the first layer of our neural network,\\\n",
    "Giving us our corresponding embedding for the index. *(1x2 embedding vector for our case)*\n",
    "\n",
    "But we will simply index into our look-up table and discard the way of one-hot encoding for the time being"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d96f2-29d6-4d21-91e6-7f93856cff24",
   "metadata": {},
   "source": [
    "But, now that we know that we want to index into our look-up embedding matrix,\\\n",
    "How do we do that simultaneously for all the inputs?\n",
    "\n",
    "PyTorch has got you covered.\n",
    "\n",
    "In PyTorch we can, very flexibly pick out rows...\n",
    "\n",
    "For example,\\\n",
    "We can do indexing with lists of indexes:\n",
    "```python\n",
    "embeddingLookUpMatrix[[1,2,3]]\n",
    "```\n",
    "\n",
    "Which gives out the rows of the corresponding indexes:\n",
    "```python\n",
    "tensor([[ 0.2118,  1.0454],\n",
    "        [ 0.1876,  0.8921],\n",
    "        [ 0.9759, -0.2606]])\n",
    "```\n",
    "\n",
    "We can also do:\n",
    "```python\n",
    "embeddingLookUpMatrix[torch.tensor([1,2,3])]\n",
    "```\n",
    "\n",
    "Which gives out the rows of the corresponding indexes:\n",
    "```python\n",
    "tensor([[ 0.2118,  1.0454],\n",
    "        [ 0.1876,  0.8921],\n",
    "        [ 0.9759, -0.2606]])\n",
    "```\n",
    "\n",
    "We can similarly pick out the same rows again and again:\n",
    "```python\n",
    "embeddingLookUpMatrix[[1,2,3,3,3,3]]\n",
    "```\n",
    "\n",
    "Which gives us the same row again and again:\n",
    "```python\n",
    "tensor([[ 0.2118,  1.0454],\n",
    "        [ 0.1876,  0.8921],\n",
    "        [ 0.9759, -0.2606],\n",
    "        [ 0.9759, -0.2606],\n",
    "        [ 0.9759, -0.266],\n",
    "        [ 0.9759, -0.2606]])\n",
    "```\n",
    "\n",
    "Lastly, the magic happens when we try to do the same with multi dimensional lists as well:\n",
    "```python\n",
    "embeddingLookUpMatrix[[1, 0], [1, 1]]\n",
    "```\n",
    "\n",
    "Which results in:\n",
    "```python\n",
    "tensor([ 1.0454, -0.9078])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ec17e65-4e17-442c-a9c8-c8d0db40377e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[ 0.9639, -0.1585],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [-1.6452,  1.4524]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585],\n",
       "         [-1.5203, -0.3711]],\n",
       "\n",
       "        [[ 0.9639, -0.1585],\n",
       "         [-1.5203, -0.3711],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[-1.5203, -0.3711],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [-0.4729, -0.1627]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [-0.4729, -0.1627],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[-0.4729, -0.1627],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [-1.6452,  1.4524]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585],\n",
       "         [-1.5203, -0.3711]],\n",
       "\n",
       "        [[ 0.9639, -0.1585],\n",
       "         [-1.5203, -0.3711],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[-1.5203, -0.3711],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [-0.8425,  1.7155]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585],\n",
       "         [-1.5203, -0.3711]],\n",
       "\n",
       "        [[ 0.9639, -0.1585],\n",
       "         [-1.5203, -0.3711],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[-1.5203, -0.3711],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 2.5917,  1.3175]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[-0.3346,  0.6162],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585]],\n",
       "\n",
       "        [[ 0.4039, -2.8433],\n",
       "         [ 0.9639, -0.1585],\n",
       "         [-1.5203, -0.3711]],\n",
       "\n",
       "        [[ 0.9639, -0.1585],\n",
       "         [-1.5203, -0.3711],\n",
       "         [-1.0575, -1.4122]],\n",
       "\n",
       "        [[-1.5203, -0.3711],\n",
       "         [-1.0575, -1.4122],\n",
       "         [-1.0575, -1.4122]],\n",
       "\n",
       "        [[-1.0575, -1.4122],\n",
       "         [-1.0575, -1.4122],\n",
       "         [-0.4729, -0.1627]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we can now easily do\n",
    "embeddingLookUpMatrix[inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3edb92e9-8f70-444a-8129-3764415a5519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 3, 2])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also check the shape of this\n",
    "embeddingLookUpMatrix[inputs].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d0c59-9e9f-40e2-a166-1e2dcd7a37dc",
   "metadata": {},
   "source": [
    "We see that the size of this index is the shape of the original size of the dataset with a 2-dimensional embedding vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc906b2-34d9-485b-88a6-b97ec2493cd4",
   "metadata": {},
   "source": [
    "So if we do:\n",
    "```python\n",
    "# Input of 5th block and 3rd index of the block\n",
    "inputs[5,2]\n",
    "```\n",
    "\n",
    "It gives us:\n",
    "```python\n",
    "tensor(14)\n",
    "```\n",
    "\n",
    "We can look that vector up by doing:\n",
    "```python\n",
    "embeddingLookUpMatrix[inputs][5,2]\n",
    "```\n",
    "\n",
    "Which gives us the corresponding vector of the item specified:\n",
    "```python\n",
    "tensor([ 1.7724, -0.9331])\n",
    "```\n",
    "\n",
    "We can verify the same by doing:\n",
    "```python\n",
    "embeddingLookUpMatrix[14]\n",
    "```\n",
    "\n",
    "Gives the same output:\n",
    "```python\n",
    "tensor([ 1.7724, -0.9331])\r",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a963c5f-13f5-47b7-84d3-33eae2eb22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can now define our embedding into a variable\n",
    "embedding = embeddingLookUpMatrix[inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d716f8-7e83-4b80-a994-5e8781195eef",
   "metadata": {},
   "source": [
    "# Constructing Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273ea35-ab3b-4dec-ab40-7c4883f7b323",
   "metadata": {},
   "source": [
    "Let's understand what we will initially have in the hidden layer.\n",
    "\n",
    "1. The hidden layer will have it's own weights & biases\n",
    "2. The hidden layer will have it's own neurons which will act as a hyper-parameter to set the number of neurons we want in this layer\n",
    "\n",
    "So let's initialize weights and biases for now...\n",
    "\n",
    "Note: The size of the weights will be based on the block size of the inputs and its corresponding vector embedding.\\\n",
    "Thus,\n",
    "\n",
    "$$\\text{Hidden Layer Size} = [(\\text{Block Size} * \\text{Vector Embedding Dimensions}), \\text{Number of Neurons(Hyperparameter)}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f11e60b6-9b9b-40c0-b854-0d4028481fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can initialize the number of neurons we want in the hidden layer\n",
    "numberOfHiddenLayerNeurons = 100\n",
    "# Then we can randomly initialize the weights of the hidden layer\n",
    "weightsOfHiddenLayer = torch.randn((inputBlockSize*embeddingFeatureSpaceLength), numberOfHiddenLayerNeurons)\n",
    "# Then we can initialize the corresponding biases as well\n",
    "biasesOfHiddenLayer = torch.randn(numberOfHiddenLayerNeurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "379233f4-d2dd-4cfe-b5d5-9723663d38dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Weights of Hidden Layer: torch.Size([6, 100])\n",
      "Shape of Biases of Hidden Layer: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# We can check the shapes of our hidden layer weights and hidden layer biases\n",
    "print(\"Shape of Weights of Hidden Layer:\", weightsOfHiddenLayer.shape)\n",
    "print(\"Shape of Biases of Hidden Layer:\", biasesOfHiddenLayer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb8329-506e-4690-aa0b-76626363adac",
   "metadata": {},
   "source": [
    "Now that we have the weights and biases initialized for our hidden layer.\n",
    "\n",
    "By convention we would like to do something like:\n",
    "$$\\text{Layer Computation} = \\text{Embeddings} * \\text{Weights} + \\text{Biases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1dd49-7470-434c-bff9-747c1ff7017a",
   "metadata": {},
   "source": [
    "But for our case:\n",
    "\n",
    "Embeddings are in the shape of the [number of blocks in all the names, block size, vector dimension size]\\\n",
    "for example, [37, 3, 2]\\\n",
    "& Weights are in the shape of [(block size * vector dimension size), number of neurons (hyperparameter)]\\\n",
    "for example, [6, 100]\n",
    "\n",
    "And thus, we cannot just simply multiply these matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f37a10-b884-47a9-8725-ac8a2cc46cca",
   "metadata": {},
   "source": [
    "There are a numerous ways to do this,\\\n",
    "Either we can convert [37, 3, 2] ---> [37, 6]\\\n",
    "Or we can convert [6, 100] ---> [3, 2, 100]\n",
    "\n",
    "We will stick with the first one, because it is fairly simpler and would reduce the complexity to understand the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386218c-e002-4531-a422-314e2b0da870",
   "metadata": {},
   "source": [
    "I invite you to also look into the documentation of <a href=\"https://pytorch.org/docs/stable/index.html\">PyTorch</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210401e5-8d41-419c-8bde-48a891b28a22",
   "metadata": {},
   "source": [
    "According to the official documentation,\n",
    "\n",
    "**TORCH.CAT**\n",
    "Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
    "\n",
    "```python\n",
    "torch.cat(tensors, dim=0, *, out=None) → Tensor\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "- tensors (sequence of Tensors) – any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.\n",
    "- dim (int, optional) – the dimension over which the tensors are concatenated\n",
    "\n",
    "Keyword Arguments:\n",
    "- out (Tensor, optional) – the output tensor.\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> x = torch.randn(2, 3)\n",
    ">>> x\n",
    "tensor([[ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497]])\n",
    ">>> torch.cat((x, x, x), 0)\n",
    "tensor([[ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497],\n",
    "        [ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497],\n",
    "        [ 0.6580, -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497]])\n",
    ">>> torch.cat((x, x, x), 1)\n",
    "tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n",
    "         -1.0969, -0.4614],\n",
    "        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n",
    "         -0.5790,  0.1497]])\n",
    "```\n",
    "\n",
    "So we can do:\n",
    "```python\n",
    "# Pickout the Embedding along the dimension 0, 1 & 2 and concatenate them along dimension 1\n",
    "# Each embedding[:, n, :] gives us the 3x2 embeddings\n",
    "torch.cat([embedding[:, 0, :], embedding[:, 1, :], embedding[:, 2, :]], dim=1)\n",
    "```\n",
    "\n",
    "And its shape turns out to be:\n",
    "```python\n",
    "torch.Size([37, 6])\n",
    "```\n",
    "\n",
    "But this is kind of ugly and we have another method...\n",
    "\n",
    "**TORCH.UNBIND**\n",
    "Removes a tensor dimension.\n",
    "\n",
    "Returns a tuple of all slices along a given dimension, already without it.\n",
    "\n",
    "```python\n",
    "torch.unbind(input, dim=0) → seq\n",
    "```\n",
    "Parameters:\n",
    "- input (Tensor) – the tensor to unbind\n",
    "- dim (int) – dimension to remove\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> torch.unbind(torch.tensor([[1, 2, 3],\n",
    ">>>                            [4, 5, 6],\n",
    ">>>                            [7, 8, 9]]))\n",
    "(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\n",
    "```\n",
    "\n",
    "So now we can do:\n",
    "```python\n",
    "torch.cat(torch.unbind(embedding, dim=1), dim=1)\n",
    "```\n",
    "\n",
    "Whose shape also turns out to be:\n",
    "```python\n",
    "torch.Size([37, 6])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0fbba-7b41-4e8b-9570-3a14aeb51038",
   "metadata": {},
   "source": [
    "We have a third way of doing the same thing.\n",
    "\n",
    "Its called:\\\n",
    "**TORCH.TENSOR.VIEW**\n",
    "\n",
    "Which gives me the paternity to explain some of the features of the internals of the PyTorch Library.\n",
    "\n",
    "We have an interesting blog post <a href=\"http://blog.ezyang.com/2019/05/pytorch-internals/\">here</a> by Edward Z. Yang which you can go through to understand more about this.\n",
    "\n",
    "To explain *TORCH.TENSOR.VIEW*.\\\n",
    "Let's take an example and explain each step one by one...\n",
    "\n",
    "For example,\\\n",
    "If we do:\n",
    "```python\n",
    "torch.arange(0,18)\n",
    "```\n",
    "\n",
    "It gives us:\n",
    "```python\n",
    "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
    "```\n",
    "\n",
    "This can also be viewed as:\n",
    "```python\n",
    "torch.arange(0,18).view(9,2)\n",
    "```\n",
    "\n",
    "Which gives us:\n",
    "```python\n",
    "tensor([[ 0,  1],\n",
    "        [ 2,  3],\n",
    "        [ 4,  5],\n",
    "        [ 6,  7],\n",
    "        [ 8,  9],\n",
    "        [10, 11],\n",
    "        [12, 13],\n",
    "        [14, 15],\n",
    "        [16, 17]])\n",
    "```\n",
    "\n",
    "This can also be written as:\n",
    "```python\n",
    "torch.arange(0,18).view(3,3,2)\n",
    "```\n",
    "\n",
    "Which gives us:\n",
    "```python\n",
    "tensor([[[ 0,  1],\n",
    "         [ 2,  3],\n",
    "         [ 4,  5]],\n",
    "\n",
    "        [[ 6,  7],\n",
    "         [ 8,  9],\n",
    "         [10, 11]],\n",
    "\n",
    "        [[12, 13],\n",
    "         [14, 15],\n",
    "         [16, 17]]])\n",
    "```\n",
    "\n",
    "So,\\\n",
    "If we have an embedding of size say: [37, 3, 2]\\\n",
    "We can essentially do:\n",
    "```python\n",
    "embedding.view(37,6)\n",
    "```\n",
    "\n",
    "We can also verify the result to be the same by doing\n",
    "```python\n",
    "embedding.view(37,6) == torch.cat(torch.unbind(embedding, dim=1), dim=1)\n",
    "```\n",
    "\n",
    "Resulting in all True values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ffce8-13f2-4b96-af4f-88a88a87fb39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
