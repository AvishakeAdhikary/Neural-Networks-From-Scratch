{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60e1889f-a41b-4a08-b77f-349907ed6f94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Welcome to NameWeave - WaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042dd73-d2a3-49c2-86b9-0d290b408e51",
   "metadata": {},
   "source": [
    "In this notebook we will try to complexify our neural network to get better results and generated names from our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a> notebook.\n",
    "\n",
    "But for starters we will have our code very similar to our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a> notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490b706-da0b-4056-aedf-c54aadc5b555",
   "metadata": {},
   "source": [
    "Also, remember how I left an intentional problem lurking under <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/NameWeave%20(MLP)%20-%20Activations%2C%20Gradients%20%26%20Batch%20Normalization.ipynb\">NameWeave (MLP) - Activations, Gradients & Batch Normalization</a> notebook?\n",
    "\n",
    "I fixed it during this starter code, so hopefully you'll also be able to recognize what we have to be careful with...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df5710-db0e-435a-b9f1-bc8a9f2e189f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d0e5c4-3866-43fd-b1ce-1f136bdc70b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af9b1f-56e8-490a-9ba5-e5406b9cf29b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff899079-b80d-4e89-b34f-60364c521141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avhis\\AppData\\Local\\Temp\\ipykernel_15456\\3082457058.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89fead9-91fd-42dd-843e-295b5088fd64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb6e322-fb26-4de4-97a0-e97d97fe2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"Datasets/Indian_Names.txt\").read().splitlines()\n",
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376b910a-40b5-40c4-afd2-496b20516362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53982"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020299d-4b5e-403f-90bd-002b0325340b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369648a6-125c-49c0-921c-a9e8c3fd6077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "STOI: {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "ITOS {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "Vocabulary Size: 27\n"
     ]
    }
   ],
   "source": [
    "# Remember we need our starting and ending tokens as well in these mappings,\n",
    "characters = sorted(list(set(''.join(words)))) # Gives us all the characters in the english alphabet, hopefully our dataset has all of them\n",
    "stoi = {s:i+1 for i,s in enumerate(characters)} # Enumerate returns the tuples of number and string, which can then be mapped to string:index\n",
    "# We manually add these tokens for convenience\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()} # After we have the string:index mapping, we can easily iterate over their items to map index:string\n",
    "print(\"Characters:\", characters)\n",
    "print(\"STOI:\", stoi)\n",
    "print(\"ITOS\", itos)\n",
    "# We define a common vocabulary size\n",
    "vocabularySize = len(stoi)\n",
    "print(\"Vocabulary Size:\", vocabularySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8363b31-4db9-4211-b145-779a2cb933c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b165d663-0d02-46dc-9a06-a1838c6fcf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Examples: 53982\n",
      "Training Examples: 43185\n",
      "Validation Examples: 5398\n",
      "Test Examples: 5399\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 3\n",
    "# Will build the dataset on only the words we take as input\n",
    "def buildDataset(words):\n",
    "    # We define two lists, inputs & outputs, where inputs are our blocks of the block size mentioned above and outputs are the label indexes\n",
    "    inputs , outputs = [], []\n",
    "    # We iterate over each word\n",
    "    for word in words:\n",
    "        # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "        block = [0] * blockSize\n",
    "        # We run another loop for each word's character, here word also needs the ending token '.'\n",
    "        for character in word + '.':\n",
    "            # We take out the index from our look-up table\n",
    "            index = stoi[character]\n",
    "            # We append the input with our block\n",
    "            inputs.append(block)\n",
    "            # We append the output label with out index of the character\n",
    "            outputs.append([index])\n",
    "            # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "            block = block[1:] + [index]\n",
    "    # We also convert these inputs and outputs to tensors for neural network processing\n",
    "    inputs = torch.tensor(inputs)\n",
    "    outputs = torch.flatten(torch.tensor(outputs))\n",
    "    # We return the inputs and outputs\n",
    "    return inputs, outputs\n",
    "\n",
    "# We define a manual seed to random\n",
    "random.seed(69)\n",
    "# We shuffle all the words, so that the model receives all kinds of data\n",
    "random.shuffle(words)\n",
    "# We define two number of inputs\n",
    "# We take the number of examples to 80% in the first variable\n",
    "numberOfInputs1 = int(0.8*len(words))\n",
    "# We take the number of examples to 90% in the first variable\n",
    "numberOfInputs2 = int(0.9*len(words))\n",
    "# Inputs and outputs that go till 80% of the examples\n",
    "trainingInputs, trainingOutputs = buildDataset(words[:numberOfInputs1])\n",
    "# Inputs and outputs that start at 80% of the examples and go till 90% of the examples\n",
    "validationInputs, validationOutputs = buildDataset(words[numberOfInputs1:numberOfInputs2])\n",
    "# Inputs and outputs that start at 90% of the examples\n",
    "testInputs, testOutputs = buildDataset(words[numberOfInputs2:])\n",
    "\n",
    "# We can check the numbers\n",
    "print(\"Total Examples:\",len(words)) # 100%\n",
    "print(\"Training Examples:\",len(words[:numberOfInputs1])) # 80%\n",
    "print(\"Validation Examples:\",len(words[numberOfInputs1:numberOfInputs2])) # 10%\n",
    "print(\"Test Examples:\",len(words[numberOfInputs2:])) # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82958536-d9f7-4f27-ae13-672273e660e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modular Block Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92509e46-8907-4dca-b1d2-752d721b7e30",
   "metadata": {},
   "source": [
    "You will see that we previously had a generator object in this modular code...\n",
    "\n",
    "This is for seed reproducability.\n",
    "\n",
    "And we will only use a manual seeding directly into PyTorch using:\n",
    "```python\n",
    "torch.manual_seed(69420)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0c6e2c-670a-4f8c-b6c0-aaa1094841de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # Constructor to initialize the weights and biases\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # We initialize the weights using fan-in and fan-out and use the kaiming initialization which is the square root of fan-in\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        # We initilize the biases using fan-out and set them to 0 values if bias is set to True else we set it to None\n",
    "        self.biases = torch.zeros(fan_out) if bias else None\n",
    "    # Defines how we forward pass these layers which is (y = wx + b)\n",
    "    def __call__(self, inputs):\n",
    "        # (w*x)\n",
    "        self.out = inputs @ self.weights\n",
    "        # Checking if we have a bias\n",
    "        if self.biases is not None:\n",
    "            # (wx) + b\n",
    "            self.out += self.biases\n",
    "        # We return (y = wx + b)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # If bias is there return (w*x + b) else return (w*x)\n",
    "        return [self.weights] + ([] if self.biases is None else [self.biases])\n",
    "class BatchNorm1d:\n",
    "    # Constructor to initialize the gain, bias, and the buffers (running mean and running variance) using the dimensions we get\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        # We initialize the gains to 1 values of dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        # We initialize the bias to 0 values of dimensions\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        # We initialize the running mean to 0 values of dimensions\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        # We initialize the running variance to 1 values of dimensions\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        # We initialize a checker parameter that checks whether we are training or not (We will discuss this later) \n",
    "        self.training = training\n",
    "    # Forward Pass and Buffer Updation based on the inputs\n",
    "    def __call__(self, inputs):\n",
    "        # Forward Pass\n",
    "        # ------------\n",
    "        # If training is set to true we will calculate the mean and variance estimated from the current batch of inputs\n",
    "        if self.training:\n",
    "            # Initializing mean of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_mean = inputs.mean(0, keepdim=True)\n",
    "            # Initializing variance of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_variance = inputs.var(0, keepdim=True)\n",
    "        # If not training, we will use the running mean and variance of the inputs of the saved values\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        # We center the inputs around zero and scaling it to have unit variance using the defined formula\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        # After calulating the unit_variance we can perform the operation (y = wx + b) on this layer\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        # If training is set to true we will update the running mean and variance estimated from the momentum (without gradient updation)\n",
    "        if self.training:\n",
    "            # We disable gradient updation\n",
    "            with torch.no_grad():\n",
    "                # We perform (batchRunningMean = 0.999 * batchRunningMean + 0.001 * batchMeanAtIteration)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                # We perform (batchRunningStandardDeviation = 0.999 * batchRunningStandardDeviation + 0.001 * batchStandardDeviationAtIteration)\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        # We return the (y = wx + b) after all the calculations are done (if any)\n",
    "        return self.out\n",
    "        # ------------\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # We return the parameters (gamma and beta)\n",
    "        return [self.gamma, self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self, inputs):\n",
    "        # Calculates tanh based on the inputs\n",
    "        self.out = torch.tanh(inputs)\n",
    "        return self.out\n",
    "    # Returns an empty list because there are no paramters in this layer\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51268a1-fe73-47ff-a9c5-a103c4364584",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Neural Network Initialization - Weights, Biases & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dc6405-bb27-4b96-bf4d-ede111513a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 12124\n"
     ]
    }
   ],
   "source": [
    "# We will define a manual seed to give a similar result on your machine, as of my machine\n",
    "torch.manual_seed(69420)\n",
    "embeddingFeatureSpaceLength = 10\n",
    "numberOfHiddenLayerNeurons = 200\n",
    "# Initializing the embedding look-up matrix\n",
    "embeddingLookUpMatrix = torch.randn((vocabularySize, embeddingFeatureSpaceLength))\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "layers = [\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False), BatchNorm1d(vocabularySize),\n",
    "]\n",
    "\n",
    "# Disabling Gradient Tracking\n",
    "with torch.no_grad():\n",
    "    # Making the final activation less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "\n",
    "# We initialize all the parameters\n",
    "parameters = [embeddingLookUpMatrix] + [parameter for layer in layers for parameter in layer.parameters()]\n",
    "# We check the number of parameters by printing its value\n",
    "print(\"Number of parameters:\", sum(parameter.nelement() for parameter in parameters))\n",
    "# We set 'requires_grad' to True in order to track gradients\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3263d5f4-80d1-447d-a329-82e724938cf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf66833-b4b7-485a-b612-87420c47fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2929\n",
      "  10000/ 200000: 2.4449\n",
      "  20000/ 200000: 1.7085\n",
      "  30000/ 200000: 2.3984\n",
      "  40000/ 200000: 1.8998\n",
      "  50000/ 200000: 2.1423\n",
      "  60000/ 200000: 1.8823\n",
      "  70000/ 200000: 1.8821\n",
      "  80000/ 200000: 2.2512\n",
      "  90000/ 200000: 2.3865\n",
      " 100000/ 200000: 2.0659\n",
      " 110000/ 200000: 1.6045\n",
      " 120000/ 200000: 1.8543\n",
      " 130000/ 200000: 2.1539\n",
      " 140000/ 200000: 1.6183\n",
      " 150000/ 200000: 1.8724\n",
      " 160000/ 200000: 1.8351\n",
      " 170000/ 200000: 1.5911\n",
      " 180000/ 200000: 1.8755\n",
      " 190000/ 200000: 2.0474\n"
     ]
    }
   ],
   "source": [
    "# We want to track the losses that we use in the training\n",
    "losses = []\n",
    "# We define the number of epochs\n",
    "epochs = 200000\n",
    "# We define the batch size\n",
    "batchSize = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Mini-Batches\n",
    "    indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(batchSize,))\n",
    "    inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "    \n",
    "    # Forward Pass (Mini-Batch)\n",
    "    embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "\n",
    "    # Check outputs after batch normalization layers\n",
    "    for layer in layers:\n",
    "        # We call the forward pass of each layer\n",
    "        concatenatedEmbedding = layer(concatenatedEmbedding)\n",
    "    loss = F.cross_entropy(concatenatedEmbedding, outputBatch)\n",
    "    # Backward Pass (Mini-Batch)\n",
    "    # We reset the gradients\n",
    "    for parameter in parameters:\n",
    "        parameter.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update Weights (Mini-Batch)\n",
    "    learning_rate = 0.1 if i < 150000 else 0.01\n",
    "    for parameter in parameters:\n",
    "        parameter.data += -learning_rate * parameter.grad\n",
    "\n",
    "    # Tracking the stats\n",
    "    # We append the losses that we use per iteration\n",
    "    if i % 10000 == 0: \n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # We do that to squash the steep curve to a nicer graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f15345a-1b9d-458e-8ee6-d411aeea1878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x246e64fc750>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de92f85-5207-4cdf-bc0d-5d99e538a008",
   "metadata": {},
   "source": [
    "Our losses look pretty ugly and we will fix that in a second..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30461e5f-dbb6-485b-ad08-fceeb6da58c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluating Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0780c1d-f4d7-45a3-9524-4d4fab0251f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:1.897056221961975\n",
      "Validation Loss:1.9188517332077026\n"
     ]
    }
   ],
   "source": [
    "# Decorator for disabling gradient tracking throughout the function underneath\n",
    "@torch.no_grad()\n",
    "def splitLoss(split):\n",
    "    input, output = {\n",
    "        'Training': (trainingInputs, trainingOutputs),\n",
    "        'Validation': (validationInputs, validationOutputs),\n",
    "        'Testing': (testInputs, testOutputs)\n",
    "    }[split]\n",
    "    # Based on the split we can then index into the embedding look-up matrix using its inputs to get the embeddings\n",
    "    embedding = embeddingLookUpMatrix[input]\n",
    "    concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        concatenatedEmbedding = layer(concatenatedEmbedding)\n",
    "    loss = F.cross_entropy(concatenatedEmbedding, output)\n",
    "    print(f\"{split} Loss:{loss.item()}\")\n",
    "# We can then call this method to calculate and print loss\n",
    "splitLoss('Training')\n",
    "splitLoss('Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57271016-63e6-42a8-8ab8-ff57da3433f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sampling from the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c86bf-d3bd-4a27-8b24-ef41adef7bef",
   "metadata": {},
   "source": [
    "Now comes the main part...\n",
    "\n",
    "I intentionally did not set the training attribute of `BatchNorm1d` layers to `False` last time.\n",
    "\n",
    "First we have to understand that batch normalization behaves differently during training and inference. During training, batch normalization computes batch statistics (mean and variance) based on the current batch of data, which can sometimes lead to issues with small batch sizes or non-representative batches. However, during inference, it's common practice to use precomputed running statistics (accumulated during training) and fix the batch normalization layers to not update their statistics further.\n",
    "\n",
    "By setting `layer.training = False` for `BatchNorm1d` layers during sampling, we effectively switch them to inference mode, ensuring that they use precomputed running statistics (such as the moving average ($\\mu$) for mean and moving variance($\\sigma^2$) for variance) and do not update their parameters (such as $\\gamma$ and $\\beta$, which are learnable parameters) during sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c8957-e106-4420-ba49-80e716ec0d23",
   "metadata": {},
   "source": [
    "So, if you're trying to pass in a single example of *context* into a `Batch-Normalization` layer which is in `Training` mode, we are going to end up estimating the `variance` of the batch.\n",
    "\n",
    "So, what is the issue then? We are trying to estimate the `variance` in the `Batch-Normalization` layer in the first place...\n",
    "\n",
    "Well, `variance` tries to look at the amount of *spread* in the data. So, `variance` of a single number, is `not-a-number` a.k.a `NaN` in PyTorch.\n",
    "\n",
    "For example, if we have a very simple example:\n",
    "```python\n",
    "torch.var(torch.tensor([69.0]))\n",
    "```\n",
    "\n",
    "We get the result:\n",
    "```python\n",
    "tensor(nan)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1516632e-0622-4557-9aeb-8885330b5317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prasenapraguruthrapa.\n",
      "swaran.\n",
      "arveeran.\n",
      "dhy.\n",
      "haranithusree.\n",
      "sud.\n",
      "kadtwaz.\n",
      "ridapathindermitdalan.\n",
      "chini.\n",
      "wen.\n",
      "sivani.\n",
      "haran.\n",
      "kala.\n",
      "yothushan.\n",
      "dheemalaimithasshan.\n",
      "akshinod.\n",
      "tha.\n",
      "prama.\n",
      "nila.\n",
      "divthirampyi.\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 3\n",
    "# We will define a generator to give the same result on your machine, as of my machine\n",
    "numberOfWordsToSample = 20\n",
    "\n",
    "for layer in layers:\n",
    "    layer.training = False\n",
    "\n",
    "# We iterate over the number of words we want to predict\n",
    "for _ in range(numberOfWordsToSample):\n",
    "    # We define a output list to append the next character and print it at the end\n",
    "    output = []\n",
    "    # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "    block = [0] * blockSize\n",
    "\n",
    "    # We will now iterate over each word's characters\n",
    "    while True:\n",
    "        # We would create an output embedding that would be based on the block\n",
    "        embedding = embeddingLookUpMatrix[torch.tensor([block])]\n",
    "        concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "        # Inside the loop where layers are applied\n",
    "        for layer in layers:\n",
    "            concatenatedEmbedding = layer(concatenatedEmbedding)\n",
    "        # Apply softmax\n",
    "        probabilities = F.softmax(concatenatedEmbedding, dim=1)\n",
    "        # We can now sample the next character\n",
    "        index = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "        block = block[1:] + [index]\n",
    "        # We then append the sampled character to the output\n",
    "        output.append(index)\n",
    "        # If we hit '.' end token, we will break out from the loop\n",
    "        if index == 0:\n",
    "            break\n",
    "    # We print generated name out\n",
    "    print(''.join(itos[index] for index in output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a8087-3475-4ecc-b61c-cb075fc34040",
   "metadata": {},
   "source": [
    "Now let's fix the loss graph first...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f428683-f41d-4e6a-a975-d1c09f1aa7ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fixing Loss Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48706631-6202-464d-b5c1-95850fce2ab6",
   "metadata": {},
   "source": [
    "Let's recall what `losses` are first...\n",
    "\n",
    "`losses` are nothing but a python list of floating point numbers.\n",
    "\n",
    "So if we try to print the first `10` numbers from the list:\n",
    "```python\n",
    "losses[:10]\n",
    "```\n",
    "We get something like this:\n",
    "```python\n",
    "[0.517582893371582,\n",
    " 0.5161539316177368,\n",
    " 0.5183981657028198,\n",
    " 0.5125000476837158,\n",
    " 0.5141331553459167,\n",
    " 0.5116288065910339,\n",
    " 0.5155861377716064,\n",
    " 0.5093725919723511,\n",
    " 0.5093583464622498,\n",
    " 0.5094155073165894]\n",
    "```\n",
    "\n",
    "We understand that we are calculating losses of mini-batches of only `32` examples, and we can get *very lucky and very unlucky* in any one of these batches, which eventually creates a very thick loss graph. Despite these fluctuations, over a large number of epochs, the trend in the loss graph typically demonstrates a gradual decrease, indicating the model's learning progress over time. To visualize this trend more effectively, we apply a `logarithmic transformation` to the `loss` values, *squashing* the steep curve into a smoother graph for better interpretation.\n",
    "\n",
    "We also saw that each of our `losses` floating point numbers were stretched out across the number of `epochs` we had during forward pass.\n",
    "\n",
    "Which means if we try to average these `losses` in short intervals of `epochs`, we will smooth out the representation of the *trend* in the `losses` over time..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbdcfa-6fd1-4894-aa79-8d92df06c276",
   "metadata": {},
   "source": [
    "Let's understand how we can do this...\n",
    "\n",
    "For example if we take a tensor of the first `10` numbers we do something like this:\n",
    "```python\n",
    "torch.arange(10)\n",
    "```\n",
    "Which gives us:\n",
    "```python\n",
    "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "```\n",
    "\n",
    "We understand that this is a `1` dimensional array.\n",
    "\n",
    "But we can recall that we can `view` this array in `2` dimensions now...\n",
    "```python\n",
    "torch.arange(10).view(2, 5)\n",
    "```\n",
    "For which we get:\n",
    "```python\n",
    "tensor([[0, 1, 2, 3, 4],\n",
    "        [5, 6, 7, 8, 9]])\n",
    "```\n",
    "Here we see that the first `10` numbers are now being *viewed* as a `2` by `5` array, where the first row consists of the first `5` elements and the second row consists of the `5` numbers right after that.\n",
    "\n",
    "Similarly, we can also do:\n",
    "```python\n",
    "torch.arange(10).view(5, 2)\n",
    "```\n",
    "For which we get:\n",
    "```python\n",
    "tensor([[0, 1],\n",
    "        [2, 3],\n",
    "        [4, 5],\n",
    "        [6, 7],\n",
    "        [8, 9]])\n",
    "```\n",
    "Here we see that the first `10` numbers are now being *viewed* as a `5` by `2` array, where each row consists of `2` elements for `5` rows.\n",
    "\n",
    "We can also recall that we can get the exact same output by using `-1` in place of one of these numbers to make python calculate what the other number must be in order to make the number of elements work out.\n",
    "\n",
    "So for example, we have:\n",
    "```python\n",
    "torch.arange(10).view(-1, 5)\n",
    "```\n",
    "or\n",
    "```python\n",
    "torch.arange(10).view(2, -1)\n",
    "```\n",
    "For which we get:\n",
    "```python\n",
    "tensor([[0, 1, 2, 3, 4],\n",
    "        [5, 6, 7, 8, 9]])\n",
    "```\n",
    "And we have:\n",
    "```python\n",
    "torch.arange(10).view(5, -1)\n",
    "```\n",
    "or\n",
    "```python\n",
    "torch.arange(10).view(-1, 2)\n",
    "```\n",
    "For which we get:\n",
    "```python\n",
    "tensor([[0, 1],\n",
    "        [2, 3],\n",
    "        [4, 5],\n",
    "        [6, 7],\n",
    "        [8, 9]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c150b8-3e6e-407c-a238-63d9f9b9e029",
   "metadata": {},
   "source": [
    "So using the same logic we can use the same logic to first create a tensor out of the original python list `losses` and check the shape of it.\n",
    "\n",
    "Using:\n",
    "```python\n",
    "torch.tensor(losses).shape\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "torch.Size([200000])\n",
    "```\n",
    "\n",
    "We see that we have the floating point loss numbers for each of those number of `epochs`...\n",
    "\n",
    "We can now use the `view` logic to let PyTorch figure out the number of rows using `-1` and use the columns arguement to specify the number of `intervals of epochs` we want to average out.\n",
    "\n",
    "Let's say we want to average out `epochs` of `1000` intervals.\n",
    "\n",
    "So now we can view our rows of `1000 epoch intervals` of `losses`:\n",
    "```python\n",
    "torch.tensor(losses).view(-1, 1000).shape\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "torch.Size([200, 1000])\n",
    "```\n",
    "\n",
    "So we can average out all the rows using the `mean()` at dimension `1`...\n",
    "\n",
    "So if we now check the shape:\n",
    "```python\n",
    "torch.tensor(losses).view(-1, 1000).mean(1).shape\n",
    "```\n",
    "And we see that we get the same number of rows:\n",
    "```python\n",
    "torch.Size([200])\n",
    "```\n",
    "\n",
    "So we can now easily plot this using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81bec19c-7a99-4327-b1ff-89612fd33517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x246e7e72550>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.tensor(losses).view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37272a78-3d19-4c9e-a458-c504e0e6bf6b",
   "metadata": {},
   "source": [
    "This smoothing doesn't eliminate the individual spikes and dips in the loss graph but provides a clearer picture of the overall trend by reducing the impact of those fluctuations...\n",
    "\n",
    "Let's now move on to the next section..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8f414-6c5e-4391-a382-69932ea70fee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# *PyTorch-ifying* Modular Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45830f-d2f2-4507-805f-090d7f9fb2c3",
   "metadata": {},
   "source": [
    "You see how our modular blocks are still a little bit *gnarly* during the forward pass?\n",
    "\n",
    "We use too many lines of code\n",
    "\n",
    "For example, we use:\n",
    "```python\n",
    "    # Forward Pass (Mini-Batch)\n",
    "    embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "    \n",
    "    for layer in layers:\n",
    "        # We call the forward pass of each layer\n",
    "        concatenatedEmbedding = layer(concatenatedEmbedding)\n",
    "    loss = F.cross_entropy(concatenatedEmbedding, outputBatch)\n",
    "```\n",
    "\n",
    "You see how the `inputBatch`(s) are used to index into `embeddingLookUpMatrix`, but it is still outside of the `layers`, and in addition to that the `view()` operation is also outside of these `layers`.\n",
    "\n",
    "Let's modularize this code by creating `layers` for these and we can add them to our `layers[]` list.\n",
    "\n",
    "So we know that we need two things:\n",
    "1. Embedding Layer\n",
    "2. Flatten Layer\n",
    "\n",
    "`Embedding` Layer is required for `embeddingLookUpMatrix` because we index into it using `inputBatch`. And `Flatten` Layer is required because we are effectively doing a *concatenation* operation for the sequence of embeddings we want to push through as an input, and stretches these embeddings into a single row of array, which is basically a **flatening** operation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14400f-ad80-4c95-bcb4-8c40be2a8f28",
   "metadata": {},
   "source": [
    "Now, not surprisingly, these `Embedding` and `Flatten` modules are already there in PyTorch block.\n",
    "\n",
    "For example, the `Embedding` module is there:\n",
    "```python\n",
    "torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, _freeze=False, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "We see that it takes a long list of arguements, but we would like to keep things simple and only have `num_embeddings` and `embedding_dim`.\n",
    "\n",
    "And according to the documentation we see that:\\\n",
    "**Parameters**:\n",
    "- **num_embeddings** (`int`)  size of the dictionary of embeddings\n",
    "- **embedding_dim** (`int`)  the size of each embedding vector\n",
    "\n",
    "Also, for example the `Flatten` module is there:\n",
    "```python\n",
    "torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "```\n",
    "\n",
    "Once again we see that it takes a number of arguements but we are have our own custom *concatenation* operation to aid...\n",
    "\n",
    "So let's now implement the modular blocks..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824884d1-8bc0-44f9-84e2-b0e24b9961b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Modular Block Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9553f3d9-c600-472c-82e0-da347d226206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # Constructor to initialize the weights and biases\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # We initialize the weights using fan-in and fan-out and use the kaiming initialization which is the square root of fan-in\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        # We initilize the biases using fan-out and set them to 0 values if bias is set to True else we set it to None\n",
    "        self.biases = torch.zeros(fan_out) if bias else None\n",
    "    # Defines how we forward pass these layers which is (y = wx + b)\n",
    "    def __call__(self, inputs):\n",
    "        # (w*x)\n",
    "        self.out = inputs @ self.weights\n",
    "        # Checking if we have a bias\n",
    "        if self.biases is not None:\n",
    "            # (wx) + b\n",
    "            self.out += self.biases\n",
    "        # We return (y = wx + b)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # If bias is there return (w*x + b) else return (w*x)\n",
    "        return [self.weights] + ([] if self.biases is None else [self.biases])\n",
    "class BatchNorm1d:\n",
    "    # Constructor to initialize the gain, bias, and the buffers (running mean and running variance) using the dimensions we get\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        # We initialize the gains to 1 values of dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        # We initialize the bias to 0 values of dimensions\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        # We initialize the running mean to 0 values of dimensions\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        # We initialize the running variance to 1 values of dimensions\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        # We initialize a checker parameter that checks whether we are training or not (We will discuss this later) \n",
    "        self.training = training\n",
    "    # Forward Pass and Buffer Updation based on the inputs\n",
    "    def __call__(self, inputs):\n",
    "        # Forward Pass\n",
    "        # ------------\n",
    "        # If training is set to true we will calculate the mean and variance estimated from the current batch of inputs\n",
    "        if self.training:\n",
    "            # Initializing mean of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_mean = inputs.mean(0, keepdim=True)\n",
    "            # Initializing variance of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_variance = inputs.var(0, keepdim=True)\n",
    "        # If not training, we will use the running mean and variance of the inputs of the saved values\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        # We center the inputs around zero and scaling it to have unit variance using the defined formula\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        # After calulating the unit_variance we can perform the operation (y = wx + b) on this layer\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        # If training is set to true we will update the running mean and variance estimated from the momentum (without gradient updation)\n",
    "        if self.training:\n",
    "            # We disable gradient updation\n",
    "            with torch.no_grad():\n",
    "                # We perform (batchRunningMean = 0.999 * batchRunningMean + 0.001 * batchMeanAtIteration)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                # We perform (batchRunningStandardDeviation = 0.999 * batchRunningStandardDeviation + 0.001 * batchStandardDeviationAtIteration)\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        # We return the (y = wx + b) after all the calculations are done (if any)\n",
    "        return self.out\n",
    "        # ------------\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # We return the parameters (gamma and beta)\n",
    "        return [self.gamma, self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self, inputs):\n",
    "        # Calculates tanh based on the inputs\n",
    "        self.out = torch.tanh(inputs)\n",
    "        return self.out\n",
    "    # Returns an empty list because there are no paramters in this layer\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Embedding:\n",
    "    # Constructor to initialize the weights of the embedding\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.weights = torch.randn((num_embeddings, embedding_dim))\n",
    "    # The indexing operation in the forward pass : Similar to -> embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    def __call__(self, indexes):\n",
    "        self.out = self.weights[indexes]\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (weights)\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "class Flatten:\n",
    "    # Concatenation operation in the forward pass : Similar to -> concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "    def __call__(self, inputs):\n",
    "        self.out = inputs.view(inputs.shape[0], -1)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (empty list)\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de3c10-62c0-452d-86db-78219c4cdacc",
   "metadata": {},
   "source": [
    "Now that we have this, we can start modifying our **initialization** to be a more simpler code...\n",
    "\n",
    "Previously we had:\n",
    "```python\n",
    "# Initializing the embedding look-up matrix\n",
    "embeddingLookUpMatrix = torch.randn((vocabularySize, embeddingFeatureSpaceLength))\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "layers = [\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False), BatchNorm1d(vocabularySize),\n",
    "]\n",
    "```\n",
    "So now we can take out the line `embeddingLookUpMatrix = torch.randn((vocabularySize, embeddingFeatureSpaceLength))` and the `Embedding` and `Flatten` layer to the `layers` list...\n",
    "\n",
    "Like this:\n",
    "```python\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "layers = [\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    Flatten(),\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False), BatchNorm1d(vocabularySize),\n",
    "]\n",
    "```\n",
    "\n",
    "Also during initialization we had the `parameters` as:\n",
    "```python\n",
    "# We initialize all the parameters\n",
    "parameters = [embeddingLookUpMatrix] + [parameter for layer in layers for parameter in layer.parameters()]\n",
    "```\n",
    "\n",
    "But now we do not need to *special case* the `embeddingLookUpMatrix` because of the `parameters()` method implemented in our module block...\n",
    "\n",
    "So now we have:\n",
    "```python\n",
    "# We initialize all the parameters\n",
    "parameters = [parameter for layer in layers for parameter in layer.parameters()]\n",
    "```\n",
    "\n",
    "Let's now simplify the **forward pass**...\n",
    "\n",
    "\n",
    "Previously we had:\n",
    "```python\n",
    "# Forward Pass (Mini-Batch)\n",
    "embedding = embeddingLookUpMatrix[inputBatch]\n",
    "concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "    \n",
    "for layer in layers:\n",
    "    # We call the forward pass of each layer\n",
    "    concatenatedEmbedding = layer(concatenatedEmbedding)\n",
    "loss = F.cross_entropy(concatenatedEmbedding, outputBatch)\n",
    "```\n",
    "\n",
    "So now we can take out the lines `embedding = embeddingLookUpMatrix[inputBatch]` and `concatenatedEmbedding = embedding.view(embedding.shape[0], -1)` and put them into our layers...\n",
    "\n",
    "So now we have:\n",
    "```python\n",
    "# Forward Pass (Mini-Batch)\n",
    "for layer in layers:\n",
    "    # We call the forward pass of each layer\n",
    "    inputBatch = layer(inputBatch)\n",
    "loss = F.cross_entropy(inputBatch, outputBatch)\n",
    "```\n",
    "\n",
    "Let's now implement it in our code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e208e-836d-46e2-bc87-92cd926ee828",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Neural Network Initialization - Weights, Biases & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1049099-6573-4f38-bad8-2e805cf90421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 12124\n"
     ]
    }
   ],
   "source": [
    "# We will define a manual seed to give a similar result on your machine, as of my machine\n",
    "torch.manual_seed(69420)\n",
    "embeddingFeatureSpaceLength = 10\n",
    "numberOfHiddenLayerNeurons = 200\n",
    "\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "layers = [\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    Flatten(),\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False), BatchNorm1d(vocabularySize),\n",
    "]\n",
    "\n",
    "# Disabling Gradient Tracking\n",
    "with torch.no_grad():\n",
    "    # Making the final activation less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "\n",
    "# We initialize all the parameters\n",
    "parameters = [parameter for layer in layers for parameter in layer.parameters()]\n",
    "# We check the number of parameters by printing its value\n",
    "print(\"Number of parameters:\", sum(parameter.nelement() for parameter in parameters))\n",
    "# We set 'requires_grad' to True in order to track gradients\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79124a3-4699-437e-8573-e773e86eb2a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbcc499-d328-4706-af8e-8176e69acb79",
   "metadata": {},
   "source": [
    "We will for now insert a `break` at the end just to run the forward pass for the first iteration to make sure it runs properly and we can take it out later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0610b150-682c-4ce4-98cf-f5a666e56785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2929\n"
     ]
    }
   ],
   "source": [
    "# We want to track the losses that we use in the training\n",
    "losses = []\n",
    "# We define the number of epochs\n",
    "epochs = 200000\n",
    "# We define the batch size\n",
    "batchSize = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Mini-Batches\n",
    "    indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(batchSize,))\n",
    "    inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "    \n",
    "    # Forward Pass (Mini-Batch)\n",
    "    for layer in layers:\n",
    "        # We call the forward pass of each layer\n",
    "        inputBatch = layer(inputBatch)\n",
    "    loss = F.cross_entropy(inputBatch, outputBatch)\n",
    "    # Backward Pass (Mini-Batch)\n",
    "    # We reset the gradients\n",
    "    for parameter in parameters:\n",
    "        parameter.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update Weights (Mini-Batch)\n",
    "    learning_rate = 0.1 if i < 150000 else 0.01\n",
    "    for parameter in parameters:\n",
    "        parameter.data += -learning_rate * parameter.grad\n",
    "\n",
    "    # Tracking the stats\n",
    "    # We append the losses that we use per iteration\n",
    "    if i % 10000 == 0: \n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # We do that to squash the steep curve to a nicer graph\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b8808-6f5d-482f-bf67-c015abc1a6af",
   "metadata": {},
   "source": [
    "Nice, so we do get an output and we can continue to *pytorchify* our code more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e2e94-10f0-49ff-a964-39aa9058a529",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Further *Pytorch-ifiying* our code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1ce6c-1b23-49a1-8a83-3a67edaae0d2",
   "metadata": {},
   "source": [
    "Right now we are maintaining our code in a naked python list of `layers`.\n",
    "\n",
    "We can also simplify this by introducing a concept of PyTorch `Containers`.\n",
    "\n",
    "We are basically building `TORCH.NN` from scratch, and inside of `TORCH.NN`, there is a concept of `Containers`.\n",
    "\n",
    "These `containers` are used for organizing layers into form of `lists` and `dictionationaries` and so on...\n",
    "\n",
    "There is a `Container` within all the containers that have been implemented in PyTorch, known as `Sequential`. This `Sequential` container maintains a list of layers and passes a given input through all the layers, sequentially.\n",
    "\n",
    "Which is the exact same thing that we have been doing in our `layers` list.\n",
    "\n",
    "So let's implement our own `Sequential` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4aab7-9240-438d-a01b-c4dea78b099e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Modular Block Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f66058ba-b1da-4377-80bb-791043972ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # Constructor to initialize the weights and biases\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # We initialize the weights using fan-in and fan-out and use the kaiming initialization which is the square root of fan-in\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        # We initilize the biases using fan-out and set them to 0 values if bias is set to True else we set it to None\n",
    "        self.biases = torch.zeros(fan_out) if bias else None\n",
    "    # Defines how we forward pass these layers which is (y = wx + b)\n",
    "    def __call__(self, inputs):\n",
    "        # (w*x)\n",
    "        self.out = inputs @ self.weights\n",
    "        # Checking if we have a bias\n",
    "        if self.biases is not None:\n",
    "            # (wx) + b\n",
    "            self.out += self.biases\n",
    "        # We return (y = wx + b)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # If bias is there return (w*x + b) else return (w*x)\n",
    "        return [self.weights] + ([] if self.biases is None else [self.biases])\n",
    "class BatchNorm1d:\n",
    "    # Constructor to initialize the gain, bias, and the buffers (running mean and running variance) using the dimensions we get\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        # We initialize the gains to 1 values of dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        # We initialize the bias to 0 values of dimensions\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        # We initialize the running mean to 0 values of dimensions\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        # We initialize the running variance to 1 values of dimensions\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        # We initialize a checker parameter that checks whether we are training or not (We will discuss this later) \n",
    "        self.training = training\n",
    "    # Forward Pass and Buffer Updation based on the inputs\n",
    "    def __call__(self, inputs):\n",
    "        # Forward Pass\n",
    "        # ------------\n",
    "        # If training is set to true we will calculate the mean and variance estimated from the current batch of inputs\n",
    "        if self.training:\n",
    "            # Initializing mean of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_mean = inputs.mean(0, keepdim=True)\n",
    "            # Initializing variance of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_variance = inputs.var(0, keepdim=True)\n",
    "        # If not training, we will use the running mean and variance of the inputs of the saved values\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        # We center the inputs around zero and scaling it to have unit variance using the defined formula\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        # After calulating the unit_variance we can perform the operation (y = wx + b) on this layer\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        # If training is set to true we will update the running mean and variance estimated from the momentum (without gradient updation)\n",
    "        if self.training:\n",
    "            # We disable gradient updation\n",
    "            with torch.no_grad():\n",
    "                # We perform (batchRunningMean = 0.999 * batchRunningMean + 0.001 * batchMeanAtIteration)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                # We perform (batchRunningStandardDeviation = 0.999 * batchRunningStandardDeviation + 0.001 * batchStandardDeviationAtIteration)\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        # We return the (y = wx + b) after all the calculations are done (if any)\n",
    "        return self.out\n",
    "        # ------------\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # We return the parameters (gamma and beta)\n",
    "        return [self.gamma, self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self, inputs):\n",
    "        # Calculates tanh based on the inputs\n",
    "        self.out = torch.tanh(inputs)\n",
    "        return self.out\n",
    "    # Returns an empty list because there are no paramters in this layer\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Embedding:\n",
    "    # Constructor to initialize the weights of the embedding\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.weights = torch.randn((num_embeddings, embedding_dim))\n",
    "    # The indexing operation in the forward pass : Similar to -> embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    def __call__(self, indexes):\n",
    "        self.out = self.weights[indexes]\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (weights)\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "class Flatten:\n",
    "    # Concatenation operation in the forward pass : Similar to -> concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "    def __call__(self, inputs):\n",
    "        self.out = inputs.view(inputs.shape[0], -1)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (empty list)\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Sequential:\n",
    "     # Constructor to initialize the layers\n",
    "    def __init__(self, layers):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.layers = layers\n",
    "    # Calling all the layers based on the inputs: Using __call__ for all the layes given all the inputs\n",
    "    def __call__(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        self.out = inputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used in all the layers in a list\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abf415-e23a-4c7f-9dba-5151447118f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Neural Network Initialization - Weights, Biases & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733f9f3-bd2a-43dd-87cc-56b0f9f8b51a",
   "metadata": {},
   "source": [
    "Now we can start simplifying our code:\n",
    "\n",
    "Previously we had:\n",
    "```python\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "layers = [\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    Flatten(),\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False), BatchNorm1d(vocabularySize),\n",
    "]\n",
    "```\n",
    "\n",
    "Now we can have:\n",
    "```python\n",
    "# Initializing the model's sequential layers into a single module called model\n",
    "model = Sequential([\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    Flatten(),\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False), BatchNorm1d(vocabularySize),\n",
    "])\n",
    "```\n",
    "\n",
    "Here we see that `layers` becomes a `model`, which is a notion of a *Module*.\n",
    "\n",
    "See how I use the word *Module* a lot?\n",
    "\n",
    "Well that's because, a **Module** is the base class for all the classes in PyTorch.\n",
    "\n",
    "Also before we had:\n",
    "```python\n",
    "# We initialize all the parameters\n",
    "parameters = [parameter for layer in layers for parameter in layer.parameters()]\n",
    "```\n",
    "\n",
    "Now, because we implemented the same within the `Sequential` class, we can have:\n",
    "```python\n",
    "# We initialize all the parameters\n",
    "parameters = model.parameters()\n",
    "```\n",
    "\n",
    "So now we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c376da6-6820-4ac1-908a-d1da321d47ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 12124\n"
     ]
    }
   ],
   "source": [
    "# We will define a manual seed to give a similar result on your machine, as of my machine\n",
    "torch.manual_seed(69420)\n",
    "embeddingFeatureSpaceLength = 10\n",
    "numberOfHiddenLayerNeurons = 200\n",
    "\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "model = Sequential([\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    Flatten(),\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False), BatchNorm1d(vocabularySize),\n",
    "])\n",
    "\n",
    "# Disabling Gradient Tracking\n",
    "with torch.no_grad():\n",
    "    # Making the final activation less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "\n",
    "# We initialize all the parameters\n",
    "parameters = model.parameters()\n",
    "# We check the number of parameters by printing its value\n",
    "print(\"Number of parameters:\", sum(parameter.nelement() for parameter in parameters))\n",
    "# We set 'requires_grad' to True in order to track gradients\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f9bfb-7c8b-4ef7-ae37-2982eb1172d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2920c182-ef9d-4178-b1df-11bc62ec80b7",
   "metadata": {},
   "source": [
    "This time, we can also improve forward pass substantially...\n",
    "\n",
    "That's because, we don't have to forward the inputs manually in a loop.\n",
    "\n",
    "Previously we had:\n",
    "```python\n",
    "# Forward Pass (Mini-Batch)\n",
    "for layer in layers:\n",
    "    # We call the forward pass of each layer\n",
    "    inputBatch = layer(inputBatch)\n",
    "loss = F.cross_entropy(inputBatch, outputBatch)\n",
    "```\n",
    "\n",
    "Now we simply call the model using the input data(`inputBatch`):\n",
    "```python\n",
    "# Forward Pass (Mini-Batch)\n",
    "logits = model(inputBatch)\n",
    "loss = F.cross_entropy(logits, outputBatch)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "595a76d3-a605-4875-a88c-d6cfc67b740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.6497\n"
     ]
    }
   ],
   "source": [
    "# We want to track the losses that we use in the training\n",
    "losses = []\n",
    "# We define the number of epochs\n",
    "epochs = 200000\n",
    "# We define the batch size\n",
    "batchSize = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Mini-Batches\n",
    "    indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(batchSize,))\n",
    "    inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "    \n",
    "    # Forward Pass (Mini-Batch)\n",
    "    logits = model(inputBatch)\n",
    "    loss = F.cross_entropy(logits, outputBatch)\n",
    "    # Backward Pass (Mini-Batch)\n",
    "    # We reset the gradients\n",
    "    for parameter in parameters:\n",
    "        parameter.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update Weights (Mini-Batch)\n",
    "    learning_rate = 0.1 if i < 150000 else 0.01\n",
    "    for parameter in parameters:\n",
    "        parameter.data += -learning_rate * parameter.grad\n",
    "\n",
    "    # Tracking the stats\n",
    "    # We append the losses that we use per iteration\n",
    "    if i % 10000 == 0: \n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # We do that to squash the steep curve to a nicer graph\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84783072-25e7-4adf-8d07-6d47963a22bd",
   "metadata": {},
   "source": [
    "#### Evaluating Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe3851-b034-4cd8-a364-10e2de1793c1",
   "metadata": {},
   "source": [
    "So, similarly like the old code:\n",
    "```python\n",
    "# Based on the split we can then index into the embedding look-up matrix using its inputs to get the embeddings\n",
    "embedding = embeddingLookUpMatrix[input]\n",
    "concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "for layer in layers:\n",
    "    concatenatedEmbedding = layer(concatenatedEmbedding)\n",
    "loss = F.cross_entropy(concatenatedEmbedding, output)\n",
    "```\n",
    "\n",
    "We can have:\n",
    "```python\n",
    "logits = model(input)\n",
    "loss = F.cross_entropy(logits, output)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c53673e-6d65-4e48-9cc3-01cc78595e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:3.6893813610076904\n",
      "Validation Loss:3.6842901706695557\n"
     ]
    }
   ],
   "source": [
    "# Decorator for disabling gradient tracking throughout the function underneath\n",
    "@torch.no_grad()\n",
    "def splitLoss(split):\n",
    "    input, output = {\n",
    "        'Training': (trainingInputs, trainingOutputs),\n",
    "        'Validation': (validationInputs, validationOutputs),\n",
    "        'Testing': (testInputs, testOutputs)\n",
    "    }[split]\n",
    "    # Based on the split we can now forward through layers to get the logits\n",
    "    logits = model(input)\n",
    "    loss = F.cross_entropy(logits, output)\n",
    "    print(f\"{split} Loss:{loss.item()}\")\n",
    "# We can then call this method to calculate and print loss\n",
    "splitLoss('Training')\n",
    "splitLoss('Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c572ac-d3e8-4e06-b504-4aac35d8b2ce",
   "metadata": {},
   "source": [
    "Don't sweat on the losses too much, this was just the first epoch and our neural network is not trained until we remove the `break` in the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd5d4f-c8cd-482e-aff5-9d1bd0c9258f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sampling from the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1af804-4387-48ef-9104-367e859ba87d",
   "metadata": {},
   "source": [
    "So previously we had:\n",
    "```python\n",
    "# We would create an output embedding that would be based on the block\n",
    "embedding = embeddingLookUpMatrix[torch.tensor([block])]\n",
    "concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "# Inside the loop where layers are applied\n",
    "for layer in layers:\n",
    "    concatenatedEmbedding = layer(concatenatedEmbedding)\n",
    "# Apply softmax\n",
    "probabilities = F.softmax(concatenatedEmbedding, dim=1)\n",
    "```\n",
    "\n",
    "Now we have:\n",
    "```python\n",
    "# Now we simply call the model on the context\n",
    "logits = model(torch.tensor([block]))\n",
    "# Apply softmax\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fafb9ff-8d8b-4e37-8edb-8fd8d5db1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvcgrvloatsayltpegyglhntbttmkotwndch.\n",
      "jpvxzmidcuvnmanf.\n",
      ".\n",
      "bnrkdfkfhvkqjhcypfpfyaqirfsewssnlvjkcgnpjnafof.\n",
      ".\n",
      "vuswvmkdkgxezoplmnkxyuowbdylnxbniapxblxbolfnzrcnyrnlw.\n",
      "dtdcwrkdg.\n",
      "o.\n",
      "lspvrnjxpzrtwqvjlgkdnktvlolscyezaarqinlsxhjivfb.\n",
      ".\n",
      "nvcxzoadxzkcscraufozhphvpabosgnddguifwlhkwle.\n",
      "rmefnadwkulfvudbaibnate.\n",
      ".\n",
      "wsbciikxsvbchwycyhamrkyvcvkjgklmcsyqdtakjufytlule.\n",
      "nbruxkzrqnoutrmgezvrcrsjhidpxhtelbmiqul.\n",
      "sssaeanffthjevnwkrjhavrbxanesphmv.\n",
      "gajcpnutzaknayuficubvnkxzcipzsaivnlcwaeknlkbsyayyurwdtqcjhszdornrwpwdfuqjnrxhxoixioowfsryrurspkmoydimfpkm.\n",
      "dbtfwjzgbepoub.\n",
      "zrvikvshesszgjk.\n",
      "bsqevdukg.\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 3\n",
    "# We will define a generator to give the same result on your machine, as of my machine\n",
    "numberOfWordsToSample = 20\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "\n",
    "# We iterate over the number of words we want to predict\n",
    "for _ in range(numberOfWordsToSample):\n",
    "    # We define a output list to append the next character and print it at the end\n",
    "    output = []\n",
    "    # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "    block = [0] * blockSize\n",
    "\n",
    "    # We will now iterate over each word's characters\n",
    "    while True:\n",
    "        # Now we simply call the model on the context\n",
    "        logits = model(torch.tensor([block]))\n",
    "        # Apply softmax\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        # We can now sample the next character\n",
    "        index = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "        block = block[1:] + [index]\n",
    "        # We then append the sampled character to the output\n",
    "        output.append(index)\n",
    "        # If we hit '.' end token, we will break out from the loop\n",
    "        if index == 0:\n",
    "            break\n",
    "    # We print generated name out\n",
    "    print(''.join(itos[index] for index in output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5847c-c1fc-4ff8-9888-be80826305f8",
   "metadata": {},
   "source": [
    "Once again, don't sweat on the outputs too much, this was just the first epoch and our neural network is not trained until we remove the `break` in the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a4105-3804-48ba-98e0-a265c1f93e1a",
   "metadata": {},
   "source": [
    "So let's re-run everything without the `break` and see where we are, and check new ways to introduce new ways to improve our code (for this notebook, `WaveNet`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c885de1-0070-4192-ac58-bcc67c813243",
   "metadata": {},
   "source": [
    "## Re-Running *PyTorch-ified* Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b94a6-7ad4-49de-81c9-3bcbbedc77e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Modular Block Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58daadae-5ece-44e6-8cb8-58179d6a7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # Constructor to initialize the weights and biases\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # We initialize the weights using fan-in and fan-out and use the kaiming initialization which is the square root of fan-in\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        # We initilize the biases using fan-out and set them to 0 values if bias is set to True else we set it to None\n",
    "        self.biases = torch.zeros(fan_out) if bias else None\n",
    "    # Defines how we forward pass these layers which is (y = wx + b)\n",
    "    def __call__(self, inputs):\n",
    "        # (w*x)\n",
    "        self.out = inputs @ self.weights\n",
    "        # Checking if we have a bias\n",
    "        if self.biases is not None:\n",
    "            # (wx) + b\n",
    "            self.out += self.biases\n",
    "        # We return (y = wx + b)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # If bias is there return (w*x + b) else return (w*x)\n",
    "        return [self.weights] + ([] if self.biases is None else [self.biases])\n",
    "class BatchNorm1d:\n",
    "    # Constructor to initialize the gain, bias, and the buffers (running mean and running variance) using the dimensions we get\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        # We initialize the gains to 1 values of dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        # We initialize the bias to 0 values of dimensions\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        # We initialize the running mean to 0 values of dimensions\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        # We initialize the running variance to 1 values of dimensions\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        # We initialize a checker parameter that checks whether we are training or not (We will discuss this later) \n",
    "        self.training = training\n",
    "    # Forward Pass and Buffer Updation based on the inputs\n",
    "    def __call__(self, inputs):\n",
    "        # Forward Pass\n",
    "        # ------------\n",
    "        # If training is set to true we will calculate the mean and variance estimated from the current batch of inputs\n",
    "        if self.training:\n",
    "            # Initializing mean of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_mean = inputs.mean(0, keepdim=True)\n",
    "            # Initializing variance of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_variance = inputs.var(0, keepdim=True)\n",
    "        # If not training, we will use the running mean and variance of the inputs of the saved values\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        # We center the inputs around zero and scaling it to have unit variance using the defined formula\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        # After calulating the unit_variance we can perform the operation (y = wx + b) on this layer\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        # If training is set to true we will update the running mean and variance estimated from the momentum (without gradient updation)\n",
    "        if self.training:\n",
    "            # We disable gradient updation\n",
    "            with torch.no_grad():\n",
    "                # We perform (batchRunningMean = 0.999 * batchRunningMean + 0.001 * batchMeanAtIteration)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                # We perform (batchRunningStandardDeviation = 0.999 * batchRunningStandardDeviation + 0.001 * batchStandardDeviationAtIteration)\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        # We return the (y = wx + b) after all the calculations are done (if any)\n",
    "        return self.out\n",
    "        # ------------\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # We return the parameters (gamma and beta)\n",
    "        return [self.gamma, self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self, inputs):\n",
    "        # Calculates tanh based on the inputs\n",
    "        self.out = torch.tanh(inputs)\n",
    "        return self.out\n",
    "    # Returns an empty list because there are no paramters in this layer\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Embedding:\n",
    "    # Constructor to initialize the weights of the embedding\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.weights = torch.randn((num_embeddings, embedding_dim))\n",
    "    # The indexing operation in the forward pass : Similar to -> embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    def __call__(self, indexes):\n",
    "        self.out = self.weights[indexes]\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (weights)\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "class Flatten:\n",
    "    # Concatenation operation in the forward pass : Similar to -> concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "    def __call__(self, inputs):\n",
    "        self.out = inputs.view(inputs.shape[0], -1)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (empty list)\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Sequential:\n",
    "     # Constructor to initialize the layers\n",
    "    def __init__(self, layers):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.layers = layers\n",
    "    # Calling all the layers based on the inputs: Using __call__ for all the layes given all the inputs\n",
    "    def __call__(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        self.out = inputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used in all the layers in a list\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814d37f-8c39-4cbf-bea6-abc1ba426b92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Neural Network Initialization - Weights, Biases & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79d55e37-058a-4258-9ce6-9e76844aab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 12124\n"
     ]
    }
   ],
   "source": [
    "# We will define a manual seed to give a similar result on your machine, as of my machine\n",
    "torch.manual_seed(69420)\n",
    "embeddingFeatureSpaceLength = 10\n",
    "numberOfHiddenLayerNeurons = 200\n",
    "\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "model = Sequential([\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    Flatten(),\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False), BatchNorm1d(vocabularySize),\n",
    "])\n",
    "\n",
    "# Disabling Gradient Tracking\n",
    "with torch.no_grad():\n",
    "    # Making the final activation less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "\n",
    "# We initialize all the parameters\n",
    "parameters = model.parameters()\n",
    "# We check the number of parameters by printing its value\n",
    "print(\"Number of parameters:\", sum(parameter.nelement() for parameter in parameters))\n",
    "# We set 'requires_grad' to True in order to track gradients\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d99fc-e908-4541-b831-d89b19f73908",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02776706-9a89-41ed-9159-3ba5cc6daa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.6497\n",
      "  10000/ 200000: 2.4653\n",
      "  20000/ 200000: 1.6796\n",
      "  30000/ 200000: 2.3646\n",
      "  40000/ 200000: 1.8944\n",
      "  50000/ 200000: 2.0996\n",
      "  60000/ 200000: 1.8235\n",
      "  70000/ 200000: 1.9176\n",
      "  80000/ 200000: 2.2351\n",
      "  90000/ 200000: 2.4220\n",
      " 100000/ 200000: 2.1087\n",
      " 110000/ 200000: 1.5772\n",
      " 120000/ 200000: 1.8115\n",
      " 130000/ 200000: 2.1501\n",
      " 140000/ 200000: 1.6242\n",
      " 150000/ 200000: 1.9227\n",
      " 160000/ 200000: 1.8169\n",
      " 170000/ 200000: 1.6318\n",
      " 180000/ 200000: 1.9186\n",
      " 190000/ 200000: 2.0971\n"
     ]
    }
   ],
   "source": [
    "# We want to track the losses that we use in the training\n",
    "losses = []\n",
    "# We define the number of epochs\n",
    "epochs = 200000\n",
    "# We define the batch size\n",
    "batchSize = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Mini-Batches\n",
    "    indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(batchSize,))\n",
    "    inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "    \n",
    "    # Forward Pass (Mini-Batch)\n",
    "    logits = model(inputBatch)\n",
    "    loss = F.cross_entropy(logits, outputBatch)\n",
    "    # Backward Pass (Mini-Batch)\n",
    "    # We reset the gradients\n",
    "    for parameter in parameters:\n",
    "        parameter.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update Weights (Mini-Batch)\n",
    "    learning_rate = 0.1 if i < 150000 else 0.01\n",
    "    for parameter in parameters:\n",
    "        parameter.data += -learning_rate * parameter.grad\n",
    "\n",
    "    # Tracking the stats\n",
    "    # We append the losses that we use per iteration\n",
    "    if i % 10000 == 0: \n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # We do that to squash the steep curve to a nicer graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613b8c6-6d47-4c02-8224-63315e08a574",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluating Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2b4b11e-09aa-4aa2-bbea-45435c9693d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:1.8957682847976685\n",
      "Validation Loss:1.9175524711608887\n"
     ]
    }
   ],
   "source": [
    "# Decorator for disabling gradient tracking throughout the function underneath\n",
    "@torch.no_grad()\n",
    "def splitLoss(split):\n",
    "    input, output = {\n",
    "        'Training': (trainingInputs, trainingOutputs),\n",
    "        'Validation': (validationInputs, validationOutputs),\n",
    "        'Testing': (testInputs, testOutputs)\n",
    "    }[split]\n",
    "    # Based on the split we can now forward through layers to get the logits\n",
    "    logits = model(input)\n",
    "    loss = F.cross_entropy(logits, output)\n",
    "    print(f\"{split} Loss:{loss.item()}\")\n",
    "# We can then call this method to calculate and print loss\n",
    "splitLoss('Training')\n",
    "splitLoss('Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35744a52-ce28-40c8-aee7-aa44c4206b4a",
   "metadata": {},
   "source": [
    "Just for a reminder, let's save the losses so that we can compare them later...\n",
    "\n",
    "For now we get:\n",
    "```python\n",
    "Training Loss:1.8957682847976685\r\n",
    "Validation Loss:1.9175524711608887\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e263760-f057-48c8-9484-4a6b6c55717d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sampling from the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0183d37a-752b-4dad-9d37-0ca08d2a85b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prasenap.\n",
      "saktithrapa.\n",
      "swar.\n",
      "pia.\n",
      "veeran.\n",
      "dhy.\n",
      "haranithusree.\n",
      "sudhandhubh.\n",
      "ridapathini.\n",
      "ummudalan.\n",
      "jaipuswanishani.\n",
      "vasun.\n",
      "swada.\n",
      "yothushan.\n",
      "dheemalaimithasshan.\n",
      "akokilan.\n",
      "tha.\n",
      "prama.\n",
      "nila.\n",
      "divanjalehy.\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 3\n",
    "# We will define a generator to give the same result on your machine, as of my machine\n",
    "numberOfWordsToSample = 20\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "\n",
    "# We iterate over the number of words we want to predict\n",
    "for _ in range(numberOfWordsToSample):\n",
    "    # We define a output list to append the next character and print it at the end\n",
    "    output = []\n",
    "    # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "    block = [0] * blockSize\n",
    "\n",
    "    # We will now iterate over each word's characters\n",
    "    while True:\n",
    "        # Now we simply call the model on the context\n",
    "        logits = model(torch.tensor([block]))\n",
    "        # Apply softmax\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        # We can now sample the next character\n",
    "        index = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "        block = block[1:] + [index]\n",
    "        # We then append the sampled character to the output\n",
    "        output.append(index)\n",
    "        # If we hit '.' end token, we will break out from the loop\n",
    "        if index == 0:\n",
    "            break\n",
    "    # We print generated name out\n",
    "    print(''.join(itos[index] for index in output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79afd38-9059-4c56-81b8-aad644533f80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# WaveNet - Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90dda5f-23b2-4c0e-a89d-b90b0025c61d",
   "metadata": {},
   "source": [
    "See how are losses, that we got from the above implementation are pretty similar to each other, which means that we are not overfitting too much on this task and we can make additional progress in our performance by scaling up everything and making bigger and deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287aab79-3ae5-4630-8a56-84a2b93e05b1",
   "metadata": {},
   "source": [
    "Now currently we are using this a version of this architecture:\n",
    "\n",
    "![A Neural Probabilistic Language Model](https://miro.medium.com/v2/resize:fit:1400/1*EqKiy4-6tuLSoPP_kub33Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ac8c3-05c6-4674-a407-d3cb485c2687",
   "metadata": {},
   "source": [
    "The problem with this architecture is that, we are crushing may too much information too quickly at the very beginning in a single hidden layer...\n",
    "\n",
    "Which means, that even if we tried to make this network deeper by introducing new hidden layers to the neural network, it would make no sense to miss out on the information that we keep missing out on the *squashed* embeddings of the characters.\n",
    "\n",
    "Instead we would want our network to look a lot more like this paper -><a href=\"https://arxiv.org/abs/1609.03499\">WaveNet: A Generative Model for Raw Audio</a>:\n",
    "![WaveNet: Architecture](ExplanationMedia/Images/Dilated_Masked_Convolutions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68598c3-c900-4c76-8c85-ab20fefeea02",
   "metadata": {},
   "source": [
    "You see how when we are trying to predict the next character in a sequence, it is a function of the *previous* characters that feed in, but these characters are not crushed into a single layer (for which we have a sandwich like our current approach), instead they are crushed slowly.\n",
    "\n",
    "So we take two characters and *fuse* them into a sort of like a **bigram** representation and we do that for all the characters **consecutively**. And then we take the bigrams and *fuse* those bigrams into sort of like a *bi-bigram*  (4 character level chuncks), and then we fuse that again, ultimately creating a *tree-like* hierarchical structure of the neural network and we fuse the information of the previous context **slowly** as we go deeper into the neural network.\n",
    "\n",
    "Now in the WaveNet's case, this is a representation of **Visualization of a stack of dilated causal convolutional layers**.\n",
    "\n",
    "Scared?\n",
    "\n",
    "Don't get scared by the name, the idea behind it is actually very simple, and is an implementation detail to make everything very fast, and we will be discussing it later. But let's just get the basic idea of how this works, which is this *progressive-fusion* of information.\n",
    "\n",
    "Which means that we want to make the neural network deeper, and at each level we want to only fuse two consecutive elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd68af9-4579-4524-b336-c51e1b715ea4",
   "metadata": {},
   "source": [
    "Now even though our already implemented network might not give us good results by making it deeper, I want to re-run the network just for the sake of education that we are not missing out on anything by **increasing the `blockSize`** to `8` and save the losses for later comparison...\n",
    "\n",
    "So let's do that now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675f6c3-4490-45a0-8d0b-f859fa0cb696",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Re-running old neural network with bigger `blockSize`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3076341-e655-4934-819a-217f2405e08d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Building Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82eb78d-14fa-4c88-876e-1c6a412998b2",
   "metadata": {},
   "source": [
    "Here I changed the code from:\n",
    "```python\n",
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 3\n",
    "```\n",
    "To this:\n",
    "```python\n",
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e2733e-d957-4d12-b59a-eab058d0cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Examples: 53982\n",
      "Training Examples: 43185\n",
      "Validation Examples: 5398\n",
      "Test Examples: 5399\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 8\n",
    "# Will build the dataset on only the words we take as input\n",
    "def buildDataset(words):\n",
    "    # We define two lists, inputs & outputs, where inputs are our blocks of the block size mentioned above and outputs are the label indexes\n",
    "    inputs , outputs = [], []\n",
    "    # We iterate over each word\n",
    "    for word in words:\n",
    "        # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "        block = [0] * blockSize\n",
    "        # We run another loop for each word's character, here word also needs the ending token '.'\n",
    "        for character in word + '.':\n",
    "            # We take out the index from our look-up table\n",
    "            index = stoi[character]\n",
    "            # We append the input with our block\n",
    "            inputs.append(block)\n",
    "            # We append the output label with out index of the character\n",
    "            outputs.append([index])\n",
    "            # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "            block = block[1:] + [index]\n",
    "    # We also convert these inputs and outputs to tensors for neural network processing\n",
    "    inputs = torch.tensor(inputs)\n",
    "    outputs = torch.flatten(torch.tensor(outputs))\n",
    "    # We return the inputs and outputs\n",
    "    return inputs, outputs\n",
    "\n",
    "# We define a manual seed to random\n",
    "random.seed(69)\n",
    "# We shuffle all the words, so that the model receives all kinds of data\n",
    "random.shuffle(words)\n",
    "# We define two number of inputs\n",
    "# We take the number of examples to 80% in the first variable\n",
    "numberOfInputs1 = int(0.8*len(words))\n",
    "# We take the number of examples to 90% in the first variable\n",
    "numberOfInputs2 = int(0.9*len(words))\n",
    "# Inputs and outputs that go till 80% of the examples\n",
    "trainingInputs, trainingOutputs = buildDataset(words[:numberOfInputs1])\n",
    "# Inputs and outputs that start at 80% of the examples and go till 90% of the examples\n",
    "validationInputs, validationOutputs = buildDataset(words[numberOfInputs1:numberOfInputs2])\n",
    "# Inputs and outputs that start at 90% of the examples\n",
    "testInputs, testOutputs = buildDataset(words[numberOfInputs2:])\n",
    "\n",
    "# We can check the numbers\n",
    "print(\"Total Examples:\",len(words)) # 100%\n",
    "print(\"Training Examples:\",len(words[:numberOfInputs1])) # 80%\n",
    "print(\"Validation Examples:\",len(words[numberOfInputs1:numberOfInputs2])) # 10%\n",
    "print(\"Test Examples:\",len(words[numberOfInputs2:])) # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8ea70-3d47-4334-8304-cd5541d66d8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Modular Block Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4dfddb-6743-47c5-abba-913cbdd2664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # Constructor to initialize the weights and biases\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # We initialize the weights using fan-in and fan-out and use the kaiming initialization which is the square root of fan-in\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        # We initilize the biases using fan-out and set them to 0 values if bias is set to True else we set it to None\n",
    "        self.biases = torch.zeros(fan_out) if bias else None\n",
    "    # Defines how we forward pass these layers which is (y = wx + b)\n",
    "    def __call__(self, inputs):\n",
    "        # (w*x)\n",
    "        self.out = inputs @ self.weights\n",
    "        # Checking if we have a bias\n",
    "        if self.biases is not None:\n",
    "            # (wx) + b\n",
    "            self.out += self.biases\n",
    "        # We return (y = wx + b)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # If bias is there return (w*x + b) else return (w*x)\n",
    "        return [self.weights] + ([] if self.biases is None else [self.biases])\n",
    "class BatchNorm1d:\n",
    "    # Constructor to initialize the gain, bias, and the buffers (running mean and running variance) using the dimensions we get\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        # We initialize the gains to 1 values of dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        # We initialize the bias to 0 values of dimensions\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        # We initialize the running mean to 0 values of dimensions\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        # We initialize the running variance to 1 values of dimensions\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        # We initialize a checker parameter that checks whether we are training or not (We will discuss this later) \n",
    "        self.training = training\n",
    "    # Forward Pass and Buffer Updation based on the inputs\n",
    "    def __call__(self, inputs):\n",
    "        # Forward Pass\n",
    "        # ------------\n",
    "        # If training is set to true we will calculate the mean and variance estimated from the current batch of inputs\n",
    "        if self.training:\n",
    "            # Initializing mean of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_mean = inputs.mean(0, keepdim=True)\n",
    "            # Initializing variance of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_variance = inputs.var(0, keepdim=True)\n",
    "        # If not training, we will use the running mean and variance of the inputs of the saved values\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        # We center the inputs around zero and scaling it to have unit variance using the defined formula\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        # After calulating the unit_variance we can perform the operation (y = wx + b) on this layer\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        # If training is set to true we will update the running mean and variance estimated from the momentum (without gradient updation)\n",
    "        if self.training:\n",
    "            # We disable gradient updation\n",
    "            with torch.no_grad():\n",
    "                # We perform (batchRunningMean = 0.999 * batchRunningMean + 0.001 * batchMeanAtIteration)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                # We perform (batchRunningStandardDeviation = 0.999 * batchRunningStandardDeviation + 0.001 * batchStandardDeviationAtIteration)\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        # We return the (y = wx + b) after all the calculations are done (if any)\n",
    "        return self.out\n",
    "        # ------------\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # We return the parameters (gamma and beta)\n",
    "        return [self.gamma, self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self, inputs):\n",
    "        # Calculates tanh based on the inputs\n",
    "        self.out = torch.tanh(inputs)\n",
    "        return self.out\n",
    "    # Returns an empty list because there are no paramters in this layer\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Embedding:\n",
    "    # Constructor to initialize the weights of the embedding\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.weights = torch.randn((num_embeddings, embedding_dim))\n",
    "    # The indexing operation in the forward pass : Similar to -> embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    def __call__(self, indexes):\n",
    "        self.out = self.weights[indexes]\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (weights)\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "class Flatten:\n",
    "    # Concatenation operation in the forward pass : Similar to -> concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "    def __call__(self, inputs):\n",
    "        self.out = inputs.view(inputs.shape[0], -1)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (empty list)\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Sequential:\n",
    "     # Constructor to initialize the layers\n",
    "    def __init__(self, layers):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.layers = layers\n",
    "    # Calling all the layers based on the inputs: Using __call__ for all the layes given all the inputs\n",
    "    def __call__(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        self.out = inputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used in all the layers in a list\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae15697-a9a4-424d-86ef-2f59031ae97d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Neural Network Initialization - Weights, Biases & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae56fd5-e7eb-41bd-b9a2-f6f6cb1002a0",
   "metadata": {},
   "source": [
    "Here I also cleaned the unnecessary `BatchNorm1d(vocabularySize)` layer, because it was not required.\n",
    "\n",
    "In order to change it I had to change the `layers[-1].gamma *= 0.1` to `model.layers[-1].weights *= 0.1` because our last layer is a linear layer and it does not have the `gamma` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e34313c-2868-4d9f-bec2-2d368af8c84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 22070\n"
     ]
    }
   ],
   "source": [
    "# We will define a manual seed to give a similar result on your machine, as of my machine\n",
    "torch.manual_seed(69420)\n",
    "embeddingFeatureSpaceLength = 10\n",
    "numberOfHiddenLayerNeurons = 200\n",
    "\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "model = Sequential([\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    Flatten(),\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False),\n",
    "])\n",
    "\n",
    "# Disabling Gradient Tracking\n",
    "with torch.no_grad():\n",
    "    # Making the final activation less confident\n",
    "    model.layers[-1].weights *= 0.1\n",
    "\n",
    "# We initialize all the parameters\n",
    "parameters = model.parameters()\n",
    "# We check the number of parameters by printing its value\n",
    "print(\"Number of parameters:\", sum(parameter.nelement() for parameter in parameters))\n",
    "# We set 'requires_grad' to True in order to track gradients\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfecfb-9f09-45e6-80a3-830fe3a84e9e",
   "metadata": {},
   "source": [
    "We also see that the number of parameters have increased from `12124` to `22070` because there is a lot more context feeding into the network now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a69b1-ce04-4123-98fe-f56e4ba2a61a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c5b86f-8f35-4e58-8c3b-0b59e7a45bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2920\n",
      "  10000/ 200000: 2.4085\n",
      "  20000/ 200000: 2.1335\n",
      "  30000/ 200000: 2.4298\n",
      "  40000/ 200000: 1.9382\n",
      "  50000/ 200000: 1.8395\n",
      "  60000/ 200000: 1.6629\n",
      "  70000/ 200000: 2.1228\n",
      "  80000/ 200000: 2.3765\n",
      "  90000/ 200000: 1.8921\n",
      " 100000/ 200000: 1.6170\n",
      " 110000/ 200000: 1.7706\n",
      " 120000/ 200000: 1.8046\n",
      " 130000/ 200000: 1.7374\n",
      " 140000/ 200000: 1.7751\n",
      " 150000/ 200000: 1.5593\n",
      " 160000/ 200000: 1.7462\n",
      " 170000/ 200000: 1.5239\n",
      " 180000/ 200000: 2.2582\n",
      " 190000/ 200000: 1.9832\n"
     ]
    }
   ],
   "source": [
    "# We want to track the losses that we use in the training\n",
    "losses = []\n",
    "# We define the number of epochs\n",
    "epochs = 200000\n",
    "# We define the batch size\n",
    "batchSize = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Mini-Batches\n",
    "    indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(batchSize,))\n",
    "    inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "    \n",
    "    # Forward Pass (Mini-Batch)\n",
    "    logits = model(inputBatch)\n",
    "    loss = F.cross_entropy(logits, outputBatch)\n",
    "    # Backward Pass (Mini-Batch)\n",
    "    # We reset the gradients\n",
    "    for parameter in parameters:\n",
    "        parameter.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update Weights (Mini-Batch)\n",
    "    learning_rate = 0.1 if i < 150000 else 0.01\n",
    "    for parameter in parameters:\n",
    "        parameter.data += -learning_rate * parameter.grad\n",
    "\n",
    "    # Tracking the stats\n",
    "    # We append the losses that we use per iteration\n",
    "    if i % 10000 == 0: \n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # We do that to squash the steep curve to a nicer graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422d954-ba39-4c13-9823-32214d56e7c8",
   "metadata": {},
   "source": [
    "#### Evaluating Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a705565-3aee-4517-b37c-943e42e89403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:1.7707616090774536\n",
      "Validation Loss:1.81971275806427\n"
     ]
    }
   ],
   "source": [
    "# Decorator for disabling gradient tracking throughout the function underneath\n",
    "@torch.no_grad()\n",
    "def splitLoss(split):\n",
    "    input, output = {\n",
    "        'Training': (trainingInputs, trainingOutputs),\n",
    "        'Validation': (validationInputs, validationOutputs),\n",
    "        'Testing': (testInputs, testOutputs)\n",
    "    }[split]\n",
    "    # Based on the split we can now forward through layers to get the logits\n",
    "    logits = model(input)\n",
    "    loss = F.cross_entropy(logits, output)\n",
    "    print(f\"{split} Loss:{loss.item()}\")\n",
    "# We can then call this method to calculate and print loss\n",
    "splitLoss('Training')\n",
    "splitLoss('Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af712240-7bb1-445c-be35-00d813995ca4",
   "metadata": {},
   "source": [
    "Original Loss:\n",
    "```python\n",
    "Training Loss:1.8957682847976685\n",
    "Validation Loss:1.9175524711608887\n",
    "```\n",
    "\n",
    "Increasing `blockSize` Loss:\n",
    "```python\n",
    "Training Loss:1.7707616090774536\r\n",
    "Validation Loss:1.81971275806427\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c1026-66df-40b5-b911-0db319b64e98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sampling from the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6ae43b4-b675-4d04-8e5c-3542d90f1a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gicharadhan.\n",
      "dupsada.\n",
      "anhiyaa.\n",
      "achen.\n",
      "nikha.\n",
      "advidhi.\n",
      "thanujlyar.\n",
      "arathayan.\n",
      "sheshika.\n",
      "jyotinika.\n",
      "kosaladhasvi.\n",
      "ayarthi.\n",
      "nathina.\n",
      "ralarajan.\n",
      "dhearana.\n",
      "thulasi.\n",
      "maruleen.\n",
      "saaranth.\n",
      "mesilenthan.\n",
      "divashni.\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 8\n",
    "# We will define a generator to give the same result on your machine, as of my machine\n",
    "numberOfWordsToSample = 20\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "\n",
    "# We iterate over the number of words we want to predict\n",
    "for _ in range(numberOfWordsToSample):\n",
    "    # We define a output list to append the next character and print it at the end\n",
    "    output = []\n",
    "    # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "    block = [0] * blockSize\n",
    "\n",
    "    # We will now iterate over each word's characters\n",
    "    while True:\n",
    "        # Now we simply call the model on the context\n",
    "        logits = model(torch.tensor([block]))\n",
    "        # Apply softmax\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        # We can now sample the next character\n",
    "        index = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "        block = block[1:] + [index]\n",
    "        # We then append the sampled character to the output\n",
    "        output.append(index)\n",
    "        # If we hit '.' end token, we will break out from the loop\n",
    "        if index == 0:\n",
    "            break\n",
    "    # We print generated name out\n",
    "    print(''.join(itos[index] for index in output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784c9a6-0f5b-4483-a12a-192a34454fde",
   "metadata": {},
   "source": [
    "Seems like we are making progress just by increasing the `blockSize`, but now that we understand that this is still not helping us, let's implement the **WaveNet**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21711e9-92a6-4e28-9cc7-5f622b163c3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Implementing **WaveNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde0dd6-4873-4d7d-be78-6ad3e0b1846a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Idea behind implementing **WaveNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34059dc-ee8b-4a24-9eae-f0c565fa6295",
   "metadata": {},
   "source": [
    "Let's take an example and try to work out how we want our **WaveNet** neural network to behave during the forward pass...\n",
    "\n",
    "If we take `4` random indeces from our entire `trainingInputs` dataset, we would generally pick out the indices like:\n",
    "```python\n",
    "indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(4,))\n",
    "```\n",
    "The code generates a 1-dimensional tensor (`indexes`), where, `low=0` specifies the lowest integer to be generated (inclusive), `high=trainingInputs.shape[0]` specifies the highest integer to be generated (inclusive), `size=(4,)` specifies the size of the output tensor to be generated, which in this case, it's a `1-dimensional tensor` with `4` elements.\n",
    "\n",
    "So if we look at the `indexes` now, we get:\n",
    "```python\n",
    "tensor([185709, 297874,  24823,  11069])\n",
    "```\n",
    "We can now use these `indexes` to get our `examples` for input using:\n",
    "```python\n",
    "inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "```\n",
    "Where `inputBatch` and `outputBatch` look like:\n",
    "```python\n",
    "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 0,  1,  4, 18,  9, 19,  8, 25],\n",
    "        [ 0,  0,  0,  0, 19,  1,  4,  8]])\n",
    "tensor([19,  8,  1, 21])       \n",
    "```\n",
    "We understand that we can simply now use our already implemented forward pass to forward the model using:\n",
    "```python\n",
    "logits = model(inputBatch)\n",
    "```\n",
    "If we check the shape of our `inputBatch` we get:\n",
    "```python\n",
    "torch.Size([4, 8])\n",
    "```\n",
    "And if we check the shapes of the output of each layer:\n",
    "```python\n",
    "model.layers[0].out.shape # Embedding Layer\n",
    "model.layers[1].out.shape # Flatten Layer\n",
    "model.layers[2].out.shape # Linear Layer\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "torch.Size([4, 8, 10])\n",
    "torch.Size([4, 80])\n",
    "torch.Size([4, 200])\n",
    "```\n",
    "\n",
    "Getting confused?\n",
    "\n",
    "I got you..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee97f3f-81f4-41c5-a5c2-83f4e772fe37",
   "metadata": {},
   "source": [
    "Let's look at it in an illustrative way:\n",
    "\n",
    "![Before Wave Net](ExplanationMedia/Images/BeforeWaveNetNetwork.png)\n",
    "> If the image is way too small to look at, the images are stored in the ExplanationMedia>Images folder for this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d270537-aee2-47e5-aa07-12f58a81e553",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Understanding Linear Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37249c4b-78fc-4ed3-9e06-cea232018295",
   "metadata": {},
   "source": [
    "Let's understand how the linear transformation inside our `Linear Layer` happens for our small example...\n",
    "\n",
    "If we look back at our modular block for `__call__()` for our `Linear` module, we have:\n",
    "```python\n",
    "# Defines how we forward pass these layers which is (y = wx + b)\n",
    "def __call__(self, inputs):\n",
    "    # (w*x)\n",
    "    self.out = inputs @ self.weights\n",
    "    # Checking if we have a bias\n",
    "    if self.biases is not None:\n",
    "        # (wx) + b\n",
    "        self.out += self.biases\n",
    "    # We return (y = wx + b)\n",
    "    return self.out\n",
    "```\n",
    "\n",
    "It looks something like this:\n",
    "```python\n",
    "torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\underbrace{\\underbrace{\\text{torch.randn(4, 80)} @ \\text{torch.randn(80, 200)}}_{\\text{[4, 200]}} + \\text{torch.randn(200)}}_{\\text{[4, 200]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e059640-f952-474e-8c4f-999028b0cab6",
   "metadata": {},
   "source": [
    "Now let me show you something pretty interesting...\n",
    "\n",
    "This same **multiplication** doesn't have to be 2-dimensional. The matrix multiplication operator in PyTorch ($@$) is quite powerful. In face, we can actually pass higer dimensional tensors and everything works just fine.\n",
    "\n",
    "For example, the same line could be used to pass higher dimensional tensors:\n",
    "```python\n",
    "(torch.randn(4, 5, 80) @ torch.randn(80, 200) + torch.randn(200)).shape\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "torch.Size([4, 5, 200])\n",
    "```\n",
    "And again\n",
    "```python\n",
    "(torch.randn(4, 5, 2, 80) @ torch.randn(80, 200) + torch.randn(200)).shape\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "torch.Size([4, 5, 2, 200])\n",
    "```\n",
    "And suddenly we see the pattern that, the matrix multiplication operator ($@$) only works on the last dimension, which in our case is `80` and the dimensions specified before it are left unchanged and treated as batch dimensions. In other words, we can have multiple batch dimensions and in parallel over all those dimensions, we end up doing the matrix multiplication over the last dimension.\n",
    "\n",
    "Suddenly this becomes very convenient because, we can now use that in our neural network now...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441948dc-9b4c-494e-82cd-b0605cd6f74c",
   "metadata": {},
   "source": [
    "Remember how we had our block of `8` characters coming in as an input for a single example?\n",
    "```python\n",
    "A B C D E F G H\n",
    "```\n",
    "It gets flattened out into a large vector. Instead we want to group these:\n",
    "```python\n",
    "(A B) (C D) (E F) (G H)\n",
    "```\n",
    "Which effectively means, all of these characters need to be flattened out in **parallel** and then multiplied by a weight matrix. Which leads us to introduce a batch dimension.\n",
    "\n",
    "So, we want to process all of these **bigram groups** in the `4` batch dimensions of an individual example, and also over the actual batch of all the examples...\n",
    "\n",
    "So let's see how that works..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fea05a-7898-4ee4-af6e-8ef06bd2bf3a",
   "metadata": {},
   "source": [
    "Right now what happens in our example is:\n",
    "```python\n",
    "torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)\n",
    "```\n",
    "\n",
    "$$\n",
    "\\underbrace{\\text{torch.randn(4, 80)}}_{\\text{Input Examples}} \\text{ @ } \\underbrace{\\text{torch.randn(80, 200)}}_{\\text{Weights}} + \\text{torch.randn(200)}\n",
    "$$\n",
    "\n",
    "1. We don't want a `(4, 80)` tensor now. We rather want a tensor which separates flattened out embeddings into groups for `4` bigrams now. Which means we want something like a `(4, 4, 20)`. (`4` examples of `4` bigram groups, each one having a `10` dimensional embedding)\n",
    "2. Now because we discussed that the matrix multiplication only works on the last dimension, so instead of having a weights matrix of `(80, 200)` we will now have a weights matrix of `(20, 200)`\n",
    "\n",
    "Which effectively tells us, instead of having a line like this during the linear transformation:\n",
    "```python\n",
    "torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)\n",
    "```\n",
    "We would have an expression of something like this:\n",
    "```python\n",
    "torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)\n",
    "```\n",
    "\n",
    "Which tells us to change the `Flatten Layer` such that it outputs a `(4, 4, 20)` instead of a `(4, 80)` tensor (Similar to a concatenation operation)...\n",
    "\n",
    "So let's do that..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55fe3f-3c87-4c8c-a151-a6c161203b72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Understanding Flatten Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49428e5-d6ed-47a8-9832-0eebc69d3220",
   "metadata": {},
   "source": [
    "Let's understand what happens to the inputs before and after the `Flatten Layer`.\n",
    "\n",
    "If we look back at our modular block for `__call__()` for our `Flatten` module, we have:\n",
    "```python\n",
    "# Concatenation operation in the forward pass : Similar to -> concatenatedEmbedding = embedding.view(embedding.shape[0], -1)\n",
    "def __call__(self, inputs):\n",
    "    self.out = inputs.view(inputs.shape[0], -1)\n",
    "    return self.out\n",
    "```\n",
    "It looks something like this:\n",
    "```python\n",
    "inputsBeforeFlatten = torch.randn([4, 8, 10])\n",
    "inputsAfterFlatten = inputsBeforeFlatten.view(inputsBeforeFlatten.shape[0], -1)\n",
    "```\n",
    "$$\n",
    "\\underbrace{\\text{inputsAfterFlatten}}_{\\text{[4, 80]}} = \\underbrace{\\text{inputsBeforeFlatten}}_{\\text{[4, 8, 10]}}\\text{.view(inputsBeforeFlatten.shape[0], -1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10157f46-b2f8-4834-8813-cfe380e0a952",
   "metadata": {},
   "source": [
    "Now let me show you something interesting once again...\n",
    "\n",
    "Let's take for example `10` numbers like this:\n",
    "```python\n",
    "torch.arange(10)\n",
    "```\n",
    "So we get something like this:\n",
    "```python\n",
    "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "```\n",
    "If you remember *slicing* in python correctly you'd probably remember that we can slice the even parts in the steps of `2` like this:\n",
    "```python\n",
    "torch.arange(10)[::2]\n",
    "```\n",
    "To get:\n",
    "```python\n",
    "tensor([0, 2, 4, 6, 8])\n",
    "```\n",
    "And we get all the odd parts in the steps of `2` starting at `1`, like this:\n",
    "```python\n",
    "torch.arange(10)[1::2]\n",
    "```\n",
    "To get:\n",
    "```python\n",
    "tensor([1, 3, 5, 7, 9])\n",
    "```\n",
    "Remembering this, we can now use this in our logic..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a39d88-2792-440d-a38b-fdca7260c20a",
   "metadata": {},
   "source": [
    "Remember how we want our `Flatten Layer` to output a `(4, 4, 20)` tensor instead of a `(4, 80)` tensor when it gets an input of `(4, 8, 10)` tensor?\n",
    "\n",
    "In other words,\\\n",
    "The inputs for this *toy* example are `4` examples of `8` characters in a block, each one having a `10` dimensional embedding. \\\n",
    "Which instead of having each of these `4` examples stretching their corresponding `10` dimensional embeddings without any bigram groups into a single row, \\\n",
    "we now want an output tensor with `4` examples of `4` bigram groups, each one having a `20` dimensional embedding.\n",
    "\n",
    "\n",
    "So what better way is it to use our old *odd and even* slicing trick to pick out the `odd` characters in a block and `even` characters in a block like this:\n",
    "```python\n",
    "inputsBeforeFlatten[:, ::2, :], inputsBeforeFlatten[:, 1::2, :] \n",
    "```\n",
    "$$\n",
    "\\underbrace{inputsBeforeFlatten[:, ::2, :]}_{Even Parts}, \\underbrace{inputsBeforeFlatten[:, 1::2, :]}_{Odd Parts}\n",
    "$$\n",
    "Which essentially does something like this for each example:\n",
    "```python\n",
    "example = ['A', 'I', 'N', 'D', 'R', 'I', 'L', 'A']\n",
    "\n",
    "oddParts = ['I', 'D', 'I', 'A']\n",
    "evenParts = ['A', 'N', 'R', 'L']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f94378-6a72-4caa-a9e9-e15b0dccc772",
   "metadata": {},
   "source": [
    "We now understand that each of these characters have their own `10` dimensional embeddings.\n",
    "\n",
    "And we can now use these odd parts and even parts to separate out the characters and use them as groups of two in parallel by using a concatenation operation along the dimension of the embeddings (which is `2`), like this:\n",
    "```python\n",
    "outputsAfterFlatten = torch.cat((inputsBeforeFlatten[:, ::2, :], inputsBeforeFlatten[:, 1::2, :]), dim=2)\n",
    "```\n",
    "Which gives us an output like this:\n",
    "```python\n",
    "# Resulting tensor\n",
    "tensor([[[*, *, *, *, *, *, *, *, *, *],  # 'A' embedding (evem)\n",
    "         [*, *, *, *, *, *, *, *, *, *],  # 'N' embedding (even)\n",
    "         [*, *, *, *, *, *, *, *, *, *],  # 'R' embedding (even)\n",
    "         [*, *, *, *, *, *, *, *, *, *],  # 'L' embedding (even)\n",
    "         [*, *, *, *, *, *, *, *, *, *],  # 'I' embedding (odd)\n",
    "         [*, *, *, *, *, *, *, *, *, *],  # 'D' embedding (odd)\n",
    "         [*, *, *, *, *, *, *, *, *, *],  # 'I' embedding (odd)\n",
    "         [*, *, *, *, *, *, *, *, *, *]], # 'A' embedding (odd)\n",
    "        [[...]], # Example 2\n",
    "        [[...]], # Example 3\n",
    "        [[...]]  # Example 4\n",
    "       ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284adc4-af29-48af-b644-77ba887e1a07",
   "metadata": {},
   "source": [
    "Remember, this may not look like the ideal thing like the diagram we saw earlier for **WaveNet**, but we want something to act like a bigram group for all the examples in parallel. And this is the kind of output that we want.\n",
    "\n",
    "But it also turns out that we can do the exactly same thing with `view()` from PyTorch and get the same shape we have now if we request the shape to be from `(4, 8, 10)` to `(4, 4, 20)`, like this:\n",
    "```python\n",
    "inputsBeforeFlatten = torch.randn([4, 8, 10]) # (4 examples of 8 characters in a block, each one having a 10 dimensional embedding)\n",
    "outputsAfterFlatten = inputsBeforeFlatten.view(4, 4, 20) # (4 examples of 4 bigram groups, each group having a concatenated 20 dimensional embedding)\n",
    "```\n",
    "So now we can check `inputsAfterFlatten == outputsAfterFlatten` we want to check if they are the same, **element wise**, otherwise we can check using \n",
    "`torch.all(input)  Tensor` (which tests if all elements in `input` evaluate to `True`), like this:\n",
    "```python\n",
    "(inputsAfterFlatten == outputsAfterFlatten).all()\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "tensor(True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7b0cd-c8e4-40b6-8b14-a201240b569e",
   "metadata": {},
   "source": [
    "So long story short, we can simple use `view()` from PyTorch to change the tensor to whatever way we want it to be...\n",
    "\n",
    "For now we want out initialize a `groups` inside `Flatten`. But the moment we do this we start to depart from the original implementation of `TORCH.FLATTEN` from PyTorch. So it's better to use a different name for the module like `ConsecutiveFlatten` that flattens only the *consecutive* elements based on `groups`.\n",
    "\n",
    "Here's the original `Flatten` from PyTorch as a reminder:\n",
    "```python\n",
    "torch.flatten(input, start_dim=0, end_dim=-1)  Tensor\n",
    "```\n",
    "**Parameters**:\n",
    "- **input** (`Tensor`)  the input tensor.\n",
    "- **start_dim** (`int`)  the first dim to flatten\n",
    "- **end_dim** (`int`)  the last dim to flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87356c-515e-4e73-86e2-d72d6bf01b22",
   "metadata": {},
   "source": [
    "Now, we want our `ConsecutiveFlatten` to use these `groups` parameter to calculate a lot of things.\n",
    "\n",
    "Remember how for our *toy* example, the inputs of `(4, 8, 10)` become outputs of `(4, 4, 20)`?\n",
    "\n",
    "We can now use these `groups` to calculate the other dimensions as well for the outputs like this:\\\n",
    "Assuming that we need groups of `2` we can now determine:\n",
    "$$\n",
    "\\displaylines{\n",
    "\\text{If, groups} = 2 \\\\\n",
    "(4, 4, 20) \\implies (4, \\frac{8}{groups}, 10 * groups) \\\\\n",
    "(4, 4, 20) \\implies (4, -1, 10 * groups)\n",
    "}\n",
    "$$\n",
    "Here `-1` is to automatically determine the bigram groups of outputs. But I like to see error messages, and rather don't like the use of `-1`, so I'd rather leave the implementation to $\\frac{8}{groups}$...\n",
    "\n",
    "But once again $\\frac{8}{groups}$ can create problems in further layers...\n",
    "\n",
    "How so? Let's take an example:\n",
    "\n",
    "Suppose we have an input of say size `(2, 2, 4)` and we specify the groups to be `2`, then:\n",
    "$$\n",
    "\\displaylines{\n",
    "\\text{If, groups} = 2 \\\\\n",
    "(2, \\frac{2}{groups}, 4 * groups) \\implies (2, 1, 8)\n",
    "}\n",
    "$$\n",
    "See how we can get a *spurious* dimension `1` as an output? Sending this as an input to the next layer can create performance problems to further calculate and index into the tensor. So we need to have a `torch.squeeze()` to squeeze out dimensions with `1`, but once again we want to be as specific as possible, so we would specify the exact dimension that is creating the problem in the first place, which is `1`...\n",
    "\n",
    "So we can now move ahead and modify our `Flatten` to make it `ConsecutiveFlatten` now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73381b-b68a-4964-b79b-80dad421ca19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Editing Modular Block `Flatten` to make it `ConsecutiveFlatten`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6e248b-0547-4b9e-a226-76a966d7e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # Constructor to initialize the weights and biases\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # We initialize the weights using fan-in and fan-out and use the kaiming initialization which is the square root of fan-in\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        # We initilize the biases using fan-out and set them to 0 values if bias is set to True else we set it to None\n",
    "        self.biases = torch.zeros(fan_out) if bias else None\n",
    "    # Defines how we forward pass these layers which is (y = wx + b)\n",
    "    def __call__(self, inputs):\n",
    "        # (w*x)\n",
    "        self.out = inputs @ self.weights\n",
    "        # Checking if we have a bias\n",
    "        if self.biases is not None:\n",
    "            # (wx) + b\n",
    "            self.out += self.biases\n",
    "        # We return (y = wx + b)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # If bias is there return (w*x + b) else return (w*x)\n",
    "        return [self.weights] + ([] if self.biases is None else [self.biases])\n",
    "class BatchNorm1d:\n",
    "    # Constructor to initialize the gain, bias, and the buffers (running mean and running variance) using the dimensions we get\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        # We initialize the gains to 1 values of dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        # We initialize the bias to 0 values of dimensions\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        # We initialize the running mean to 0 values of dimensions\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        # We initialize the running variance to 1 values of dimensions\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        # We initialize a checker parameter that checks whether we are training or not (We will discuss this later) \n",
    "        self.training = training\n",
    "    # Forward Pass and Buffer Updation based on the inputs\n",
    "    def __call__(self, inputs):\n",
    "        # Forward Pass\n",
    "        # ------------\n",
    "        # If training is set to true we will calculate the mean and variance estimated from the current batch of inputs\n",
    "        if self.training:\n",
    "            # Initializing mean of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_mean = inputs.mean(0, keepdim=True)\n",
    "            # Initializing variance of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_variance = inputs.var(0, keepdim=True)\n",
    "        # If not training, we will use the running mean and variance of the inputs of the saved values\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        # We center the inputs around zero and scaling it to have unit variance using the defined formula\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        # After calulating the unit_variance we can perform the operation (y = wx + b) on this layer\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        # If training is set to true we will update the running mean and variance estimated from the momentum (without gradient updation)\n",
    "        if self.training:\n",
    "            # We disable gradient updation\n",
    "            with torch.no_grad():\n",
    "                # We perform (batchRunningMean = 0.999 * batchRunningMean + 0.001 * batchMeanAtIteration)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                # We perform (batchRunningStandardDeviation = 0.999 * batchRunningStandardDeviation + 0.001 * batchStandardDeviationAtIteration)\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        # We return the (y = wx + b) after all the calculations are done (if any)\n",
    "        return self.out\n",
    "        # ------------\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # We return the parameters (gamma and beta)\n",
    "        return [self.gamma, self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self, inputs):\n",
    "        # Calculates tanh based on the inputs\n",
    "        self.out = torch.tanh(inputs)\n",
    "        return self.out\n",
    "    # Returns an empty list because there are no paramters in this layer\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Embedding:\n",
    "    # Constructor to initialize the weights of the embedding\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.weights = torch.randn((num_embeddings, embedding_dim))\n",
    "    # The indexing operation in the forward pass : Similar to -> embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    def __call__(self, indexes):\n",
    "        self.out = self.weights[indexes]\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (weights)\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "class ConsecutiveFlatten:\n",
    "     # Constructor to initialize the number of groups\n",
    "    def __init__(self, groups):\n",
    "        # Initializing the groups \n",
    "        self.groups = groups\n",
    "    # Concatenation operation in the forward pass : Flatten in groups\n",
    "    def __call__(self, inputs):\n",
    "        # Takes out each dimension from the inputs\n",
    "        batch, charactersInBlock, embeddings = inputs.shape\n",
    "        # Views as the desired group size\n",
    "        outputs = inputs.view(batch, charactersInBlock//self.groups, embeddings*self.groups)\n",
    "        # Checks if dimension 1 creates a spurious dimension\n",
    "        if outputs.shape[1] == 1:\n",
    "            # Squeezes out the dimension 1 to keep it from creating a spurious dimension\n",
    "            outputs = outputs.squeeze(dim=1)\n",
    "        self.out = outputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (empty list)\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Sequential:\n",
    "     # Constructor to initialize the layers\n",
    "    def __init__(self, layers):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.layers = layers\n",
    "    # Calling all the layers based on the inputs: Using __call__ for all the layes given all the inputs\n",
    "    def __call__(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        self.out = inputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used in all the layers in a list\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b1b82-2baf-48d7-ade6-6eede65a2e63",
   "metadata": {},
   "source": [
    "We can now test it out on our *toy* example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd491b57-ac9f-4063-9ff9-5a7435f7ae03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Testing `ConsecutiveFlatten` on our *toy-example*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ddd782-f14d-419c-97bb-c7f43410cae0",
   "metadata": {},
   "source": [
    "Remember, for now we will keep the `groups` to current `blockSize`, because we understand that taking the groups of the maximum number the group can have divides the groups to individual elements, ultimately recalling the previous behaviour for our case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b51641e6-114c-46a2-9bb5-23de6d1f5a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 22070\n"
     ]
    }
   ],
   "source": [
    "# We will define a manual seed to give a similar result on your machine, as of my machine\n",
    "torch.manual_seed(69420)\n",
    "embeddingFeatureSpaceLength = 10\n",
    "numberOfHiddenLayerNeurons = 200\n",
    "\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "model = Sequential([\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    ConsecutiveFlatten(groups=blockSize),\n",
    "    Linear(embeddingFeatureSpaceLength * blockSize, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(             numberOfHiddenLayerNeurons,             vocabularySize, bias=False),\n",
    "])\n",
    "\n",
    "# Disabling Gradient Tracking\n",
    "with torch.no_grad():\n",
    "    # Making the final activation less confident\n",
    "    model.layers[-1].weights *= 0.1\n",
    "\n",
    "# We initialize all the parameters\n",
    "parameters = model.parameters()\n",
    "# We check the number of parameters by printing its value\n",
    "print(\"Number of parameters:\", sum(parameter.nelement() for parameter in parameters))\n",
    "# We set 'requires_grad' to True in order to track gradients\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364759a6-c8e0-4b8b-813f-76398853adf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0, 11],\n",
       "        [ 0,  0,  0,  0,  0,  7, 15,  7],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0, 16]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(4,))\n",
    "inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "logits = model(inputBatch)\n",
    "print(inputBatch.shape)\n",
    "inputBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6edf70b-d077-4d92-9bf5-ce890033e8bb",
   "metadata": {},
   "source": [
    "I have also added a small code snippet to check the shapes of the outputs after each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f04549ff-5faf-4634-9129-46c596f9ed8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : torch.Size([4, 8, 10])\n",
      "ConsecutiveFlatten : torch.Size([4, 80])\n",
      "Linear : torch.Size([4, 200])\n",
      "BatchNorm1d : torch.Size([4, 200])\n",
      "Tanh : torch.Size([4, 200])\n",
      "Linear : torch.Size([4, 27])\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, \":\", layer.out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1bd8e1-9d7a-4c78-89a4-7385698707e5",
   "metadata": {},
   "source": [
    "We see that the shapes work out fine after every single layer and its output...\n",
    "\n",
    "Now, let's try to restructure the layer initialization for the `model` and instead of keeping `groups` to current `blockSize`, let's keep it in groups of `2` such that we can have the hierarchy we expect from the very first intention of the **WaveNet**...\n",
    "\n",
    "But now we have to also be careful because, simply specifying groups of `ConsecutiveFlatten` might change the inputs of `Linear` as well, because right now we if we feed in a `4, 4, 20` output of `ConsecutiveFlatten` to a `Linear` layer that expects the inputs of size `embeddingFeatureSpaceLength * blockSize` which evaluates to `80`, its not going to work out...\n",
    "\n",
    "Instead, we need to modify these `fan-in`s of the `Linear` layers to be `embeddingFeatureSpaceLength * groups` which evaluates to a concatenated `20` dimensional embedding as an input for the first layer, and the later `Linear` layers to be `numberOfHiddenLayerNeurons * groups` to do the similar operation...\n",
    "\n",
    "After we have these implemented, we can now chain these entire layers just fine...\n",
    "\n",
    "So let's do this now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ba63706-88c4-4d32-9648-0c2a7417a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 170870\n"
     ]
    }
   ],
   "source": [
    "# We will define a manual seed to give a similar result on your machine, as of my machine\n",
    "torch.manual_seed(69420)\n",
    "embeddingFeatureSpaceLength = 10\n",
    "numberOfHiddenLayerNeurons = 200\n",
    "groups = 2\n",
    "\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "model = Sequential([\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    # Bigram Flatten\n",
    "    ConsecutiveFlatten(groups=groups), Linear(embeddingFeatureSpaceLength * groups, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    # Bi-bigram Flatten\n",
    "    ConsecutiveFlatten(groups=groups), Linear(numberOfHiddenLayerNeurons * groups, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    # Bi-Bi-bigram Flatten\n",
    "    ConsecutiveFlatten(groups=groups), Linear(numberOfHiddenLayerNeurons * groups, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(numberOfHiddenLayerNeurons, vocabularySize, bias=False),\n",
    "])\n",
    "\n",
    "# Disabling Gradient Tracking\n",
    "with torch.no_grad():\n",
    "    # Making the final activation less confident\n",
    "    model.layers[-1].weights *= 0.1\n",
    "\n",
    "# We initialize all the parameters\n",
    "parameters = model.parameters()\n",
    "# We check the number of parameters by printing its value\n",
    "print(\"Number of parameters:\", sum(parameter.nelement() for parameter in parameters))\n",
    "# We set 'requires_grad' to True in order to track gradients\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2e738-be2b-4fdb-b172-2fc39f7c5388",
   "metadata": {},
   "source": [
    "We suddenly see that we have a much bigger `model`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd7f87a2-66b4-4108-9013-b7cf4350f749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  1],\n",
       "        [ 0,  0,  0, 10,  1, 14,  9, 12],\n",
       "        [ 0,  0, 13,  1,  1, 14,  9, 19],\n",
       "        [ 0,  0, 10,  1, 14,  5, 20,  8]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(4,))\n",
    "inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "logits = model(inputBatch)\n",
    "print(inputBatch.shape)\n",
    "inputBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a03fda3f-ec26-4144-9eda-11700a108d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : torch.Size([4, 8, 10])\n",
      "ConsecutiveFlatten : torch.Size([4, 4, 20])\n",
      "Linear : torch.Size([4, 4, 200])\n",
      "BatchNorm1d : torch.Size([4, 4, 200])\n",
      "Tanh : torch.Size([4, 4, 200])\n",
      "ConsecutiveFlatten : torch.Size([4, 2, 400])\n",
      "Linear : torch.Size([4, 2, 200])\n",
      "BatchNorm1d : torch.Size([4, 2, 200])\n",
      "Tanh : torch.Size([4, 2, 200])\n",
      "ConsecutiveFlatten : torch.Size([4, 400])\n",
      "Linear : torch.Size([4, 200])\n",
      "BatchNorm1d : torch.Size([4, 200])\n",
      "Tanh : torch.Size([4, 200])\n",
      "Linear : torch.Size([4, 27])\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, \":\", layer.out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d250db5-1f07-4a62-a62b-04afcf2e8d1e",
   "metadata": {},
   "source": [
    "Looking at this, when we verify the shapes, we see that all the shapes work out fine, but now we also see that `BatchNorm1d` runs out of the box without creating any errors.\n",
    "\n",
    "We still need to verify `BatchNorm1d`, and if it works like the way it should...\n",
    "\n",
    "But before we do that, let's run our neural network once to evaluate the losses once more, just to keep score, and then we can keep fixing bugs for the `BatchNorm1d`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950d49a-3e09-4ce5-a444-626339b49046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Running **WaveNet** for implemented `ConsecutiveFlatten`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc361eda-b49d-4d38-a212-4256ca56c9ca",
   "metadata": {},
   "source": [
    "Before we run this, I want to specify that I will be changing the `numberOfHiddenLayerNeurons` from `200` to `68` in order to make the architecture parameters to be the same as our old neural network, so that we have the same amount of capacity as the neural network to check if we are properly utilizing these parameters in a more efficient architecture as a comparison.\n",
    "\n",
    "And I have also increased the batchSize from `32` to `128`, such that it does not lead to numerical instability like we discussed in our previous notebooks..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3036b9-d169-4774-85a1-98bc51feb502",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4c842e-b3b4-4476-9cd5-7b71baffcc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Examples: 53982\n",
      "Training Examples: 43185\n",
      "Validation Examples: 5398\n",
      "Test Examples: 5399\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 8\n",
    "# Will build the dataset on only the words we take as input\n",
    "def buildDataset(words):\n",
    "    # We define two lists, inputs & outputs, where inputs are our blocks of the block size mentioned above and outputs are the label indexes\n",
    "    inputs , outputs = [], []\n",
    "    # We iterate over each word\n",
    "    for word in words:\n",
    "        # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "        block = [0] * blockSize\n",
    "        # We run another loop for each word's character, here word also needs the ending token '.'\n",
    "        for character in word + '.':\n",
    "            # We take out the index from our look-up table\n",
    "            index = stoi[character]\n",
    "            # We append the input with our block\n",
    "            inputs.append(block)\n",
    "            # We append the output label with out index of the character\n",
    "            outputs.append([index])\n",
    "            # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "            block = block[1:] + [index]\n",
    "    # We also convert these inputs and outputs to tensors for neural network processing\n",
    "    inputs = torch.tensor(inputs)\n",
    "    outputs = torch.flatten(torch.tensor(outputs))\n",
    "    # We return the inputs and outputs\n",
    "    return inputs, outputs\n",
    "\n",
    "# We define a manual seed to random\n",
    "random.seed(69)\n",
    "# We shuffle all the words, so that the model receives all kinds of data\n",
    "random.shuffle(words)\n",
    "# We define two number of inputs\n",
    "# We take the number of examples to 80% in the first variable\n",
    "numberOfInputs1 = int(0.8*len(words))\n",
    "# We take the number of examples to 90% in the first variable\n",
    "numberOfInputs2 = int(0.9*len(words))\n",
    "# Inputs and outputs that go till 80% of the examples\n",
    "trainingInputs, trainingOutputs = buildDataset(words[:numberOfInputs1])\n",
    "# Inputs and outputs that start at 80% of the examples and go till 90% of the examples\n",
    "validationInputs, validationOutputs = buildDataset(words[numberOfInputs1:numberOfInputs2])\n",
    "# Inputs and outputs that start at 90% of the examples\n",
    "testInputs, testOutputs = buildDataset(words[numberOfInputs2:])\n",
    "\n",
    "# We can check the numbers\n",
    "print(\"Total Examples:\",len(words)) # 100%\n",
    "print(\"Training Examples:\",len(words[:numberOfInputs1])) # 80%\n",
    "print(\"Validation Examples:\",len(words[numberOfInputs1:numberOfInputs2])) # 10%\n",
    "print(\"Test Examples:\",len(words[numberOfInputs2:])) # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab61b0-d5cd-4426-bd11-10b6d06f8db3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Modular Block Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75839035-3862-4e7f-9615-2617b68244e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # Constructor to initialize the weights and biases\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # We initialize the weights using fan-in and fan-out and use the kaiming initialization which is the square root of fan-in\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        # We initilize the biases using fan-out and set them to 0 values if bias is set to True else we set it to None\n",
    "        self.biases = torch.zeros(fan_out) if bias else None\n",
    "    # Defines how we forward pass these layers which is (y = wx + b)\n",
    "    def __call__(self, inputs):\n",
    "        # (w*x)\n",
    "        self.out = inputs @ self.weights\n",
    "        # Checking if we have a bias\n",
    "        if self.biases is not None:\n",
    "            # (wx) + b\n",
    "            self.out += self.biases\n",
    "        # We return (y = wx + b)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # If bias is there return (w*x + b) else return (w*x)\n",
    "        return [self.weights] + ([] if self.biases is None else [self.biases])\n",
    "class BatchNorm1d:\n",
    "    # Constructor to initialize the gain, bias, and the buffers (running mean and running variance) using the dimensions we get\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        # We initialize the gains to 1 values of dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        # We initialize the bias to 0 values of dimensions\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        # We initialize the running mean to 0 values of dimensions\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        # We initialize the running variance to 1 values of dimensions\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        # We initialize a checker parameter that checks whether we are training or not (We will discuss this later) \n",
    "        self.training = training\n",
    "    # Forward Pass and Buffer Updation based on the inputs\n",
    "    def __call__(self, inputs):\n",
    "        # Forward Pass\n",
    "        # ------------\n",
    "        # If training is set to true we will calculate the mean and variance estimated from the current batch of inputs\n",
    "        if self.training:\n",
    "            # Initializing mean of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_mean = inputs.mean(0, keepdim=True)\n",
    "            # Initializing variance of current batch of inputs along dimension 0 and keeping the dimensions\n",
    "            input_variance = inputs.var(0, keepdim=True)\n",
    "        # If not training, we will use the running mean and variance of the inputs of the saved values\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        # We center the inputs around zero and scaling it to have unit variance using the defined formula\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        # After calulating the unit_variance we can perform the operation (y = wx + b) on this layer\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        # If training is set to true we will update the running mean and variance estimated from the momentum (without gradient updation)\n",
    "        if self.training:\n",
    "            # We disable gradient updation\n",
    "            with torch.no_grad():\n",
    "                # We perform (batchRunningMean = 0.999 * batchRunningMean + 0.001 * batchMeanAtIteration)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                # We perform (batchRunningStandardDeviation = 0.999 * batchRunningStandardDeviation + 0.001 * batchStandardDeviationAtIteration)\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        # We return the (y = wx + b) after all the calculations are done (if any)\n",
    "        return self.out\n",
    "        # ------------\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # We return the parameters (gamma and beta)\n",
    "        return [self.gamma, self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self, inputs):\n",
    "        # Calculates tanh based on the inputs\n",
    "        self.out = torch.tanh(inputs)\n",
    "        return self.out\n",
    "    # Returns an empty list because there are no paramters in this layer\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Embedding:\n",
    "    # Constructor to initialize the weights of the embedding\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.weights = torch.randn((num_embeddings, embedding_dim))\n",
    "    # The indexing operation in the forward pass : Similar to -> embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    def __call__(self, indexes):\n",
    "        self.out = self.weights[indexes]\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (weights)\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "class ConsecutiveFlatten:\n",
    "     # Constructor to initialize the number of groups\n",
    "    def __init__(self, groups):\n",
    "        # Initializing the groups \n",
    "        self.groups = groups\n",
    "    # Concatenation operation in the forward pass : Flatten in groups\n",
    "    def __call__(self, inputs):\n",
    "        # Takes out each dimension from the inputs\n",
    "        batch, charactersInBlock, embeddings = inputs.shape\n",
    "        # Views as the desired group size\n",
    "        outputs = inputs.view(batch, charactersInBlock//self.groups, embeddings*self.groups)\n",
    "        # Checks if dimension 1 creates a spurious dimension\n",
    "        if outputs.shape[1] == 1:\n",
    "            # Squeezes out the dimension 1 to keep it from creating a spurious dimension\n",
    "            outputs = outputs.squeeze(dim=1)\n",
    "        self.out = outputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (empty list)\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Sequential:\n",
    "     # Constructor to initialize the layers\n",
    "    def __init__(self, layers):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.layers = layers\n",
    "    # Calling all the layers based on the inputs: Using __call__ for all the layes given all the inputs\n",
    "    def __call__(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        self.out = inputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used in all the layers in a list\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59c9b0-ac77-4a3c-be8d-52c0ce14c7c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Neural Network Initialization - Weights, Biases & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62af8c07-c87c-4edc-9fd2-b927d85930ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 22370\n"
     ]
    }
   ],
   "source": [
    "# We will define a manual seed to give a similar result on your machine, as of my machine\n",
    "torch.manual_seed(69420)\n",
    "embeddingFeatureSpaceLength = 10\n",
    "numberOfHiddenLayerNeurons = 68\n",
    "groups = 2\n",
    "\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "model = Sequential([\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    # Bigram Flatten\n",
    "    ConsecutiveFlatten(groups=groups), Linear(embeddingFeatureSpaceLength * groups, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    # Bi-bigram Flatten\n",
    "    ConsecutiveFlatten(groups=groups), Linear(numberOfHiddenLayerNeurons * groups, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    # Bi-Bi-bigram Flatten\n",
    "    ConsecutiveFlatten(groups=groups), Linear(numberOfHiddenLayerNeurons * groups, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(numberOfHiddenLayerNeurons, vocabularySize, bias=False),\n",
    "])\n",
    "\n",
    "# Disabling Gradient Tracking\n",
    "with torch.no_grad():\n",
    "    # Making the final activation less confident\n",
    "    model.layers[-1].weights *= 0.1\n",
    "\n",
    "# We initialize all the parameters\n",
    "parameters = model.parameters()\n",
    "# We check the number of parameters by printing its value\n",
    "print(\"Number of parameters:\", sum(parameter.nelement() for parameter in parameters))\n",
    "# We set 'requires_grad' to True in order to track gradients\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a82679-0165-46bd-bc2e-fc860a41dcf6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4536b8e-0083-4c76-8b0c-acede22099bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2710\n",
      "  10000/ 200000: 1.7531\n",
      "  20000/ 200000: 1.9206\n",
      "  30000/ 200000: 1.8199\n",
      "  40000/ 200000: 1.8127\n",
      "  50000/ 200000: 1.7926\n",
      "  60000/ 200000: 1.7219\n",
      "  70000/ 200000: 1.6272\n",
      "  80000/ 200000: 1.8719\n",
      "  90000/ 200000: 1.8040\n",
      " 100000/ 200000: 1.7283\n",
      " 110000/ 200000: 1.9233\n",
      " 120000/ 200000: 2.0418\n",
      " 130000/ 200000: 1.8635\n",
      " 140000/ 200000: 1.7831\n",
      " 150000/ 200000: 1.7067\n",
      " 160000/ 200000: 1.7407\n",
      " 170000/ 200000: 1.6656\n",
      " 180000/ 200000: 1.7841\n",
      " 190000/ 200000: 1.5795\n"
     ]
    }
   ],
   "source": [
    "# We want to track the losses that we use in the training\n",
    "losses = []\n",
    "# We define the number of epochs\n",
    "epochs = 200000\n",
    "# We define the batch size\n",
    "batchSize = 128\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Mini-Batches\n",
    "    indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(batchSize,))\n",
    "    inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "    \n",
    "    # Forward Pass (Mini-Batch)\n",
    "    logits = model(inputBatch)\n",
    "    loss = F.cross_entropy(logits, outputBatch)\n",
    "    # Backward Pass (Mini-Batch)\n",
    "    # We reset the gradients\n",
    "    for parameter in parameters:\n",
    "        parameter.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update Weights (Mini-Batch)\n",
    "    learning_rate = 0.1 if i < 150000 else 0.01\n",
    "    for parameter in parameters:\n",
    "        parameter.data += -learning_rate * parameter.grad\n",
    "\n",
    "    # Tracking the stats\n",
    "    # We append the losses that we use per iteration\n",
    "    if i % 10000 == 0: \n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # We do that to squash the steep curve to a nicer graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ce2a3-5f02-4f52-b166-4900e70a366b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Evaluating Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f10cb2b3-5c33-4722-baca-522a1fa819f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:1.720413088798523\n",
      "Validation Loss:1.7969316244125366\n"
     ]
    }
   ],
   "source": [
    "# Decorator for disabling gradient tracking throughout the function underneath\n",
    "@torch.no_grad()\n",
    "def splitLoss(split):\n",
    "    input, output = {\n",
    "        'Training': (trainingInputs, trainingOutputs),\n",
    "        'Validation': (validationInputs, validationOutputs),\n",
    "        'Testing': (testInputs, testOutputs)\n",
    "    }[split]\n",
    "    # Based on the split we can now forward through layers to get the logits\n",
    "    logits = model(input)\n",
    "    loss = F.cross_entropy(logits, output)\n",
    "    print(f\"{split} Loss:{loss.item()}\")\n",
    "# We can then call this method to calculate and print loss\n",
    "splitLoss('Training')\n",
    "splitLoss('Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ae32f-d36c-4601-9d6c-4dfd5935a0ca",
   "metadata": {},
   "source": [
    "Original Loss:\n",
    "```python\n",
    "Training Loss:1.8957682847976685\n",
    "Validation Loss:1.9175524711608887\n",
    "```\n",
    "\n",
    "Increasing `blockSize` Loss:\n",
    "```python\n",
    "Training Loss:1.7707616090774536\n",
    "Validation Loss:1.81971275806427\n",
    "```\n",
    "\n",
    "Implementing WaveNet for `ConsecutiveFlatten` Loss:\n",
    "```python\n",
    "Training Loss:1.720413088798523\r\n",
    "Validation Loss:1.7969316244125366\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f6777-658c-4c23-be03-11c4fbbcd6a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fixing `BatchNorm1d` for implemented `ConsecutiveFlatten`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee544c-115a-46e7-97a2-d42045029fd4",
   "metadata": {},
   "source": [
    "Let's recall the `BatchNorm1d` layer code implementation first:\n",
    "\n",
    "```python\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        self.training = training\n",
    "    def __call__(self, inputs):\n",
    "        if self.training:\n",
    "            input_mean = inputs.mean(0, keepdim=True)\n",
    "            input_variance = inputs.var(0, keepdim=True)\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e477def-2da7-4ad6-844d-64707ebea962",
   "metadata": {},
   "source": [
    "Now let's try to think through the `inputs` and `outputs` of a `BatchNorm1d` layer by looking at the shapes of inputs...\n",
    "\n",
    "Let's try to take an example...\n",
    "\n",
    "Right now, the shapes of outputs look something like this:\n",
    "```python\n",
    "Embedding : torch.Size(32, 8, 10)\n",
    "ConsecutiveFlatten : torch.Size(32, 4, 20)\n",
    "Linear : torch.Size(32, 4, 68)\n",
    "BatchNorm1d : torch.Size(32, 4, 68)\n",
    "Tanh : torch.Size(32, 4, 68)\n",
    "ConsecutiveFlatten : torch.Size(32, 2, 136)\n",
    "Linear : torch.Size(32, 2, 68)\n",
    "BatchNorm1d : torch.Size(32, 2, 68)\n",
    "Tanh : torch.Size(32, 2, 68)\n",
    "ConsecutiveFlatten : torch.Size(32, 136)\n",
    "Linear : torch.Size(32, 68)\n",
    "BatchNorm1d : torch.Size(32, 68)\n",
    "Tanh : torch.Size(32, 68)\n",
    "Linear: torch.Size(32, 27)\n",
    "```\n",
    "\n",
    "Let's take the inputs for the first `BatchNorm1d` layer, which is the output of the `Linear` layer...\n",
    "\n",
    "Right now the forward pass looks something like this:\n",
    "```python\n",
    "inputs = torch.randn(32, 4, 68)\n",
    "input_mean = inputs.mean(0, keepdim=True)\n",
    "input_variance = inputs.var(0, keepdim=True)\n",
    "input_hat = (inputs - input_mean) / torch.sqrt(input_variance + 1e-5)\n",
    "input_hat.shape\n",
    "```\n",
    "And we see that the output broadcasts properly:\n",
    "```python\n",
    "torch.Size([32, 4, 68])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68606e-7bf9-4d90-91ab-5bf98e166ad5",
   "metadata": {},
   "source": [
    "Still not understandable?\n",
    "\n",
    "Okay let's break it down with illustrations...\n",
    "\n",
    "Previously we had something like this *toy* example:\\\n",
    "![BatchNormApproachBeforeWaveNet](ExplanationMedia/Images/BatchNormApproachBeforeWaveNet.png)\n",
    "\n",
    "Here we had a batch of `4` examples, with let's say `6` dimensional embeddings, and we calculated mean across all the examples in a batch.\n",
    "\n",
    "But now we have something like this:\n",
    "![BatchNormBeforeFixingWaveNet](ExplanationMedia/Images/BatchNormBeforeFixingWaveNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418e423-ee29-4e6f-a90d-7401150e95b9",
   "metadata": {},
   "source": [
    "Here, we see that we mean across only the batch of examples, which is not what we want...\n",
    "\n",
    "Rather we also need to do mean across all the bigram groups like this (treat the bigram groups as a **batch-dimension** as well):\n",
    "![BatchNormAfterFixingWaveNet](ExplanationMedia/Images/BatchNormAfterFixingWaveNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce13b5-ef21-4569-a608-c06760fd4ff7",
   "metadata": {},
   "source": [
    "Turns out we can do this pretty easily when we look at the documentation of `TORCH.MEAN` from PyTorch, we have:\n",
    "\n",
    "```python\n",
    "torch.mean(input, *, dtype=None)  Tensor\n",
    "```\n",
    "\n",
    "**Parameters**:\n",
    "- **input** (`Tensor`)  the input tensor.\n",
    "- **dim** (`int` or `tuple of ints`)  the dimension or dimensions to reduce.\n",
    "- **keepdim** (`bool`)  whether the output tensor has dim retained or not.\n",
    "\n",
    "**Keyword Arguments**:\n",
    "- **dtype** (`torch.dtype`, *optional*)  the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.\n",
    "- **out** (`Tensor`, *optional*)  the output tensor.\n",
    "\n",
    "We see that instead of just an `int` we can specify a `tuple of ints` to reduce over dimensions, so we can easily perform reduction over the dimensions specified in the tuple...\n",
    "\n",
    "For us, it will be dimension `0` and `1`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481fd38d-3598-4b60-8a08-4eef1814901d",
   "metadata": {},
   "source": [
    "But also see how in the later layers, the `BatchNorm1d` performs the batch normalization on `2` dimensional inputs as well, so we need to handle different calculations for different dimensions as well...\n",
    "\n",
    "And it seems pretty straight forward it seems... We simply check if the inputs are of shape of `2` dimensions then we pass in `dim=0` into the `input_mean` and `input_variance` otherwise we pass in `dim=(0, 1)` if we get a three dimensional input, otherwise it will throw an error which is good in my opinion...\n",
    "\n",
    "But the moment we think of this we should also know that, even here we start departing from the original `BATCHNORM1D` from PyTorch.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because if we look at the documentation of `BATCHNORM1D`, it states the shapes should be:\n",
    "- **Input**: `(N,C)` or `(N,C,L)`, where `N` is the batch size, `C` is the number of features or channels, and `L` is the sequence length\n",
    "- **Output**: `(N,C)` or `(N,C,L)` (same shape as input)\n",
    "\n",
    "Which means `BATCHNORM1D` from PyTorch reduces the dimensions `(0, 2)` instead of `(0, 1)`, because our `BatchNorm1d` expects `(N,C)` or `(N,L,C)`. \n",
    "\n",
    "Honestly I like the expression `(0, 1)` instead of `(0, 2)` so we will keep the implemenation as we have done till now...\n",
    "\n",
    "Let's now edit our modular block for `BatchNorm1d`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09952f8-6a3f-4fbd-8ad7-b86149e4e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # Constructor to initialize the weights and biases\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # We initialize the weights using fan-in and fan-out and use the kaiming initialization which is the square root of fan-in\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        # We initilize the biases using fan-out and set them to 0 values if bias is set to True else we set it to None\n",
    "        self.biases = torch.zeros(fan_out) if bias else None\n",
    "    # Defines how we forward pass these layers which is (y = wx + b)\n",
    "    def __call__(self, inputs):\n",
    "        # (w*x)\n",
    "        self.out = inputs @ self.weights\n",
    "        # Checking if we have a bias\n",
    "        if self.biases is not None:\n",
    "            # (wx) + b\n",
    "            self.out += self.biases\n",
    "        # We return (y = wx + b)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # If bias is there return (w*x + b) else return (w*x)\n",
    "        return [self.weights] + ([] if self.biases is None else [self.biases])\n",
    "class BatchNorm1d:\n",
    "    # Constructor to initialize the gain, bias, and the buffers (running mean and running variance) using the dimensions we get\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        # We initialize the gains to 1 values of dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        # We initialize the bias to 0 values of dimensions\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        # We initialize the running mean to 0 values of dimensions\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        # We initialize the running variance to 1 values of dimensions\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        # We initialize a checker parameter that checks whether we are training or not (We will discuss this later) \n",
    "        self.training = training\n",
    "    # Forward Pass and Buffer Updation based on the inputs\n",
    "    def __call__(self, inputs):\n",
    "        # Forward Pass\n",
    "        # ------------\n",
    "        # If training is set to true we will calculate the mean and variance estimated from the current batch of inputs\n",
    "        if self.training:\n",
    "            # Checking if the inputs are 2-dimensional\n",
    "            if inputs.ndim == 2:\n",
    "                # Setting the dimension to reduce over to 0\n",
    "                dimensions = 0\n",
    "            # Checking if the inputs are 3-dimensional\n",
    "            elif inputs.ndim == 3:\n",
    "                # Setting the dimension to reduce over to (0, 1)\n",
    "                dimensions = 0\n",
    "            # Initializing mean of current batch of inputs along dimensions and keeping the dimensions\n",
    "            input_mean = inputs.mean(dimensions, keepdim=True)\n",
    "            # Initializing variance of current batch of inputs along dimensions and keeping the dimensions\n",
    "            input_variance = inputs.var(dimensions, keepdim=True)\n",
    "        # If not training, we will use the running mean and variance of the inputs of the saved values\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        # We center the inputs around zero and scaling it to have unit variance using the defined formula\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        # After calulating the unit_variance we can perform the operation (y = wx + b) on this layer\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        # If training is set to true we will update the running mean and variance estimated from the momentum (without gradient updation)\n",
    "        if self.training:\n",
    "            # We disable gradient updation\n",
    "            with torch.no_grad():\n",
    "                # We perform (batchRunningMean = 0.999 * batchRunningMean + 0.001 * batchMeanAtIteration)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                # We perform (batchRunningStandardDeviation = 0.999 * batchRunningStandardDeviation + 0.001 * batchStandardDeviationAtIteration)\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        # We return the (y = wx + b) after all the calculations are done (if any)\n",
    "        return self.out\n",
    "        # ------------\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # We return the parameters (gamma and beta)\n",
    "        return [self.gamma, self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self, inputs):\n",
    "        # Calculates tanh based on the inputs\n",
    "        self.out = torch.tanh(inputs)\n",
    "        return self.out\n",
    "    # Returns an empty list because there are no paramters in this layer\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Embedding:\n",
    "    # Constructor to initialize the weights of the embedding\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.weights = torch.randn((num_embeddings, embedding_dim))\n",
    "    # The indexing operation in the forward pass : Similar to -> embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    def __call__(self, indexes):\n",
    "        self.out = self.weights[indexes]\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (weights)\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "class ConsecutiveFlatten:\n",
    "     # Constructor to initialize the number of groups\n",
    "    def __init__(self, groups):\n",
    "        # Initializing the groups \n",
    "        self.groups = groups\n",
    "    # Concatenation operation in the forward pass : Flatten in groups\n",
    "    def __call__(self, inputs):\n",
    "        # Takes out each dimension from the inputs\n",
    "        batch, charactersInBlock, embeddings = inputs.shape\n",
    "        # Views as the desired group size\n",
    "        outputs = inputs.view(batch, charactersInBlock//self.groups, embeddings*self.groups)\n",
    "        # Checks if dimension 1 creates a spurious dimension\n",
    "        if outputs.shape[1] == 1:\n",
    "            # Squeezes out the dimension 1 to keep it from creating a spurious dimension\n",
    "            outputs = outputs.squeeze(dim=1)\n",
    "        self.out = outputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (empty list)\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Sequential:\n",
    "     # Constructor to initialize the layers\n",
    "    def __init__(self, layers):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.layers = layers\n",
    "    # Calling all the layers based on the inputs: Using __call__ for all the layes given all the inputs\n",
    "    def __call__(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        self.out = inputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used in all the layers in a list\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b102b-66d5-4a05-9812-c255a8e18265",
   "metadata": {},
   "source": [
    "Let's now try to run the WaveNet with fixed `BatchNorm1d` now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff6051-9daa-4761-b3f0-4a31828e742c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Running **WaveNet** for implemented `ConsecutiveFlatten` and `BatchNorm1d`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23238434-ad97-4991-8904-dd99985e2a2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb51537c-ae67-4ce7-9369-7a72d7209209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Examples: 53982\n",
      "Training Examples: 43185\n",
      "Validation Examples: 5398\n",
      "Test Examples: 5399\n"
     ]
    }
   ],
   "source": [
    "# We define a Block Size based on the number of characters we feed are going to feed to predict the next one\n",
    "blockSize = 8\n",
    "# Will build the dataset on only the words we take as input\n",
    "def buildDataset(words):\n",
    "    # We define two lists, inputs & outputs, where inputs are our blocks of the block size mentioned above and outputs are the label indexes\n",
    "    inputs , outputs = [], []\n",
    "    # We iterate over each word\n",
    "    for word in words:\n",
    "        # We define the block for each iteration and fill it with 0 values -> [0, 0, 0]\n",
    "        block = [0] * blockSize\n",
    "        # We run another loop for each word's character, here word also needs the ending token '.'\n",
    "        for character in word + '.':\n",
    "            # We take out the index from our look-up table\n",
    "            index = stoi[character]\n",
    "            # We append the input with our block\n",
    "            inputs.append(block)\n",
    "            # We append the output label with out index of the character\n",
    "            outputs.append([index])\n",
    "            # We then take the block, crop it 1 size from the left and append the next index to it (sliding window of name)\n",
    "            block = block[1:] + [index]\n",
    "    # We also convert these inputs and outputs to tensors for neural network processing\n",
    "    inputs = torch.tensor(inputs)\n",
    "    outputs = torch.flatten(torch.tensor(outputs))\n",
    "    # We return the inputs and outputs\n",
    "    return inputs, outputs\n",
    "\n",
    "# We define a manual seed to random\n",
    "random.seed(69)\n",
    "# We shuffle all the words, so that the model receives all kinds of data\n",
    "random.shuffle(words)\n",
    "# We define two number of inputs\n",
    "# We take the number of examples to 80% in the first variable\n",
    "numberOfInputs1 = int(0.8*len(words))\n",
    "# We take the number of examples to 90% in the first variable\n",
    "numberOfInputs2 = int(0.9*len(words))\n",
    "# Inputs and outputs that go till 80% of the examples\n",
    "trainingInputs, trainingOutputs = buildDataset(words[:numberOfInputs1])\n",
    "# Inputs and outputs that start at 80% of the examples and go till 90% of the examples\n",
    "validationInputs, validationOutputs = buildDataset(words[numberOfInputs1:numberOfInputs2])\n",
    "# Inputs and outputs that start at 90% of the examples\n",
    "testInputs, testOutputs = buildDataset(words[numberOfInputs2:])\n",
    "\n",
    "# We can check the numbers\n",
    "print(\"Total Examples:\",len(words)) # 100%\n",
    "print(\"Training Examples:\",len(words[:numberOfInputs1])) # 80%\n",
    "print(\"Validation Examples:\",len(words[numberOfInputs1:numberOfInputs2])) # 10%\n",
    "print(\"Test Examples:\",len(words[numberOfInputs2:])) # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44c2ee-1e4e-4020-a2c1-29822205ba52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Modular Block Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ff54ac-7f00-4447-a2ba-3634998d7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # Constructor to initialize the weights and biases\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # We initialize the weights using fan-in and fan-out and use the kaiming initialization which is the square root of fan-in\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        # We initilize the biases using fan-out and set them to 0 values if bias is set to True else we set it to None\n",
    "        self.biases = torch.zeros(fan_out) if bias else None\n",
    "    # Defines how we forward pass these layers which is (y = wx + b)\n",
    "    def __call__(self, inputs):\n",
    "        # (w*x)\n",
    "        self.out = inputs @ self.weights\n",
    "        # Checking if we have a bias\n",
    "        if self.biases is not None:\n",
    "            # (wx) + b\n",
    "            self.out += self.biases\n",
    "        # We return (y = wx + b)\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # If bias is there return (w*x + b) else return (w*x)\n",
    "        return [self.weights] + ([] if self.biases is None else [self.biases])\n",
    "class BatchNorm1d:\n",
    "    # Constructor to initialize the gain, bias, and the buffers (running mean and running variance) using the dimensions we get\n",
    "    def __init__(self, dimensions, epsilon=1e-5, momentum=0.1, training=True):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # Parameters using dimensions\n",
    "        # We initialize the gains to 1 values of dimensions\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        # We initialize the bias to 0 values of dimensions\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "        # Buffers\n",
    "        # We initialize the running mean to 0 values of dimensions\n",
    "        self.running_mean = torch.zeros(dimensions)\n",
    "        # We initialize the running variance to 1 values of dimensions\n",
    "        self.running_variance = torch.ones(dimensions)\n",
    "        # We initialize a checker parameter that checks whether we are training or not (We will discuss this later) \n",
    "        self.training = training\n",
    "    # Forward Pass and Buffer Updation based on the inputs\n",
    "    def __call__(self, inputs):\n",
    "        # Forward Pass\n",
    "        # ------------\n",
    "        # If training is set to true we will calculate the mean and variance estimated from the current batch of inputs\n",
    "        if self.training:\n",
    "            # Checking if the inputs are 2-dimensional\n",
    "            if inputs.ndim == 2:\n",
    "                # Setting the dimension to reduce over to 0\n",
    "                dimensions = 0\n",
    "            # Checking if the inputs are 3-dimensional\n",
    "            elif inputs.ndim == 3:\n",
    "                # Setting the dimension to reduce over to (0, 1)\n",
    "                dimensions = 0\n",
    "            # Initializing mean of current batch of inputs along dimensions and keeping the dimensions\n",
    "            input_mean = inputs.mean(dimensions, keepdim=True)\n",
    "            # Initializing variance of current batch of inputs along dimensions and keeping the dimensions\n",
    "            input_variance = inputs.var(dimensions, keepdim=True)\n",
    "        # If not training, we will use the running mean and variance of the inputs of the saved values\n",
    "        else:\n",
    "            input_mean = self.running_mean\n",
    "            input_variance = self.running_variance\n",
    "        # We center the inputs around zero and scaling it to have unit variance using the defined formula\n",
    "        unit_variance = (inputs - input_mean) / torch.sqrt(input_variance) + self.epsilon\n",
    "        # After calulating the unit_variance we can perform the operation (y = wx + b) on this layer\n",
    "        self.out = self.gamma * unit_variance + self.beta\n",
    "        # ------------\n",
    "        # Buffer Updation\n",
    "        # ------------\n",
    "        # If training is set to true we will update the running mean and variance estimated from the momentum (without gradient updation)\n",
    "        if self.training:\n",
    "            # We disable gradient updation\n",
    "            with torch.no_grad():\n",
    "                # We perform (batchRunningMean = 0.999 * batchRunningMean + 0.001 * batchMeanAtIteration)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * input_mean\n",
    "                # We perform (batchRunningStandardDeviation = 0.999 * batchRunningStandardDeviation + 0.001 * batchStandardDeviationAtIteration)\n",
    "                self.running_variance = (1 - self.momentum) * self.running_variance + self.momentum * input_variance\n",
    "        # We return the (y = wx + b) after all the calculations are done (if any)\n",
    "        return self.out\n",
    "        # ------------\n",
    "    # Defines a method to return all the parameters used by this layer\n",
    "    def parameters(self):\n",
    "        # We return the parameters (gamma and beta)\n",
    "        return [self.gamma, self.beta]\n",
    "class Tanh:\n",
    "    def __call__(self, inputs):\n",
    "        # Calculates tanh based on the inputs\n",
    "        self.out = torch.tanh(inputs)\n",
    "        return self.out\n",
    "    # Returns an empty list because there are no paramters in this layer\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Embedding:\n",
    "    # Constructor to initialize the weights of the embedding\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.weights = torch.randn((num_embeddings, embedding_dim))\n",
    "    # The indexing operation in the forward pass : Similar to -> embedding = embeddingLookUpMatrix[inputBatch]\n",
    "    def __call__(self, indexes):\n",
    "        self.out = self.weights[indexes]\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (weights)\n",
    "    def parameters(self):\n",
    "        return [self.weights]\n",
    "class ConsecutiveFlatten:\n",
    "     # Constructor to initialize the number of groups\n",
    "    def __init__(self, groups):\n",
    "        # Initializing the groups \n",
    "        self.groups = groups\n",
    "    # Concatenation operation in the forward pass : Flatten in groups\n",
    "    def __call__(self, inputs):\n",
    "        # Takes out each dimension from the inputs\n",
    "        batch, charactersInBlock, embeddings = inputs.shape\n",
    "        # Views as the desired group size\n",
    "        outputs = inputs.view(batch, charactersInBlock//self.groups, embeddings*self.groups)\n",
    "        # Checks if dimension 1 creates a spurious dimension\n",
    "        if outputs.shape[1] == 1:\n",
    "            # Squeezes out the dimension 1 to keep it from creating a spurious dimension\n",
    "            outputs = outputs.squeeze(dim=1)\n",
    "        self.out = outputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used by this layer in a list (empty list)\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Sequential:\n",
    "     # Constructor to initialize the layers\n",
    "    def __init__(self, layers):\n",
    "        # Initializing the weights of the embedding\n",
    "        self.layers = layers\n",
    "    # Calling all the layers based on the inputs: Using __call__ for all the layes given all the inputs\n",
    "    def __call__(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer(inputs)\n",
    "        self.out = inputs\n",
    "        return self.out\n",
    "    # Defines a method to return all the parameters used in all the layers in a list\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2e1aa-1935-4aab-9952-b508a3528efc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Neural Network Initialization - Weights, Biases & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bce5277-ce80-4fbc-a61e-eee2a1df68dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 22370\n"
     ]
    }
   ],
   "source": [
    "# We will define a manual seed to give a similar result on your machine, as of my machine\n",
    "torch.manual_seed(69420)\n",
    "embeddingFeatureSpaceLength = 10\n",
    "numberOfHiddenLayerNeurons = 68\n",
    "groups = 2\n",
    "\n",
    "# Initializing the layers into a single list called 'layers', stacking them one after another\n",
    "model = Sequential([\n",
    "    Embedding(vocabularySize, embeddingFeatureSpaceLength),\n",
    "    # Bigram Flatten\n",
    "    ConsecutiveFlatten(groups=groups), Linear(embeddingFeatureSpaceLength * groups, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    # Bi-bigram Flatten\n",
    "    ConsecutiveFlatten(groups=groups), Linear(numberOfHiddenLayerNeurons * groups, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    # Bi-Bi-bigram Flatten\n",
    "    ConsecutiveFlatten(groups=groups), Linear(numberOfHiddenLayerNeurons * groups, numberOfHiddenLayerNeurons, bias=False), BatchNorm1d(numberOfHiddenLayerNeurons), Tanh(),\n",
    "    Linear(numberOfHiddenLayerNeurons, vocabularySize, bias=False),\n",
    "])\n",
    "\n",
    "# Disabling Gradient Tracking\n",
    "with torch.no_grad():\n",
    "    # Making the final activation less confident\n",
    "    model.layers[-1].weights *= 0.1\n",
    "\n",
    "# We initialize all the parameters\n",
    "parameters = model.parameters()\n",
    "# We check the number of parameters by printing its value\n",
    "print(\"Number of parameters:\", sum(parameter.nelement() for parameter in parameters))\n",
    "# We set 'requires_grad' to True in order to track gradients\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2d841-1fb8-4e18-97f3-ccce03234675",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a5a3b2-9f35-4684-9c7c-1f72230a319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2983\n",
      "  10000/ 200000: 1.7105\n",
      "  20000/ 200000: 2.0210\n",
      "  30000/ 200000: 1.8848\n",
      "  40000/ 200000: 2.0244\n",
      "  50000/ 200000: 1.8346\n",
      "  60000/ 200000: 1.9690\n",
      "  70000/ 200000: 1.6829\n",
      "  80000/ 200000: 1.6780\n",
      "  90000/ 200000: 1.6910\n",
      " 100000/ 200000: 1.5295\n",
      " 110000/ 200000: 1.6481\n",
      " 120000/ 200000: 1.6747\n",
      " 130000/ 200000: 1.6409\n",
      " 140000/ 200000: 1.6980\n",
      " 150000/ 200000: 1.7050\n",
      " 160000/ 200000: 1.5885\n",
      " 170000/ 200000: 1.7374\n",
      " 180000/ 200000: 1.7919\n",
      " 190000/ 200000: 1.6802\n"
     ]
    }
   ],
   "source": [
    "# We want to track the losses that we use in the training\n",
    "losses = []\n",
    "# We define the number of epochs\n",
    "epochs = 200000\n",
    "# We define the batch size\n",
    "batchSize = 128\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Mini-Batches\n",
    "    indexes = torch.randint(low=0, high=trainingInputs.shape[0], size=(batchSize,))\n",
    "    inputBatch, outputBatch = trainingInputs[indexes], trainingOutputs[indexes]\n",
    "    \n",
    "    # Forward Pass (Mini-Batch)\n",
    "    logits = model(inputBatch)\n",
    "    loss = F.cross_entropy(logits, outputBatch)\n",
    "    # Backward Pass (Mini-Batch)\n",
    "    # We reset the gradients\n",
    "    for parameter in parameters:\n",
    "        parameter.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update Weights (Mini-Batch)\n",
    "    learning_rate = 0.1 if i < 150000 else 0.01\n",
    "    for parameter in parameters:\n",
    "        parameter.data += -learning_rate * parameter.grad\n",
    "\n",
    "    # Tracking the stats\n",
    "    # We append the losses that we use per iteration\n",
    "    if i % 10000 == 0: \n",
    "        print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item()) # We do that to squash the steep curve to a nicer graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4212b27-26d1-4cf7-8399-9c1f7a1a4f62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Evaluating Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "077a8aa2-1dcd-4cd5-ad60-9a54a98d8d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:1.7204307317733765\n",
      "Validation Loss:1.7969590425491333\n"
     ]
    }
   ],
   "source": [
    "# Decorator for disabling gradient tracking throughout the function underneath\n",
    "@torch.no_grad()\n",
    "def splitLoss(split):\n",
    "    input, output = {\n",
    "        'Training': (trainingInputs, trainingOutputs),\n",
    "        'Validation': (validationInputs, validationOutputs),\n",
    "        'Testing': (testInputs, testOutputs)\n",
    "    }[split]\n",
    "    # Based on the split we can now forward through layers to get the logits\n",
    "    logits = model(input)\n",
    "    loss = F.cross_entropy(logits, output)\n",
    "    print(f\"{split} Loss:{loss.item()}\")\n",
    "# We can then call this method to calculate and print loss\n",
    "splitLoss('Training')\n",
    "splitLoss('Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426b838-c17a-4fcc-bedc-ae3d3df31242",
   "metadata": {},
   "source": [
    "Original Loss:\n",
    "```python\n",
    "Training Loss:1.8957682847976685\n",
    "Validation Loss:1.9175524711608887\n",
    "```\n",
    "\n",
    "Increasing `blockSize` Loss:\n",
    "```python\n",
    "Training Loss:1.7707616090774536\n",
    "Validation Loss:1.81971275806427\n",
    "```\n",
    "\n",
    "Implementing WaveNet for `ConsecutiveFlatten` Loss:\n",
    "```python\n",
    "Training Loss:1.720413088798523\n",
    "Validation Loss:1.7969316244125366\n",
    "```\n",
    "\n",
    "Implementing WaveNet for `ConsecutiveFlatten` and `BatchNorm1d` Loss:\n",
    "```python\n",
    "Training Loss:1.7204307317733765\r\n",
    "Validation Loss:1.7969590425491333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b6c7a-e96c-44b9-a6a7-15ec072bf7e9",
   "metadata": {},
   "source": [
    "We see that we get a slightly improved statistics as we go forward...\n",
    "\n",
    "We still expect a slightly improved performance because now we are not only estimating the means and the variances of `128` examples in a batch, instead now we are estimating the means and the variances of `128` times `4` groups in a batch, which means that we have a lot more numbers that go into any one estimate of the mean and variance, which makes it a lot more stable and less *wiggly* for those estimates of those statistics...\n",
    "\n",
    "We can now try to scale up the WaveNet now... And try to sample from it... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c98bb0-79b9-4a38-8cfd-3d35fa3a4c51",
   "metadata": {},
   "source": [
    "# Scaling Up the **WaveNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c8d61-dcc6-4a18-a8da-f50c890c8fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99664b-5e11-417c-b735-fc08296f793d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b981a66-50c0-4417-bdb0-6df9a10db035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d018a83-a68f-4017-81ff-8452a5163b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89af18a-cc30-4f9f-b746-d83777529084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6600434-ed6e-4136-b159-44a2fbe25e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415ed58-c827-4044-836a-231b6fd27b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43604f7b-1a9e-4f0d-b011-41cb71c3b390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbae448-fb25-49d9-ba0f-34c7ac57a907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc24c4-6afd-4e76-ac87-3c2acb071b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
