{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to OpenAI GPT v2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is the continuation of <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch</a> and <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20Tokenizer.ipynb\">GPT Tokenizer</a> notebooks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to reproduce the <a href=\"https://github.com/openai/gpt-2\">OpenAI's GPT 2</a> model, the (`124M`) version of it.\n",
    "\n",
    "Now, when OpenAI released GPT 2, they released it with this <a href=\"https://openai.com/index/better-language-models/\">blog post</a> and this <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">paper</a>. And on top of that, they released this <a href=\"https://github.com/openai/gpt-2\">code</a> on GitHub..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, when reproducing GPT 2, we have to be careful, because we are going to be reproducing the `124M` parameter model. And the thing to be careful with it is there's always a sub-series of models of different sizes when these model releases are made and usually the biggest model is called the **\"GPT\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the charts that we have in the paper for a second: \\\n",
    "![OpenAIGPT2 Graphs](ExplanationMedia/Images/OpenAIGPT2Graphs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the reason we have multiple models is because, according to the above graphs we see that we consider the `Number of parameters in the Language Model` in the `x-axis` and the `y-axis` we put a lot of *downstream metrics* that we are interested in like (\"Translation\", \"Summerization\", \"Question Answering\") and so on and we can chart out the *downstream metrics* as the model size increases.\n",
    "\n",
    "And in the paper we see a table like this:\n",
    "\n",
    "| Parameters | Layers | $d_{model}$ |\n",
    "|------------|--------|-----------|\n",
    "| 117M       | 12     | 768       |\n",
    "| 345M       | 24     | 1024      |\n",
    "| 762M       | 36     | 1280      |\n",
    "| 1542M      | 48     | 1600      |\n",
    "\n",
    "And we see `4` models in the `GPT-2` sub series, starting at `124M` all the way up to `1558M`...\n",
    "\n",
    "But you might be thinking that I might have made a mistake because, in the table the numbers are different and the numbers I spoke of are different. And the reason my numbers disagree with this table is because this entire table is wrong and if we go to their <a href=\"https://github.com/openai/gpt-2\">GitHub repository</a> we see a note that says:\n",
    "> * *Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper). Thus you may have seen small referred to as 117M and medium referred to as 345M.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the `124M` parameter model, we see that they used `12 Layers` in the Transformer and `768 Channel Dimensions` in the Transformer.\n",
    "\n",
    "And by the end of this notebook we will try to beat the original `GPT-2 124M` model and will be looking at loss graphs to see our model perform better.\n",
    "\n",
    "The thing to note here is, this paper is more than 5 years old now and it was probably a very complicated optimization at the time and the computation was very low at the time, but today we can reproduce the same model's performance in roughly an hour or so and it will cost us around $10 (if we want to do this on a cloud compute, or in other words, a computer that we can all rent). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one more thing to mention is, OpenAI did release it's model's weights and it is available on it's GitHub repository, but it's paper is not good with all of it's details with the training.\n",
    "\n",
    "So, in addition to the GPT-2 paper, we will also be referring to the <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3 paper</a>, which is a lot more concrete and a lot of the hyper-parameters and optimization settings and so on, which is not a huge departure from the architecture of GPT-2 version of the model.\n",
    "\n",
    "\n",
    "So, let's do this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Hugging Face Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first thing we'd like to do is start at the very end. Or in other words, we'll load the `GPT-2 124M` model as it was released by OpenAI and take it for a spin and sample some `tokens` from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the issue is...\n",
    "\n",
    "When we look at the code base and look for the <a href=\"\">`model.py`</a> we see these imports:\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.training import HParams\n",
    "```\n",
    "\n",
    "And we realise that the code is written in <a href=\"https://www.tensorflow.org/\">TensorFlow</a> (another alternative for creating and training deep learning models offered by Google). Meaning that the original `GPT-2` code was written in TensorFlow and is not used anymore...\n",
    "\n",
    "And as per our previous notebooks, we'd like to use PyTorch. And it will be a lot easier if we'd be able to work with the old explanations.\n",
    "\n",
    "But the problem with that is that the initial code is in TensorFlow and we'd like to use PyTorch. So, in order to get the targets we'd like to use the <a href=\"https://huggingface.co/docs/transformers/en/index\">`Hugging Face Transformers Library`</a> released at PyPi. We can use this <a href=\"https://huggingface.co/docs/transformers/en/installation\">installation documentaiton</a> to walk through the steps to install the library in our system..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check out Hugging Face's implementation of that transformer in their <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\">`modeling_gpt2.py`</a>. Which did a lot of work to convert all those TensorFlow code to PyTorch such that it becomes easier to load and work with.\n",
    "\n",
    "So in particular we can look at the <a href=\"https://huggingface.co/openai-community/gpt2\">Hugging Face GPT-2</a> model and load it using the Hugging Face transformers...\n",
    "\n",
    "\n",
    "So this is what the code looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "huggingface_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "huggingfaceStateDictionary = huggingface_model.state_dict()\n",
    "\n",
    "for key, value in huggingfaceStateDictionary.items():\n",
    "    print(key, value.shape)\n",
    "```\n",
    "Which gives us the result:\n",
    "```python\n",
    "transformer.wte.weight torch.Size([50257, 768])\n",
    "transformer.wpe.weight torch.Size([1024, 768])\n",
    "transformer.h.0.ln_1.weight torch.Size([768])\n",
    "transformer.h.0.ln_1.bias torch.Size([768])\n",
    "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.0.ln_2.weight torch.Size([768])\n",
    "transformer.h.0.ln_2.bias torch.Size([768])\n",
    "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.1.ln_1.weight torch.Size([768])\n",
    "transformer.h.1.ln_1.bias torch.Size([768])\n",
    "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.1.ln_2.weight torch.Size([768])\n",
    "transformer.h.1.ln_2.bias torch.Size([768])\n",
    "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
    "...\n",
    "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.ln_f.weight torch.Size([768])\n",
    "transformer.ln_f.bias torch.Size([768])\n",
    "lm_head.weight torch.Size([50257, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One awkward thing about this is, when we say `gpt2` it actually loads the `124M` parameter model and if we want the actual `GPT-2` model we'd specify it as `gpt2-xl`...\n",
    "\n",
    "Now when we actually get this `GPT-2` initialized, we want to get the **state dictionary** which is the **raw tensors loaded with values** and we can get those using the `.state_dict()` method. and we can print the `key` (which are the tensors) and the `value` (which are the tensor values) and we can look at the shapes of the `value` tensors to get an idea of the shapes of the states in the model...\n",
    "\n",
    "So, we can now look at the different parameters inside the `GPT-2` model and their shapes...\n",
    "\n",
    "And we can see that there are a lot of short forms of the terms that we already know of, so let's recall that:\n",
    "1. **wte**: Word Token Embeddings\n",
    "2. **wpe**: Word Position Embeddings\n",
    "3. **ln**: Layer Normalization\n",
    "4. **attn**: Attention\n",
    "5. **c_attn**: Cross Attention (awkward because `GPT-2` is a decoder only architecture and should be named **self attention**)\n",
    "6. **c_proj**: Projection layer within attention or MLP\n",
    "7. **mlp**: Multi-Layer Perceptron\n",
    "8. **lm_head**: Language Model Head (output layer)\n",
    "9. **c_fc**: Current/Common Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially can recall the very first key-value pair `transformer.wte.weight torch.Size([50257, 768])` as the `Word Token Embeddings` having a shape of `[50257, 768]` and it comes from the `50257` vocabulary of tokens (which is exactly the number of tokens we spoke about in our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20Tokenizer.ipynb\">Tokenizer Notebook</a>) having `768` feature space (or embedding vector space, or `768 dimensional embedding`)...\n",
    "\n",
    "We can also look at the second key-value pair `transformer.wpe.weight torch.Size([1024, 768])`, we can recall them as `Word Positional Embeddings` having a shape of `[1024, 768]`. So, because `GPT-2` has a maximum sequence length of `1024` we have upto `1024` positions that each token can attend to in the past. And every one of those positions in `GPT-2` has a fixed vector of `768` that is learnt by optimization.\n",
    "\n",
    "And everything else is just the other weights and biases of this transformer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now for example, if we take just the `Positional Embeddings` and we flatten it out (we get a `[1, 768]` vector) and take just the first `20` elements of the `768` embeddings we can see that we get the proper weights as an output for this code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "huggingfaceStateDictionary['transformer.wpe.weight'].view(-1)[:20]\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "tensor([-0.0188, -0.1974,  0.0040,  0.0113,  0.0638, -0.1050,  0.0369, -0.1680,\n",
    "        -0.0491, -0.0565, -0.0025,  0.0135, -0.0042,  0.0151,  0.0166, -0.1381,\n",
    "        -0.0063, -0.0461,  0.0267, -0.2042])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot these weights and try to see what they represent like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(huggingfaceStateDictionary['transformer.wpe.weight'], cmap='gray')\n",
    "```\n",
    "\n",
    "![GPT-2.transformer.wpe.weight](ExplanationMedia/Images/GPT-2.transformer.wpe.weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that this has structure, because these positional embeddings end up learning these **sinusoids** and **cosines** to represent each of these positions and each row here stands in for that position and is processed by the transformer to recover all the relative positions and realise which token is where and attend to them depending on their position not just their content...\n",
    "\n",
    "So now if we look at the individual columns of these we see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "plt.plot(huggingfaceStateDictionary['transformer.wpe.weight'][:, 150])\n",
    "plt.plot(huggingfaceStateDictionary['transformer.wpe.weight'][:, 200])\n",
    "plt.plot(huggingfaceStateDictionary['transformer.wpe.weight'][:, 250])\n",
    "```\n",
    "![GPT-2Graphs.transformer.wpe.weight](ExplanationMedia/Images/GPT-2Graphs.transformer.wpe.weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we still don't know what these embeddings are doing and why they are the way they are.\n",
    "\n",
    "But we can still see that the lines are a little noisy and jittery and that is because this model was not fully trained, and the more trained this model becomes the more we'd expect these graphs to smooth out, which also tells us that the original `GPT-2` is an **under-trained** model.\n",
    "\n",
    "If I remember correctly, in the original \"Attention-Is-All-You-Need\" paper, the `positional embeddings` are actually initialized and fixed to sinusoids and cosines of different frequencies, but in `GPT-2` these are trained from scratch and they seem to recover these features during the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the `Hugging Face Transformers` we can not just get all the raw weights but also get something called `pipeline` and sample from it...\n",
    "\n",
    "Here is the sample code snippet for `5` different generations of the same context window of tokens `\"Hello, I'm a language model,\"`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generator(\"Hello, I'm a language model,\", max_length=50, num_return_sequences=5)\n",
    "```\n",
    "For which we get:\n",
    "```python\n",
    "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are the ones I like the most. To do your research, please contact me, this isn't your\"},\n",
    " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\\nBut my job as a C programmer is to sort through every single line of the script so I\"},\n",
    " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend from college a bit earlier, and in the context of the current language model I think it's important\"},\n",
    " {'generated_text': 'Hello, I\\'m a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string-replace \"\\\\r\" ))) {\\n\\nconsole. log\\n\\n}\\n\\nthat\\'s'},\n",
    " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example. I'm making an API for a game where I want a character to play a little bit of a\"}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, even though we are setting a seed we get different generations from both the code and the official <a href=\"https://huggingface.co/openai-community/gpt2\">Hugging Face GPT-2 Hosted Inference API</a>.\n",
    "\n",
    "But at this stage what is important is, we are getting coherent text and we were successfully able to load the model and look at all of it's parameters and the keys tell us, where in the model these come from...\n",
    "\n",
    "But we want to actually write our own `GPT-2` class so that we have a full understanding of what's happening there and we also don't want to work with something like the <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\">`modeling_gpt2.py`</a> because it's too complicated and we want to write this from scratch ourselves.\n",
    "\n",
    "So we are going to be implementing our `GPT-2` model in `GPT_v2.5.py` script inside our `GPT Scripts` directory in parallel...\n",
    "\n",
    "But first let's load the `GPT-2 124M` into our `GPT_v2.5.py` for the class that we are going to develop from scratch, which is going to give us confidence that we can load the OpenAI model and there's a setting of weights that exactly is the `124M` model and we will try to surpass our own created `GPT` class...\n",
    "\n",
    "So, we're going to get different weights and everything is going to look different and hopefully even better and we will have the confidence that we are in the same model family and same model class and we just have to re-discover a good setting of the weights from scratch... \n",
    "\n",
    "So let's now write the `GPT-2` model and let's load the weights and make sure that we can also generate text that looks coherent..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Pre-Trained Weight Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer - Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now swing over to the <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\" paper</a> that started everything and look at the Transformer architecture:\n",
    "\n",
    "![Transformer_Model_Architecture](ExplanationMedia/Images/Transformer_Model_Architecture.png)\n",
    "\n",
    "Now, once again, like the last notebook, we mentioned that this architecture has changed over the years and `GPT-2` is slightly modified than the original `Transformer`... \n",
    "\n",
    "In particular, we do **NOT** have the **Encoder**, and `GPT-2` is a **Decoder** only `Transformer` as we call it. In addition to that the **Cross-Attention** that is used by that **Encoder** is also **missing**. Everything else stays almost the same, but there are some differences that we are going to see next..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are two main differences: \\\n",
    "When we go to the `GPT-2` <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">paper</a>, under section `2.3 Model` we see that there's a re-shuffling of the layer-normalizations (they change place) and an additional layer normalization was added after the final self-attention block..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters (`GPTConfiguration`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the skeleton of the `nn.Module`(s) in our GPT Script and in particular we want to match up the schema that we got from `Hugging Face GPT-2`...\n",
    "\n",
    "And we will use a decorator called `@dataclass` which provides a decorator and functions for automatically adding generated special methods such as `__init__()` and `__repr__()` to user-defined classes...\n",
    "\n",
    "And we will use it to define all the hyper-parameters as a Class called `GPTConfiguration`...\n",
    "\n",
    "Now because we are going to be implementing the `124M GPT-2 Model`, when we go to the paper we see these hyper-parameters:\n",
    "1. block-size (context window) → 1024\n",
    "2. vocabulary-size (token vocabulary) → 50257\n",
    "3. n-layer (number of layers) → 12\n",
    "4. n-head (number of self-attention heads) → 12\n",
    "5. embedding-dimensions ($d_{model}$) → 768\n",
    "\n",
    "So let's implement this now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now our code looks like this:\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# GPT configuration hyper-parameters\n",
    "@dataclass\n",
    "class GPTConfiguration:\n",
    "    blockSize: int = 1024\n",
    "    vocabularySize: int = 50257\n",
    "    numberOfLayers: int = 12\n",
    "    numberOfHeads: int = 12\n",
    "    numberOfEmbeddingDimensions: int = 768\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GPTModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be able to use this configuration under the `GPTModel` class that we are going to write...\n",
    "\n",
    "For now our empty `GPTModel` class looks like this:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.configuration = configuration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to **copy** the schema from the **Hugging Face `GPT-2` model** by utilizing the `huggingfaceStateDictionary`...\n",
    "\n",
    "And here's what I came up with...\n",
    "\n",
    "We see that the container in the schema is called `transformer` which contains all the modules and we can create something like that using `torch.nn.ModuleDict` which is just a dictionary of torch `Module`(s) which let's us index into `Module`(s) using **keys**, just like a normal python dictionary...\n",
    "\n",
    "Within that we can create something called `wordTokenEmbeddings` which corresponds with `wte` and create something called `wordPositionalEmbeddings` which corresponds with `wpe`, and we can match the shapes and create our initial layers...\n",
    "\n",
    "Then in the **Hugging Face `GPT-2` model** we see that we have a long list of **hidden** layers represented by a `.h` and followed by a range of number `.0` to `.11` hinting us about the number of layers as `12`, so we can now utilize our `numberOfLayers` hyper-parameter to construct these long list of layers. And instead of a `torch.nn.ModuleDict` we can use a `torch.nn.ModuleList` instead, which is just a list of `Module`(s).\n",
    "\n",
    "The important thing to note is, in those hidden layers we see different kinds of layer **weights** and **biases** of different **layers** all having their own shapes and sizes, but we do see a pattern that they repeat themselves in terms of **layer number**, so for now we can just consider these **layers** as `Block`(s) and iterate them through a list and return itself to the list. Keep in mind that the `Block`'s defination has not been defined yet, and we will define it later, but we want all the `Block`(s) to take in the same `configuration` object and construct the layer objects through it, because we already have all the hyper-parameters set inside it...\n",
    "\n",
    "Now that we have our long list of **hidden layers** it is time to construct the final **layer normalization** layer according to the `GPT-2` paper, so we can create something like `finalLayerNorm` and match the shapes which corresponds to the `ln_f`...\n",
    "\n",
    "And lastly we can construct our **final classifier** (or the **language model head**) which is just a **Linear Layer** that projects all the **embeddings** to their respective **tokens**, having **no bias**. So, we can easily construct this **languageModelingHead** which corresponds to the `lm_head` and finish with our skeleton of the `GPT-2` model...\n",
    "\n",
    "Now we end up with a code like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# GPT configuration hyper-parameters\n",
    "@dataclass\n",
    "class GPTConfiguration:\n",
    "    blockSize: int = 1024\n",
    "    vocabularySize: int = 50257\n",
    "    numberOfLayers: int = 12\n",
    "    numberOfHeads: int = 12\n",
    "    numberOfEmbeddingDimensions: int = 768\n",
    "\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.configuration = configuration\n",
    "\n",
    "        self.transformer = torch.nn.ModuleDict(dict(\n",
    "            wordTokenEmbeddings = torch.nn.Embedding(configuration.vocabularySize, configuration.numberOfEmbeddingDimensions),\n",
    "            wordPositionalEmbeddings = torch.nn.Embedding(configuration.blockSize, configuration.numberOfEmbeddingDimensions),\n",
    "            hidden = torch.nn.ModuleList(Block(configuration) for _ in range(configuration.numberOfLayers)),\n",
    "            finalLayerNorm = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        ))\n",
    "\n",
    "        self.languageModelingHead = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, configuration.vocabularySize, bias=False)\n",
    "\n",
    "model = GPTModel(GPTConfiguration())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Block`(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the `Block` class which is currently undefined...\n",
    "\n",
    "Now, here the `Block` refers to the `Transformer Block` that gets repeated again and again as hidden layers...\n",
    "\n",
    "Now, according to our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch</a> notebook, we already defined this `Block`, and as we mentioned `GPT-2` also has a slightly modified `Transformer Block`...\n",
    "\n",
    "For now we are going to use this template:\n",
    "```python\n",
    "# Transformer Block\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For, now we understand that we are left with the following modules & properties:\n",
    "1. Add & Norm\n",
    "2. Attention\n",
    "3. Feed Forward Network\n",
    "4. Residual Pathways\n",
    "\n",
    "Let's start with **Addition and Normalization (Add & Norm)**.\n",
    "\n",
    "According to the diagram, the **Add & Norm** is there **AFTER** the **Attention & Feed Forward Network**, but in our case we will use them **BEFORE** the **Attention & Feed Forward Network**, making it a **Pre-Normalization (Pre-Norm)**...\n",
    "\n",
    "Then we have our **Attention** module, and for now we can relate it to the attention we built in our last <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch</a> notebook. Specifically, we built two modules (`MultiHeadAttention` and `Head`), but here we will implement both modules in a combined and mathematically optimized class called `CausalSelfAttention`...\n",
    "\n",
    "Then we have our **Feed Forward Network**, and we will call it our **MultiLayerPerceptron**.\n",
    "\n",
    "The thing to note is, once again two of the modules (`CausalSelfAttention` and `MultiLayerPerceptron`) remain undefined, and we are going to define them later...\n",
    "\n",
    "For now, we end up with a code like this:\n",
    "```python\n",
    "# Transformer Block\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.layerNormalization1 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.attention = CausalSelfAttention(configuration)\n",
    "        self.layerNormalization2 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.multiLayerPerceptron = MultiLayerPerceptron(configuration)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "```\n",
    "\n",
    "And lastly, we arrive at the **Residual Pathways**, and we see that the normalizations are **inside** the residual stream, or in other words, the residual pathway has normalizations **inside** them (which is not very good or desirable from an optimization perspective) and we actually prefer to have a single and clean residual stream all the way from **supervision** to all the way to the **inputs (or `tokens`)**, which is desirable because the gradients that flow from the top distributes the gradients equally because of additions, indicating that the gradients from the top flow straight to the inputs through the residual pathway (unchanged) but then addition to that, the gradient also flows through the blocks and the blocks contribute their own contribution over time when the optimization kicks in.\n",
    "\n",
    "Which means that we want to apply a **clean residual pathway**. And to do that we need the normalization to be applied to the residual pathway (result of layer normalization) **before** adding it back to the original **inputs**. This ensures that the residual connections are additive and do not interfere with the normalization process, facilitating better gradient flow and optimization stability.\n",
    "\n",
    "Therefore, we end up with a code like this:\n",
    "```python\n",
    "# Transformer Block\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.layerNormalization1 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.attention = CausalSelfAttention(configuration)\n",
    "        self.layerNormalization2 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.multiLayerPerceptron = MultiLayerPerceptron(configuration)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs + self.attention(self.layerNormalization1(inputs))\n",
    "        inputs = inputs + self.multiLayerPerceptron(self.layerNormalization2(inputs))\n",
    "        return inputs\n",
    "```\n",
    "\n",
    "Here's how the residual pathways are structured:\n",
    "- **Attention Layer**: After applying `self.layerNormalization1`, the residual (inputs) are added to `attention_output`. This adheres to the clean residual pathway because the normalization (`layerNormalization1`) is applied **before** adding to inputs.\n",
    "- **MLP Layer**: Similarly, after applying `self.layerNormalization2`, the residual (inputs) is added to `mlp_output`. This also adheres to the clean residual pathway for the same reason as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one more thing that is interesting to note is that, **Attention** is a **communication operation**, and it is where all the tokens line-up in a sequence and this is where the tokens communicate and exchange information. And **Attention** is an **aggregation function**, it's a **pooling function**, it's a **weighted sum function**, it's a **reduce operation**.\n",
    "\n",
    "Whereas, **Multi Layer Perceptron** happens at every single token individually (**mapped**), and there is no information being exchanged or collected between the tokens.\n",
    "\n",
    "So, the **Attention** is the **reduce** and **Multi Layer Perceptron** is the **map**. And what we end up with is a repeated application of **Map-Reduce**. And this is where the `tokens` communicate and this is where they *think* individually about the information that they gathered. And every one of these blocks, iteratively refines the representation inside the residual stream...\n",
    "\n",
    "And now we can move on the to implementation of `CausalSelfAttention` and `MultiLayerPerceptron`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MultiLayerPerceptron`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move on to the `MultiLayerPerceptron (MLP)`, and I implemented the class as follows...\n",
    "\n",
    "```python\n",
    "# Multi Layer Perceptron (MLP)\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "```\n",
    "\n",
    "It is relatively straight forward. For now, we just have two `Linear` layers which wrap around a `GELU` non-linearity layer. So our block now becomes something like this:\n",
    "```python\n",
    "# Multi Layer Perceptron (MLP)\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.currentFullyConnected = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, 4 * configuration.numberOfEmbeddingDimensions)\n",
    "        self.gelu = torch.nn.GELU(approximate=\"tanh\")\n",
    "        self.currentProjection = torch.nn.Linear(4 * configuration.numberOfEmbeddingDimensions, configuration.numberOfEmbeddingDimensions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.currentFullyConnected(inputs)\n",
    "        inputs = self.gelu(inputs)\n",
    "        inputs = self.currentProjection(inputs)\n",
    "        return inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we swing over to the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\">GELU PyTorch Documentation</a>, we see **two** different `GELU`(s) being hinted there:\n",
    "1. Original GELU formulation (We will discuss this in a bit):\n",
    "   $$ \\text{GELU}(x) = x \\cdot \\Phi(x) $$\n",
    "2. Approximate GELU formulation:\n",
    "   $$ \\text{GELU}(x) = 0.5 \\cdot x \\cdot \\left(1 + \\tanh \\left( \\frac{2}{\\pi} \\cdot \\left(x + 0.044715 \\cdot x^3 \\right) \\right) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GELU](ExplanationMedia/Images/GELU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a preview, we can see that `GELU` is basically like a `ReLU`, except there's **no exactly flat tail at exactly `0`**. Otherwise, it just looks more like a slightly *smoother* `ReLU`. And it comes from this paper <a href=\"https://arxiv.org/abs/1606.08415\">\"Gaussian Error Linear Units (GELUs)\"</a> and there's a little bit of history here and I also invite you to step through the paper if you'd like. But for now, we will use the **approximate** version of the `GELU`, because that's what `GPT-2` in their model used..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one other reason of why we prefer to use `GELU` is that, in previous notebooks we have spoken about the **Dead-ReLU-Neuron-Problem** where, in the tail of a `ReLU`, where it's exactly flat at `0`, any activations that fall there will get exactly `0` gradient (meaning that there's no change, there's no adaptation, there's no development of the network), but `GELU` always contributes to a **local-gradient** and so there's always going to be a change and there's always going to be an adaptation in a *smoothed-out* way which empirically working better, as demonstrated in the paper.\n",
    "\n",
    "And we also followed the rule of *\"Position-wise Feed-Forward Networks\"* section of the original \"Attention-is-all-you-need\" paper, which is why we have the `4 * numberOfEmbeddingDimensions` in the shapes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can now move on to implement the `CausalSelfAttention` part of the code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CausalSelfAttention`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start implementing our `CausalSelfAttention` block which is none other than the combination of **Scaled Dot-Product Attention** and **Multi-Head Attention**...\n",
    "\n",
    "For now we have a skeleton like this:\n",
    "```python\n",
    "# Causal Self Attention (Scaled Dot-Product Attention + Multi-Head Attention)\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch notebook</a>, that **Multi-Head Attention** is just multiple **Scaled Dot-Product Attention**(s) running in parellel and their outputs are just being concatenated and that becomes the output.\n",
    "\n",
    "Instead, we do a bunch of tensor *gymnastics* of mathematical operations of the same logic used behind both these **Multi-Head Attention** & **Scaled Dot-Product Attention** modules in a single block. But fundamentally, and algorithmically, nothing is different from what we implemented previously...\n",
    "\n",
    "And this is what we end up with:\n",
    "```python\n",
    "# Causal Self Attention (Scaled Dot-Product Attention + Multi-Head Attention)\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        assert configuration.numberOfEmbeddingDimensions % configuration.numberOfHeads == 0\n",
    "        self.causalAttention = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, 3 * configuration.numberOfEmbeddingDimensions)\n",
    "        self.causalProjection = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, configuration.numberOfEmbeddingDimensions)\n",
    "        self.numberOfHeads = configuration.numberOfHeads\n",
    "        self.numberOfEmbeddingDimensions = configuration.numberOfEmbeddingDimensions\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(configuration.blockSize, configuration.blockSize)).view(1, 1, configuration.blockSize, configuration.blockSize))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        B, T, C = inputs.size()\n",
    "        query_key_value = self.causalAttention(inputs)\n",
    "        query, key, value = query_key_value.split(self.numberOfEmbeddingDimensions, dim=2)\n",
    "        query = query.view(B, T, self.numberOfHeads, C // self.numberOfHeads).transpose(1, 2)\n",
    "        key = key.view(B, T, self.numberOfHeads, C // self.numberOfHeads).transpose(1, 2)\n",
    "        value = value.view(B, T, self.numberOfHeads, C // self.numberOfHeads).transpose(1, 2)\n",
    "        attention = (query @ key.transpose(-2, -1)) * (1.0 / math.sqrt(key.size(-1)))\n",
    "        attention = attention.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        outputs = attention @ value\n",
    "        outputs = outputs.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        outputs = self.causalProjection(outputs)\n",
    "        return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Weights Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the skeleton ready, we can now move on to transfer the weights of the `Hugging Face GPT-2` to our `Custom GPT-2`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `from_pretrained()` - Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by understanding what we want to do first...\n",
    "\n",
    "We want to have a `from_pretrained()` method in our `GPTModel` class, that will transfer the weights for any kind of model we pass it (among the `4` models that are there in `GPT-2`), and copy the weights of each of those parameters and ensure their sizes and shapes match perfectly...\n",
    "\n",
    "And we also want our `from_pretrained()` method to be decorated by a `@classmethod` such that it could be accessed directly using the class reference and it is also able to modify the state of the class and return the appropriate model along with their appropriate parameters...\n",
    "\n",
    "So for now we can have a skeleton like this:\n",
    "```python\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        ... # Rest of the code\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained gpt: %s\" % modelType)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `from_pretrained()` - Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the separate configuartion arguements for all `4` `GPT-2` configurations...\n",
    "\n",
    "And our code will look like this:\n",
    "```python\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        ... # Rest of the code\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained gpt: %s\" % modelType)\n",
    "\n",
    "        # Creating separate configurations for separate GPT-2 models\n",
    "        configurationArguements = {\n",
    "            'gpt2':         dict(numberOfLayers=12, numberOfHeads=12, numberOfEmbeddingDimensions=768),  # 124M parameters\n",
    "            'gpt2-medium':  dict(numberOfLayers=24, numberOfHeads=16, numberOfEmbeddingDimensions=1024), # 350M parameters\n",
    "            'gpt2-large':   dict(numberOfLayers=36, numberOfHeads=20, numberOfEmbeddingDimensions=1280), # 774M parameters\n",
    "            'gpt2-xl':      dict(numberOfLayers=48, numberOfHeads=25, numberOfEmbeddingDimensions=1600), # 1558M parameters\n",
    "        }[modelType]\n",
    "        configurationArguements['vocabularySize'] = 50257\n",
    "        configurationArguements['blockSize'] = 1024\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can unpack our configurations into a variable called `configuration` based on the `modelType` arguement we pass as a parameter. And then we can initialize our `GPTModel` based on the `configuration` that we initialize. And then we can also copy the state-dictionary containing all the layers in our model in a variable called `stateDictionary` with the method `state_dict()` and unpack it's keys using the `keys()` method into a variable called `stateDictionaryKeys` (We have to keep in mind that we discard all the buffers that are not a part of the parameters like **Attention Mask** and **Attention Bias**)...\n",
    "\n",
    "So, now we have a code like this:\n",
    "```python\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        ... # Rest of the code\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # Creating separate configurations for separate GPT-2 models\n",
    "        configurationArguements = {\n",
    "            'gpt2':         dict(numberOfLayers=12, numberOfHeads=12, numberOfEmbeddingDimensions=768),  # 124M parameters\n",
    "            'gpt2-medium':  dict(numberOfLayers=24, numberOfHeads=16, numberOfEmbeddingDimensions=1024), # 350M parameters\n",
    "            'gpt2-large':   dict(numberOfLayers=36, numberOfHeads=20, numberOfEmbeddingDimensions=1280), # 774M parameters\n",
    "            'gpt2-xl':      dict(numberOfLayers=48, numberOfHeads=25, numberOfEmbeddingDimensions=1600), # 1558M parameters\n",
    "        }[modelType]\n",
    "        configurationArguements['vocabularySize'] = 50257\n",
    "        configurationArguements['blockSize'] = 1024\n",
    "\n",
    "        configuration = GPTConfiguration(**configurationArguements)\n",
    "        model = GPTModel(configuration)\n",
    "\n",
    "        stateDictionary = model.state_dict()\n",
    "        stateDictionaryKeys = stateDictionary.keys()\n",
    "        stateDictionaryKeys = [key for key in stateDictionaryKeys if not key.endswith('.attention.bias')]\n",
    "\n",
    "        return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `from_pretrained()` - Hugging Face Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our own custom model completely initialized, we can go ahead and initialize the `Hugging Face GPT-2 Model` into a single variable and it's state dictionary and keys into other variables called `huggingfaceStateDictionary` and `huggingfaceStateDictionaryKeys`...\n",
    "\n",
    "And then we can start copying the weights after **ignoring the buffers**. But now, before copying we have to keep in mind that the original code for the `GPT-2` model was trained using the `TensorFlow` library and some of the weights are **transposed** in that architecture, so we will manually hard-code those weights and **copy them after transposing them to their original PyTorch form**...\n",
    "\n",
    "One last thing to be careful about is, in our model we are using **custom names for our variables** but the `Hugging Face GPT-2 Model` has an architecture of **short forms**, so it is better to have a `parameterKeyMapping` that **maps our custom keys with the Hugging Face GPT-2 keys of the state-dictionary** such that it becomes much easier to iterate through...\n",
    "\n",
    "So, now we have a final code like this:\n",
    "```python\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        ... # Rest of the code\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained gpt: %s\" % modelType)\n",
    "        \n",
    "        # Creating separate configurations for separate GPT-2 models\n",
    "        blockSize = 1024\n",
    "        vocabularySize = 50257\n",
    "        configurationArguements = {\n",
    "            'gpt2':         dict(numberOfLayers=12, numberOfHeads=12, numberOfEmbeddingDimensions=768),  # 124M parameters\n",
    "            'gpt2-medium':  dict(numberOfLayers=24, numberOfHeads=16, numberOfEmbeddingDimensions=1024), # 350M parameters\n",
    "            'gpt2-large':   dict(numberOfLayers=36, numberOfHeads=20, numberOfEmbeddingDimensions=1280), # 774M parameters\n",
    "            'gpt2-xl':      dict(numberOfLayers=48, numberOfHeads=25, numberOfEmbeddingDimensions=1600), # 1558M parameters\n",
    "        }[modelType]\n",
    "        configurationArguements['vocabularySize'] = 50257\n",
    "        configurationArguements['blockSize'] = 1024\n",
    "\n",
    "        configuration = GPTConfiguration(**configurationArguements)\n",
    "        model = GPTModel(configuration)\n",
    "        stateDictionary = model.state_dict()\n",
    "        stateDictionaryKeys = stateDictionary.keys()\n",
    "        stateDictionaryKeys = [key for key in stateDictionaryKeys if not key.endswith('.attention.bias')]\n",
    "\n",
    "        huggingfaceModel = GPT2LMHeadModel.from_pretrained(modelType)\n",
    "        huggingfaceStateDictionary = huggingfaceModel.state_dict()\n",
    "        huggingfaceStateDictionaryKeys = huggingfaceStateDictionary.keys()\n",
    "        huggingfaceStateDictionaryKeys = [key for key in huggingfaceStateDictionaryKeys if not key.endswith('.attn.masked_bias')]\n",
    "        huggingfaceStateDictionaryKeys = [key for key in huggingfaceStateDictionaryKeys if not key.endswith('.attn.bias')]\n",
    "        transposedParameters = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        assert len(huggingfaceStateDictionaryKeys) == len(stateDictionaryKeys), f\"Mismatched Keys: {len(huggingfaceStateDictionaryKeys)} != {len(stateDictionaryKeys)}\"\n",
    "\n",
    "        parameterKeyMapping = {\n",
    "            customKey: huggingfaceKey\n",
    "            for customKey, huggingfaceKey in zip(stateDictionaryKeys, huggingfaceStateDictionaryKeys)\n",
    "            }\n",
    "\n",
    "        for customKey, huggingfaceKey in parameterKeyMapping.items():\n",
    "            if (huggingfaceStateDictionary[huggingfaceKey].shape != stateDictionary[customKey].shape):\n",
    "                # Special treatment for the Conv1D weights (Transposed Weights)\n",
    "                if (huggingfaceKey.endswith(word) for word in transposedParameters):\n",
    "                    assert huggingfaceStateDictionary[huggingfaceKey].shape[::-1] == stateDictionary[customKey].shape\n",
    "                    with torch.no_grad():\n",
    "                        stateDictionary[customKey].copy_(huggingfaceStateDictionary[huggingfaceKey].t())\n",
    "            # Vanilla copy for other parameters\n",
    "            else:\n",
    "                assert huggingfaceStateDictionary[huggingfaceKey].shape == stateDictionary[customKey].shape\n",
    "                with torch.no_grad():\n",
    "                    stateDictionary[customKey].copy_(huggingfaceStateDictionary[huggingfaceKey])\n",
    "        \n",
    "        return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as we do this, you will see that it will start downloading the model from Hugging Face like this:\n",
    "```bash\n",
    "Loading weights from pretrained gpt: gpt2\n",
    "config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 665/665 [00:00<?, ?B/s]\n",
    "model.safetensors:  34%|████████████████████████████████████████████████████▋                                                                                                    | 189M/548M [00:18<00:35, 10.1MB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done we can now move on to the generation phase of the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Forward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we can generate from this model that we have implemented, we need a forward function that forwards the token sequence through the model to get the logits...\n",
    "\n",
    "Now the inputs are none other than the `token indeces` that is represented in a shape of `(Batch, Time)` tensor, where `Batch` dimension is the **independent sequences** and `Time` dimension is the **maximum sequence length**. Meaning that we have inputs in the shape of a matrix with each row having independent sequences of a maximum length of a sequence.\n",
    "\n",
    "So, we first unpack the `Batch` and `Time` dimensions in a variable. Then we forward the `positional embeddings` by creating a different tensor as `tokenPositions` which is none other than the `(0, Time)` dimension of the `token indeces`...\n",
    "\n",
    "Then we forward the `positional embeddings` and the `token embeddings` and when we get their respective outputs, we will concatenate them in a variable called `inputs`.\n",
    "\n",
    "And lastly, we will loop through every block of the `transformer` and forward the `inputs` through them, and finally forward them through the final `layer normalization` and `language modeling head` to get the logits...\n",
    "\n",
    "So now we have a forward loop inside the `GPTModel` class like this:\n",
    "```python\n",
    "def forward(self, indeces):\n",
    "    Batch, Time = indeces.size()\n",
    "    assert T <= self.configuration.blockSize, f\"Cannot forward sequence of length {Time}, Block Size is only {self.configuration.blockSize}\"\n",
    "\n",
    "    tokenPositions = torch.arange(0, Time, dtype=torch.long)\n",
    "    positionalEmbeddings = self.transformer.wordPositionalEmbeddings(tokenPositions)\n",
    "    tokenEmbeddings = self.transformer.wordTokenEmbeddings(indeces)\n",
    "    inputs = tokenEmbeddings + positionalEmbeddings\n",
    "\n",
    "    for block in self.transformer.hidden:\n",
    "        inputs = block(x)\n",
    "    inputs = self.transformer.finalLayerNorm(inputs)\n",
    "    logits = self.languageModelingHead(inputs)\n",
    "    return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to use this model on the `GPU`, so for now we can change the `tokenPositions` that we create to and specify the correct device like this:\n",
    "`tokenPositions = torch.arange(0, Time, dtype=torch.long, device=indeces.device)`.\n",
    "\n",
    "And then we can move on to the generation phase of the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we generate from the model, we need to think through what we will generate from the model and what will go through the inputs...\n",
    "\n",
    "We will firstly put the model into `eval()` mode because the even though we don't have layers that uses different mechanisms during training time and inference time, it is a good practice to keep the model change it's state if we do any further changes to the model.\n",
    "\n",
    "Well, we will forward `encoded tokens` into the model and get `encoded tokens` as output which we need to decode again to see the generation. Now, these tokens will have two hyper-parameters that we will define, one being the `maximum generation length` (denotes how much tokens will the model generate for each independant token sequence) and one being the `number of sequences to generate` (denotes the number of sequences we are trying to generate in a single run in parallel).\n",
    "\n",
    "You might be wondering \"where suddenly these `encoded tokens` are coming from?\".\n",
    "\n",
    "To answer this, we can go back to the <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20Tokenizer.ipynb\">GPT Tokenizer</a> notebook and refer to it to understand that we will use a library to encode and decode tokens called `tiktoken`...\n",
    "\n",
    "And we will encode the same sequence:\n",
    "> \"Hello, I'm a language model,\"\n",
    "\n",
    "And if we go to <a href=\"https://tiktokenizer.vercel.app/?model=gpt2\">tiktokenizer</a> and see the token sequence size, we will see that there are `8` tokens in this sequence...\n",
    "\n",
    "So, we can now create a tensor called `tokens` which will be of shape `(numberOfSequences, numberOfTokensInSequence)` (Let's say we want to generate `5` sequences, then the shape of this `tokens` tensor will be of shape `(5, 8)`).\n",
    "\n",
    "Then we will create loop with `maximumGenerationLength` and forward the `encoded tokens` to get the `logits`. Now, because the `logits` will be of shape `(Batch, Time, Channel)` where `Batch` represents the **independent sequences**, `Time` represents the **tokens in a sequence** and `Channel` represents the **vocabulary that `logits` will classify into**, we will select only the **last** generated `Time` dimension from the `logits` tensor and pass them through a `softmax()` to get the `probabilites`...\n",
    "\n",
    "Now, by default the Hugging Face pipeline uses **top-k probabilites of 50 by default**, so, we will also implement this in our loop by using PyTorch's `topk()` method and pass in our probabilites and specify the correct dimension. This `topk()` method will then return the final probabilites (`topKProbabilites`) and the indeces of these probabilites(`tokKIndeces`). **This helps us to never sample very rare tokens.**\n",
    "\n",
    "Then we can sample a token from the `multinomial distribution` of these `topKProbabilites` and get the correct `tokenIndeces` (which is a single sampled `token` in a batch). Then we can create a column of `tokenIndeces` using <a href=\"https://pytorch.org/docs/stable/generated/torch.gather.html\">`torch.gather()`</a> and append them on the original `tokens` to have our generated tokens in a sequence...\n",
    "\n",
    "We will also keep this entire section inside the loop into `torch.no_grad()` to let PyTorch know that we won't be needing any backward processing (gradient calculation and intermediate operation caching) to save us some memory and hopefully some time...\n",
    "\n",
    "And finally, when we have our `encoded tokens in a sequence` which will be in the shape of `(numberOfSequences, maximumGenerationLength)` and we can decode them using a loop based on `numberOfSequences`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we end up with a final code like this:\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import tiktoken\n",
    "\n",
    "# Causal Self Attention (Scaled Dot-Product Attention + Multi-Head Attention)\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        assert configuration.numberOfEmbeddingDimensions % configuration.numberOfHeads == 0\n",
    "        self.causalAttention = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, 3 * configuration.numberOfEmbeddingDimensions)\n",
    "        self.causalProjection = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, configuration.numberOfEmbeddingDimensions)\n",
    "        self.numberOfHeads = configuration.numberOfHeads\n",
    "        self.numberOfEmbeddingDimensions = configuration.numberOfEmbeddingDimensions\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(configuration.blockSize, configuration.blockSize)).view(1, 1, configuration.blockSize, configuration.blockSize))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        B, T, C = inputs.size()\n",
    "        query_key_value = self.causalAttention(inputs)\n",
    "        query, key, value = query_key_value.split(self.numberOfEmbeddingDimensions, dim=2)\n",
    "        query = query.view(B, T, self.numberOfHeads, C // self.numberOfHeads).transpose(1, 2)\n",
    "        key = key.view(B, T, self.numberOfHeads, C // self.numberOfHeads).transpose(1, 2)\n",
    "        value = value.view(B, T, self.numberOfHeads, C // self.numberOfHeads).transpose(1, 2)\n",
    "        attention = (query @ key.transpose(-2, -1)) * (1.0 / math.sqrt(key.size(-1)))\n",
    "        attention = attention.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        outputs = attention @ value\n",
    "        outputs = outputs.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        outputs = self.causalProjection(outputs)\n",
    "        return outputs\n",
    "\n",
    "# Multi Layer Perceptron (MLP)\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.currentFullyConnected = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, 4 * configuration.numberOfEmbeddingDimensions)\n",
    "        self.gelu = torch.nn.GELU(approximate=\"tanh\")\n",
    "        self.currentProjection = torch.nn.Linear(4 * configuration.numberOfEmbeddingDimensions, configuration.numberOfEmbeddingDimensions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.currentFullyConnected(inputs)\n",
    "        inputs = self.gelu(inputs)\n",
    "        inputs = self.currentProjection(inputs)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "# Transformer Block\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.layerNormalization1 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.attention = CausalSelfAttention(configuration)\n",
    "        self.layerNormalization2 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.multiLayerPerceptron = MultiLayerPerceptron(configuration)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs + self.attention(self.layerNormalization1(inputs))\n",
    "        inputs = inputs + self.multiLayerPerceptron(self.layerNormalization2(inputs))\n",
    "        return inputs\n",
    "\n",
    "# GPT configuration hyper-parameters\n",
    "@dataclass\n",
    "class GPTConfiguration:\n",
    "    blockSize: int = 1024\n",
    "    vocabularySize: int = 50257\n",
    "    numberOfLayers: int = 12\n",
    "    numberOfHeads: int = 12\n",
    "    numberOfEmbeddingDimensions: int = 768\n",
    "\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.configuration = configuration\n",
    "\n",
    "        self.transformer = torch.nn.ModuleDict(dict(\n",
    "            wordTokenEmbeddings = torch.nn.Embedding(configuration.vocabularySize, configuration.numberOfEmbeddingDimensions),\n",
    "            wordPositionalEmbeddings = torch.nn.Embedding(configuration.blockSize, configuration.numberOfEmbeddingDimensions),\n",
    "            hidden = torch.nn.ModuleList(Block(configuration) for _ in range(configuration.numberOfLayers)),\n",
    "            finalLayerNorm = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        ))\n",
    "\n",
    "        self.languageModelingHead = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, configuration.vocabularySize, bias=False)\n",
    "    \n",
    "    def forward(self, indeces):\n",
    "        Batch, Time = indeces.size()\n",
    "        assert Time <= self.configuration.blockSize, f\"Cannot forward sequence of length {Time}, Block Size is only {self.configuration.blockSize}\"\n",
    "\n",
    "        tokenPositions = torch.arange(0, Time, dtype=torch.long, device=indeces.device)\n",
    "        positionalEmbeddings = self.transformer.wordPositionalEmbeddings(tokenPositions)\n",
    "        tokenEmbeddings = self.transformer.wordTokenEmbeddings(indeces)\n",
    "        inputs = tokenEmbeddings + positionalEmbeddings\n",
    "\n",
    "        for block in self.transformer.hidden:\n",
    "            inputs = block(inputs)\n",
    "        inputs = self.transformer.finalLayerNorm(inputs)\n",
    "        logits = self.languageModelingHead(inputs)\n",
    "        return logits\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained GPT: %s\" % modelType)\n",
    "        \n",
    "        # Creating separate configurations for separate GPT-2 models\n",
    "        blockSize = 1024\n",
    "        vocabularySize = 50257\n",
    "        configurationArguements = {\n",
    "            'gpt2':         dict(numberOfLayers=12, numberOfHeads=12, numberOfEmbeddingDimensions=768),  # 124M parameters\n",
    "            'gpt2-medium':  dict(numberOfLayers=24, numberOfHeads=16, numberOfEmbeddingDimensions=1024), # 350M parameters\n",
    "            'gpt2-large':   dict(numberOfLayers=36, numberOfHeads=20, numberOfEmbeddingDimensions=1280), # 774M parameters\n",
    "            'gpt2-xl':      dict(numberOfLayers=48, numberOfHeads=25, numberOfEmbeddingDimensions=1600), # 1558M parameters\n",
    "        }[modelType]\n",
    "        configurationArguements['vocabularySize'] = 50257\n",
    "        configurationArguements['blockSize'] = 1024\n",
    "\n",
    "        configuration = GPTConfiguration(**configurationArguements)\n",
    "        model = GPTModel(configuration)\n",
    "        stateDictionary = model.state_dict()\n",
    "        stateDictionaryKeys = stateDictionary.keys()\n",
    "        stateDictionaryKeys = [key for key in stateDictionaryKeys if not key.endswith('.attention.bias')]\n",
    "\n",
    "        huggingfaceModel = GPT2LMHeadModel.from_pretrained(modelType)\n",
    "        huggingfaceStateDictionary = huggingfaceModel.state_dict()\n",
    "        huggingfaceStateDictionaryKeys = huggingfaceStateDictionary.keys()\n",
    "        huggingfaceStateDictionaryKeys = [key for key in huggingfaceStateDictionaryKeys if not key.endswith('.attn.masked_bias')]\n",
    "        huggingfaceStateDictionaryKeys = [key for key in huggingfaceStateDictionaryKeys if not key.endswith('.attn.bias')]\n",
    "        transposedParameters = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        assert len(huggingfaceStateDictionaryKeys) == len(stateDictionaryKeys), f\"Mismatched Keys: {len(huggingfaceStateDictionaryKeys)} != {len(stateDictionaryKeys)}\"\n",
    "\n",
    "        parameterKeyMapping = {\n",
    "            customKey: huggingfaceKey\n",
    "            for customKey, huggingfaceKey in zip(stateDictionaryKeys, huggingfaceStateDictionaryKeys)\n",
    "            }\n",
    "\n",
    "        for customKey, huggingfaceKey in parameterKeyMapping.items():\n",
    "            if (huggingfaceStateDictionary[huggingfaceKey].shape != stateDictionary[customKey].shape):\n",
    "                # Special treatment for the Conv1D weights (Transposed Weights)\n",
    "                if (huggingfaceKey.endswith(word) for word in transposedParameters):\n",
    "                    assert huggingfaceStateDictionary[huggingfaceKey].shape[::-1] == stateDictionary[customKey].shape\n",
    "                    with torch.no_grad():\n",
    "                        stateDictionary[customKey].copy_(huggingfaceStateDictionary[huggingfaceKey].t())\n",
    "            # Vanilla copy for other parameters\n",
    "            else:\n",
    "                assert huggingfaceStateDictionary[huggingfaceKey].shape == stateDictionary[customKey].shape\n",
    "                with torch.no_grad():\n",
    "                    stateDictionary[customKey].copy_(huggingfaceStateDictionary[huggingfaceKey])\n",
    "        return model\n",
    "\n",
    "model = GPTModel.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.eval()\n",
    "model.to(device=device)\n",
    "\n",
    "# Generation\n",
    "maximumGenerationLength = 30\n",
    "numberOfSequences = 5\n",
    "\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "encodedTokens = encoder.encode(\"Hello, I'm a language model,\")\n",
    "encodedTokens = torch.tensor(encodedTokens, dtype=torch.long)\n",
    "encodedTokens = encodedTokens.unsqueeze(0).repeat(numberOfSequences, 1)\n",
    "inputs = encodedTokens.to(device=device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "while inputs.size(1) < maximumGenerationLength:\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs)\n",
    "        logits = logits[:, -1, :]\n",
    "        probabilites = F.softmax(logits, dim=-1)\n",
    "\n",
    "        topKProbabilites, tokKIndeces = torch.topk(input=probabilites, k=50, dim=-1)\n",
    "\n",
    "        tokenIndeces = torch.multinomial(input=topKProbabilites, num_samples=1)\n",
    "        columnOfTokenIndeces = torch.gather(input=tokKIndeces, dim=-1, index=tokenIndeces)\n",
    "\n",
    "        inputs = torch.cat((inputs, columnOfTokenIndeces), dim=1)\n",
    "\n",
    "for i in range(numberOfSequences):\n",
    "    tokensToDecode = inputs[i, :maximumGenerationLength].tolist()\n",
    "    decodedTokens = encoder.decode(tokensToDecode)\n",
    "    print(\">\", decodedTokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get the following generations:\n",
    "```plaintext\n",
    "Loading weights from pretrained GPT: gpt2\n",
    "> Hello, I'm a language model, as's the - a. and for they.. were also. is of -\n",
    " to/ ' can\n",
    "> Hello, I'm a language model, I (, the\n",
    " ( have \" to ( \" of are\n",
    "., ' the'sa. that\n",
    "> Hello, I'm a language model, - on the<|endoftext|> was will also's't: of or<|endoftext|>.. are is to he, is the\n",
    "> Hello, I'm a language model, or \"\n",
    " a will. will the the.. The and in.,- ofThe's- for\n",
    "> Hello, I'm a language model,.. willThe's. and was and, was I would of a's his's's's. or\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, comes the interesting part... We want to initialize everything from scratch... We don't want to use any of these weights, and we want to use random numbers and initialize them and train them and generate from them...\n",
    "\n",
    "So let's now move on to the next part of this notebook..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Model to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in case you do not have a `GPU` available, you can still follow along the notebook to some extent, but probably not to the very end because we will be actually using multiple `GPU`(s) and an actually perform a serious training run, but for now you can actually follow along with the notebook...\n",
    "\n",
    "And the one thing that I'd like to do is to *auto-detect* the **device** that is available to you and run the code on the highest compute capability...\n",
    "\n",
    "And you can do that with a code like this:\n",
    "```python\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using Device: {device}\")\n",
    "```\n",
    "\n",
    "We see that by default, the device is the `CPU` which is available everywhere, but then, we can detect the `GPU` using `CUDA`, and then if we don't have a `CUDA` we can detect if it atleast has `MPS` which is the backend for `Apple Silicon` (Newer Macbook Models)...\n",
    "\n",
    "And once we have this `device` we can potentially use this in the model:\n",
    "```python\n",
    "model.to(device=device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll remember that we used `tokenPositions` by carefully setting the device to `device=indeces.device` in the `GPTModel`, and we did this to carefully set the location of initialization of this tensor to the correct device to **prevent the device mis-match**...\n",
    "\n",
    "Now, I do want to loop back around to this section to 'what it means to have different devices in PyTorch, and what it is exactly that PyTorch does in the background when we do something like `model.to(device=device)` and how it works', but for now we'd like to get to training of this model and we'd like to start training the model and for now let's just say the `device` makes the code go fast..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset and Encoding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we are initializing our weights at random already because PyTorch already initializes our layers randomly and by default...\n",
    "\n",
    "Right now for the Hugging Face model initialization we are using the code:\n",
    "```python\n",
    "model = GPTModel.from_pretrained('gpt2')\n",
    "```\n",
    "But if we want to use our default initialization we can use our older code:\n",
    "```python\n",
    "model = GPTModel(GPTConfiguration())\n",
    "```\n",
    "\n",
    "And for now if we try to run our code it *blabbers* garbage like this:\n",
    "```plaintext\n",
    "Using Device: cpu\n",
    "> Hello, I'm a language model,FCConnect Sandwich 64 Seed SHARparam Bloodyivil Sketch arrang Deaths backdoor Steeledoorccording bathingParentiven revers cafeteria trustees\n",
    "> Hello, I'm a language model, Brenblescler Awakens55 collar foe COUNboarding70710erry Evidence promotionMeet Icononticient Copobyl survivor Advoc Gro\n",
    "> Hello, I'm a language model, asylumacaninventoryQuantity physician OHBillyhirt controlling doctrines Summers wallet disdain Test repercussions Nighthew Goblinbreeding flight amuse Most paradox\n",
    "> Hello, I'm a language model,Double lendersortion book appetite Times complaint regulationokinglyrelease vans351specialRail Fal Faustmessage()); Vo BryceHeightuserc\n",
    "> Hello, I'm a language model,regonUU enc trouble correctly dentist weekends involve Spirit Cars benef assessing sporadic mattress Neckliterallyributes** awkwardly canned contingは\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'd like to start training the model, and to train the model we are going to need some dataset, and for me the best and simplest debugging dataset that I like to use is the `Harry_Potter_Books.txt` dataset and it's available at this <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/Datasets/Harry_Potter_Books.txt\">URL</a>...\n",
    "\n",
    "And if you're not moving the files around in this repository, you will see that it is availble under the `Datasets/Harry_Potter_Books.txt` and I already have this on the local system...\n",
    "\n",
    "And now we can read the entire text that we have here in this dataset by using this code:\n",
    "```python\n",
    "with open(\"Datasets/Harry_Potter_Books.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "```\n",
    "And in order to produce tokens from our dataset, we can use the `GPT-2 Tokenizer from TikToken` and convert our dataset into a list of `tokens` like this:\n",
    "```python\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "encodedDataTokens = encoder.encode(text)\n",
    "```\n",
    "\n",
    "And now we actually want to process these token sequences and feed them into a transformer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we actually want to process these token sequences and feed them into a transformer...\n",
    "\n",
    "And in particular, we want to rearrange these tokens into the `indeces` variable that's available inside the `GPTModel`'s `forward()` function...\n",
    "\n",
    "So, we don't want a single very long one-dimensional sequence, instead, we want entire `Batch`(s) of `Time` sequences (where `Time` is the **maximum-sequence-length** or the **context-window**).\n",
    "\n",
    "Let's get to understand what we want with a small *toy-example* now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a tensor like this:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "buffer = torch.randint(low=0, high=50257, size=(24, ))\n",
    "print(buffer)\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "tensor([ 9360, 29278,  3940, 26797, 47584,  1285, 28358, 45295, 29845, 38908,\n",
    "        35303, 48343,  2579, 34456, 15560,  6453, 10159, 28005, 11891,  3940,\n",
    "        33806, 27357, 36749, 40952])\n",
    "```\n",
    "Here, each item in the above tensor represents the `token` in a sequence...\n",
    "\n",
    "And if we wanted to create batches out of it, we can use `view()` from PyTorch to stack up incremental parts of the tensor in a tensor like this:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "buffer = torch.randint(low=0, high=50257, size=(24, ))\n",
    "inputs = buffer.view(4, 6)\n",
    "\n",
    "print(inputs)\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "tensor([[ 9360, 29278,  3940, 26797, 47584,  1285],\n",
    "        [28358, 45295, 29845, 38908, 35303, 48343],\n",
    "        [ 2579, 34456, 15560,  6453, 10159, 28005],\n",
    "        [11891,  3940, 33806, 27357, 36749, 40952]])\n",
    "```\n",
    "But even if this is the case that we were able to make the `batches`, this **does not make sense until we know what we want to do with these `batches`**...\n",
    "\n",
    "And we want to take the next `token` in a sequence and we want them to be the `label(s)` for the current sequence for the model to train on and calculate the `loss`...\n",
    "\n",
    "We also see that for this example, for the `token` `45295` the token `29845` comes next as a `label`. But, at the same time the last `token` `40952`, we cannot determine the next `label` because we don't have any information about it...\n",
    "\n",
    "So, let me show you my favourite way to get the `label(s)`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand what we want to have...\n",
    "\n",
    "We want to have a tensor that contains the `label(s)` at every single position and is the same size as the `inputs`...\n",
    "\n",
    "And this is the way I like to do this:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "buffer = torch.randint(low=0, high=50257, size=(25, ))\n",
    "\n",
    "inputs = buffer[:-1].view(4, 6)\n",
    "labels = buffer[1:].view(4, 6)\n",
    "\n",
    "print(inputs)\n",
    "print(labels)\n",
    "```\n",
    "And we get:\n",
    "```python\n",
    "tensor([[ 9360, 29278,  3940, 26797, 47584,  1285],\n",
    "        [28358, 45295, 29845, 38908, 35303, 48343],\n",
    "        [ 2579, 34456, 15560,  6453, 10159, 28005],\n",
    "        [11891,  3940, 33806, 27357, 36749, 40952]])\n",
    "tensor([[29278,  3940, 26797, 47584,  1285, 28358],\n",
    "        [45295, 29845, 38908, 35303, 48343,  2579],\n",
    "        [34456, 15560,  6453, 10159, 28005, 11891],\n",
    "        [ 3940, 33806, 27357, 36749, 40952,  8182]])\n",
    "```\n",
    "You will see that I took a `buffer` of `25 tokens` this time instead of `24 tokens` to specify that we have a longer sequence and as `inputs` we are taking **everything excluding the last `token`** and for `labels` we are taking **everything starting from the first `token` (Offset the `tokens` by `1`)** and using `view()` to make batches of them...\n",
    "\n",
    "And we can also understand that the `buffer`'s size is $\\text{Batch} * \\text{Time} + 1$ and we are viewing the `inputs` and the `labels` as `(Batch, Time)`...\n",
    "\n",
    "So, we can now implement this in our main script now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now our script snippet looks like this:\n",
    "```python\n",
    "# Device Auto-Detection\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# Data-Loader\n",
    "with open(\"Datasets/Harry_Potter_Books.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "encodedDataTokens = encoder.encode(text)\n",
    "\n",
    "Batch, Time = 4, 32\n",
    "buffer = torch.tensor(encodedDataTokens[:Batch*Time + 1])\n",
    "inputs = buffer[:-1].view(Batch, Time)\n",
    "labels = buffer[1:].view(Batch, Time)\n",
    "\n",
    "# Constructing Model\n",
    "model = GPTModel(GPTConfiguration())\n",
    "\n",
    "model.eval()\n",
    "model.to(device=device)\n",
    "\n",
    "logits = model(inputs)\n",
    "print(logits.shape)\n",
    "\n",
    "# Halting Generation...(Will Remove Later)\n",
    "import sys; sys.exit(0)\n",
    "```\n",
    "And we get something like this:\n",
    "```python\n",
    "Using Device: cuda\n",
    "torch.Size([4, 32, 50257])\n",
    "```\n",
    "Keep in mind, that this is just a single batch... And we will modify this code later to take the entire text to load into batches and run the optimization... For now, this looks good an we can move on to calculate the loss, do the backward pass for us to run the optimization..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating `Loss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the loss first...\n",
    "\n",
    "And in order to calculate the `loss` we are going to modify the `forward()` function inside our `GPTModel` module...\n",
    "\n",
    "In particular, we are not just going to return the `logits`, but we are also going to return the `loss` for the function, and we are not just going to pass in the `input(s) indeces` for it to train, but we are also going to pass in the `label(s)`...\n",
    "\n",
    "So this old code:\n",
    "```python\n",
    "logits = model(inputs)\n",
    "```\n",
    "Becomes:\n",
    "```python\n",
    "logits, loss = model(inputs, labels)\n",
    "```\n",
    "And these `labels` will be optional because we will train our model when we have the `labels` as an input to the model otherwise we will use the `forward()` to generate `tokens` for the already implemented code that we have written in our script...\n",
    "\n",
    "So this old code:\n",
    "```python\n",
    "def forward(self, indeces):\n",
    "    ...\n",
    "    return logits\n",
    "```\n",
    "Becomes:\n",
    "```python\n",
    "def forward(self, indeces, labels=None):\n",
    "    ...\n",
    "    return logits, loss\n",
    "```\n",
    "\n",
    "And we will be calculating the `cross_entropy()` loss, and we have already discussed this in our previous notebooks, as to why we are using the `cross_entropy()` loss...\n",
    "\n",
    "But, `cross_entropy()` does not take multi-dimensional inputs. And if we remember properly, our `logits` came out in the shape `[4, 32, 50257]` which clearly is a multi-dimensional input to the function...\n",
    "\n",
    "So, we stretch the `[4, 32, 50257]` tensor of `logits` to be `[128, 50257]` which is $\\text{Batch} * \\text{Time}$ and also stretch the `labels` to be a single long tensor of `128` (or $\\text{Batch} * \\text{Time}$) and pass them as arguements in our `cross_entropy()` function like this:\n",
    "```python\n",
    "loss = None\n",
    "if labels is not None:\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "return logits, loss\n",
    "```\n",
    "And if we try to print out the loss now, we will see something like this:\n",
    "```python\n",
    "tensor(10.9503, grad_fn=<NllLossBackward0>)\n",
    "```\n",
    "Which seems fairly reasonable if we try to deduce what loss should we expect during initialization. We have a vocabulary of `50257` and if we take the probability of a single `token` and calculate the **negative log likelihood** of the probability by the formula:\n",
    "$$-\\ln{(\\frac{1}{50257})}$$\n",
    "\n",
    "This is because, at initialization you'd expect every token to get a uniform probability such that the model does not favor any `token` way too much, and we are not confidently wrong about a `token` at initialization...\n",
    "\n",
    "We have around `10.82` (which is fairly reasonable for the `loss` that we have already as an output)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can successfully move on to the optimization..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the optimization we will use an `Optimizer` object from PyTorch and we will use the `Adam` optimizer, which is the alternative to the `Stochastic Gradient Descent (SGD)` optimizer, which is a bit more evolved than the `SGD`. And specifically we will use the `AdamW` variation, which in my opinion it kind of fixes a bug.\n",
    "\n",
    "And when we go the documentation of <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\">AdamW</a> we see that it is a little bit more complecated than the `SGD` that we have used before in our previous notebooks, because in addition to obtaining the parameters with the gradient scaled by the learning rate it keeps some buffers around ($m_0 \\rightarrow \\text{ first moment and }  v_0 \\rightarrow \\text{ second moment }$), which is something that looks like momentum and something that looks like **Root Mean Square Propagation**. And it's something like a normalization that happens at each gradient individually and speeds up the optimization especially for language models...\n",
    "\n",
    "Also, the learning rate I used was `3e-4`, which is a fairly good default for most optimizations that you want to run at a very early debugging stage...\n",
    "\n",
    "So our optimizer object initialization code looks like this:\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=3e-4)\n",
    "```\n",
    "\n",
    "And to run an optimization loop we will use this sequence:\n",
    "1. Zero the gradients (because backward pass does a `+=` to the gradients and we should start with a `0` gradient)\n",
    "2. Forward the inputs to the model to get loss and the logits\n",
    "3. Complete the backward pass\n",
    "4. Step the optimizer\n",
    "\n",
    "So our old code:\n",
    "```python\n",
    "logits, loss = model(inputs, labels)\n",
    "print(loss)\n",
    "```\n",
    "Becomes:\n",
    "```python\n",
    "# Optimization\n",
    "epochs = 50\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=3e-4)\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(inputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step: {epoch}, Loss: {loss.item()}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But suddenly we get an error:\n",
    "```bash\n",
    "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! when resuming training\n",
    "```\n",
    "And we see that there's something called `cuda:0`, and that is because I have `8 GPUs` on my system and `cuda:0` is the $1^{\\text{st}}$ GPU out of `8`...\n",
    "\n",
    "But let's fix this error...\n",
    "\n",
    "It seems that the error is generating from the `buffer` that we created during batch construction, and we never moved this to the device...\n",
    "\n",
    "And we have to be careful because we can't just do `buffer.to(device=device)`, we have to do `buffer = buffer.to(device=device)` and there's a big internal reason for this...\n",
    "\n",
    "In PyTorch, when you create a tensor using `torch.tensor()`, it **returns** a new tensor on the **default device** (typically **CPU**). When you use `.to(device)` on a tensor, it **returns** a new tensor that is on the **specified device** (e.g., **GPU**), but **it does not modify the original tensor in place**...\n",
    "\n",
    "So let's run our code now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do we expect to see?\n",
    "\n",
    "We expect to see a reasonable loss in the beginning and then we continue to optimize just a **single batch**, and we want to see if we can overfit this single batch, crush this little batch and perfectly predict the indeces on just this little batch...\n",
    "\n",
    "Also, I changed our manual seed from `42` to `69`, because I like the number `69`...\n",
    "\n",
    "And this is the output I get:\n",
    "```bash\n",
    "Using Device: cpu\n",
    "Step: 0, Loss: 11.032245635986328\n",
    "Step: 1, Loss: 6.84295654296875\n",
    "Step: 2, Loss: 4.308750629425049\n",
    "Step: 3, Loss: 2.497817039489746\n",
    "...\n",
    "Step: 48, Loss: 0.003069450380280614\n",
    "Step: 49, Loss: 0.002997332951053977\n",
    "```\n",
    "And that's what we get... We get a very very low loss at the end of the optimization of a **single batch**. Or, in other words, the `transformer` network is **memorizing** this single individual batch...\n",
    "\n",
    "But now, we don't want to overfit a single batch, instead we want to run an actual optimization on actual fresh batches each time through a **data-loader**..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Loader (Considering Fresh Batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the code:\n",
    "```python\n",
    "# Data-Loader\n",
    "with open(\"Datasets/Harry_Potter_Books.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "encodedDataTokens = encoder.encode(text)\n",
    "\n",
    "Batch, Time = 4, 32\n",
    "buffer = torch.tensor(encodedDataTokens[:Batch*Time + 1])\n",
    "buffer = buffer.to(device=device)\n",
    "inputs = buffer[:-1].view(Batch, Time)\n",
    "labels = buffer[1:].view(Batch, Time)\n",
    "```\n",
    "\n",
    "And we will modify this code and convert it into a class called `DataLoaderLite` such that our code becomes cleaner and gets easier to access the fresh batches...\n",
    "\n",
    "And this `DataLoaderLite` class will take in the shape of the batch as two separate parameters `Batch` and `Time` (which for our case we used `4` and `32` respectively), and give us back the `inputs` and the `labels` as an output when we call a method called `nextBatch()` on this class's object...\n",
    "\n",
    "So, for now our emply class skeleton looks like this:\n",
    "```python\n",
    "# Data-Loader\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, Batch, Time):\n",
    "        pass\n",
    "    \n",
    "    def nextBatch(self):\n",
    "        pass\n",
    "```\n",
    "\n",
    "We can initialize the `Batch` and `Time` inside of the class to reuse the shapes now, and read the file and get the encoding inside it just like we did before, and also make sure to convert the encoded `tokens` in a tensor and save it...\n",
    "\n",
    "We can also print out the total number of `tokens` just to make sure what we are dealing with and also print out the number of `batches` in a single **epoch** of iterating over this dataset (how many `unique batches` do we output before we loop back around to the beginning of the dataset to start reading it again)...\n",
    "\n",
    "So, now our implementation looks like this:\n",
    "```python\n",
    "# Data-Loader\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, Batch, Time):\n",
    "        self.Batch = Batch\n",
    "        self.Time = Time\n",
    "        with open(\"Datasets/Harry_Potter_Books.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
    "            text = file.read()\n",
    "        encoder = tiktoken.get_encoding('gpt2')\n",
    "        encodedDataTokens = encoder.encode(text)\n",
    "        self.encodedDataTokens = torch.tensor(encodedDataTokens)\n",
    "        print(f\"Loaded {len(self.encodedDataTokens)} Tokens\")\n",
    "        print(f\"1 Epoch = {len(self.encodedDataTokens) // (Batch * Time)} Batches\")\n",
    "\n",
    "        # State\n",
    "        self.currentPosition = 0\n",
    "        \n",
    "    def nextBatch(self):\n",
    "        pass\n",
    "```\n",
    "Now let's implement the `nextBatch()` method...\n",
    "\n",
    "You will see that I have also used something called the `self.currentPosition = 0`. This is the state of the `DataLoaderLite` object at initialization, and it will be used in the `nextBatch()` method to take chunks of data and convert them into batches...\n",
    "\n",
    "Previously, we used this line of code:\n",
    "```python\n",
    "buffer = torch.tensor(encodedDataTokens[:Batch*Time + 1])\n",
    "```\n",
    "Which was used to take the first encoded tokens of `Batch*Time + 1`, but now, because we are using chunks, we will use the **slice of current position up till the current position succeeded by `Batch*Time + 1`**. And we can copy and paste our old `inputs` and `labels` code safely now...\n",
    "\n",
    "Which turns our code into:\n",
    "```python\n",
    "buffer = torch.tensor(encodedDataTokens[self.currentPosition : self.currentPosition + Batch*Time + 1])\n",
    "inputs = buffer[:-1].view(Batch, Time)\n",
    "labels = buffer[1:].view(Batch, Time)\n",
    "```\n",
    "\n",
    "We also need to advance our `currentPosition` by exactly `Batch*Time` to get the chunks...\n",
    "\n",
    "And remember that we are fetching `Batch*Time + 1` but we are chunking `Batch*Time`. This might create the out of bounds problem for our tensor and we need to handle that as well, and we will also run back our `currentPosition` to `0` if we are out of data in our dataset...\n",
    "\n",
    "So, our entire `DataLoaderLite` code now becomes:\n",
    "```python\n",
    "# Data-Loader\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, Batch, Time):\n",
    "        self.Batch = Batch\n",
    "        self.Time = Time\n",
    "        with open(\"Datasets/Harry_Potter_Books.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
    "            text = file.read()\n",
    "        encoder = tiktoken.get_encoding('gpt2')\n",
    "        encodedDataTokens = encoder.encode(text)\n",
    "        self.encodedDataTokens = torch.tensor(encodedDataTokens)\n",
    "        print(f\"Loaded {len(self.encodedDataTokens)} Tokens\")\n",
    "        print(f\"1 Epoch = {len(self.encodedDataTokens) // (Batch * Time)} Batches\")\n",
    "\n",
    "        # State\n",
    "        self.currentPosition = 0\n",
    "        \n",
    "    def nextBatch(self):\n",
    "        Batch, Time = self.Batch, self.Time\n",
    "        buffer = self.encodedDataTokens[self.currentPosition : self.currentPosition + Batch*Time + 1]\n",
    "        inputs = buffer[:-1].view(Batch, Time)\n",
    "        labels = buffer[1:].view(Batch, Time)\n",
    "        self.currentPosition += Batch * Time\n",
    "        if self.currentPosition + (Batch * Time + 1) > len(self.encodedDataTokens):\n",
    "            self.currentPosition = 0\n",
    "        return inputs, labels\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we also need to modify the optimization loop to get the correct batches...\n",
    "\n",
    "And we can initialize a `DataLoaderLite` object with a variable called `trainingLoader` and the same arguements that we used before and use this object to get the `inputs` and `labels` using `nextBatch()` method...\n",
    "\n",
    "We also need to be careful to not run into the same error that we encountered before because previously if you remember we used this line:\n",
    "```python\n",
    "buffer = buffer.to(device=device)\n",
    "```\n",
    "But now, because we are directly getting the separate tensors as `inputs` and `labels`, we need to guide them both to their specific device like this:\n",
    "```python\n",
    "inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
    "```\n",
    "\n",
    "So our entire initialization and optimization code looks like:\n",
    "```python\n",
    "# Data-Loader\n",
    "Batch, Time = 4, 32\n",
    "trainingLoader = DataLoaderLite(Batch=Batch, Time=Time)\n",
    "...\n",
    "# Optimization\n",
    "epochs = 50\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=3e-4)\n",
    "for epoch in range(epochs):\n",
    "    inputs, labels = trainingLoader.nextBatch()\n",
    "    inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(inputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step: {epoch}, Loss: {loss.item()}\")\n",
    "```\n",
    "So, let's now run the optimization and discuss what we expect to see in this optimization..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we expect our loss to come down pretty fast during the first few epochs, because in our vocabulary of `50257` tokens we don't actually use all of them and thus, there are pretty easier gains in learning of the network (basically deleting the usage of tokens that never occur). But we also don't expect the loss to go down by too much because we only have `50` epoch iterations at the time and it is not enough to perform a complete document run...\n",
    "\n",
    "Let's see what we get...\n",
    "\n",
    "We get an output like this:\n",
    "```bash\n",
    "Using Device: cpu\n",
    "Loaded 2100390 Tokens\n",
    "1 Epoch = 16409 Batches\n",
    "Step: 0, Loss: 10.99841022491455\n",
    "Step: 1, Loss: 9.390006065368652\n",
    "Step: 2, Loss: 9.049430847167969\n",
    "Step: 3, Loss: 8.327993392944336\n",
    "...\n",
    "Step: 48, Loss: 4.897827625274658\n",
    "Step: 49, Loss: 5.134008884429932\n",
    "```\n",
    "Which is what we are expecting...\n",
    "\n",
    "We also need to change the generation code, because now our code supports forwarding the model with or without the `labels`, so we need to handle the return of the `forward()` method correctly as well...\n",
    "\n",
    "So our old code:\n",
    "```python\n",
    "logits = model(inputs)\n",
    "```\n",
    "Becomes:\n",
    "```python\n",
    "logits, loss = model(inputs)\n",
    "```\n",
    "\n",
    "And also, because we have moved the initialization of the encoder within the `DataLoaderLite` class, we don't have an explicit encoder initialized, and we need to initialize it as well...\n",
    "\n",
    "```python\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "```\n",
    "\n",
    "\n",
    "And now you can modify the hyper-parameters and play around, and we can also safely say that we have successfully implemented `GPT-2` architecture supporting both the weight transfer and own weight training in less than `250` lines of code, whereas Hugging Face and OpenAI uses around `2000` lines of code to implement it...\n",
    "\n",
    "And now we can move on to the next section..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Weight Sharing (Fixing A Bug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we actually want to fix a bug that we have in our code... It's not a major bug, but it is indeed a bug with respect to how `GPT-2` training should happen.\n",
    "\n",
    "So, let's have a look at the bug...\n",
    "\n",
    "In our Hugging Face model, if we try to print these layer shapes:\n",
    "```python\n",
    "print(huggingfaceStateDictionary[\"lm_head.weight\"].shape)\n",
    "print(huggingfaceStateDictionary[\"transformer.wte.weight\"].shape)\n",
    "```\n",
    "They output:\n",
    "```python\n",
    "torch.Size([50257, 768])\n",
    "torch.Size([50257, 768])\n",
    "```\n",
    "We see that they are both `2D tensors` and are identical. And we can also understand that the `wte` is none other than the `word token embedding` at the bottom of the `transformer` and `lm_head` is none other than the `language modelling head` at the top of the `transformer`...\n",
    "\n",
    "Let's have a look at the transformer architecture image to have a clearer understanding:\\\n",
    "![Parameter_Weight_Sharing_Transformer_Model_Architecture](ExplanationMedia/Images/Parameter_Weight_Sharing_Transformer_Model_Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, now if we try to have an element wise equality like this:\n",
    "```python\n",
    "print((huggingfaceStateDictionary[\"lm_head.weight\"] == huggingfaceStateDictionary[\"transformer.wte.weight\"]).all())\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "tensor(True)\n",
    "```\n",
    "Which means every single element inside both the tensors are identical.\n",
    "\n",
    "And what's interesting is that, if we try to look at their **data pointer(s)** using `.data_ptr()` like this:\n",
    "```python\n",
    "print(huggingfaceStateDictionary[\"lm_head.weight\"].data_ptr())\n",
    "print(huggingfaceStateDictionary[\"transformer.wte.weight\"].data_ptr())\n",
    "```\n",
    "I get:\n",
    "```python\n",
    "3069647520000\n",
    "3069647520000\n",
    "```\n",
    "We see that the pointer points to the same location as well...\n",
    "\n",
    "So, not only do these tensors happen to have these same shapes and elements, they are actually pointing to the identical tensor...\n",
    "\n",
    "And what's happening here is a common weight-tying-scheme.That actually comes from the original <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> paper. And if we come down to the section **3.4 Embeddings and Softmax**, we see a text mentioning:\n",
    "```plaintext\n",
    "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
    "linear transformation, similar to [30].\n",
    "```\n",
    "Which is an awkward way to say that these matrices are shared and that they are tied and are the same matrix.\n",
    "\n",
    "And this `[30]` is just another paper <a href=\"https://arxiv.org/abs/1608.05859\">Using the Output Embedding to Improve Language Models</a>, and it argues for this weight-tying-scheme.\n",
    "\n",
    "But, the conclusion we arrive is, **we actually want these matrices to behave similarly** in the following sense: \n",
    "\n",
    "If two tokens are very similar *symantically* (maybe one token is lowercase and other token is uppercase or it's the same token in a different language etc.), presumably we would expect that the lie nearby in the `token embedding space`, but in the exact same way if two tokens are very similar *symantically*, we'd expect them to get the same probabilities at the output of the `transformer`...\n",
    "\n",
    "So, both positions (top and bottom) have this property that **similar tokens should have similar embeddings or similar weights**...\n",
    "\n",
    "And this scheme has already been implemented in the `Hugging Face GPT-2`'s <a href=\"https://github.com/openai/gpt-2/blob/master/src/model.py\">`model.py`</a>, and one way to implement this is simply point the weights to the same memory location explicitly after initialization like this:\n",
    "```python\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.configuration = configuration\n",
    "\n",
    "        self.transformer = torch.nn.ModuleDict(dict(\n",
    "            wordTokenEmbeddings = torch.nn.Embedding(configuration.vocabularySize, configuration.numberOfEmbeddingDimensions),\n",
    "            wordPositionalEmbeddings = torch.nn.Embedding(configuration.blockSize, configuration.numberOfEmbeddingDimensions),\n",
    "            hidden = torch.nn.ModuleList(Block(configuration) for _ in range(configuration.numberOfLayers)),\n",
    "            finalLayerNorm = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        ))\n",
    "\n",
    "        self.languageModelingHead = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, configuration.vocabularySize, bias=False)\n",
    "    \n",
    "        # Weight-Sharing-Scheme (Parameter Weight Sharing)\n",
    "        self.transformer.wordTokenEmbeddings.weight = self.languageModelingHead.weight\n",
    "    ...\n",
    "```\n",
    "\n",
    "Meaning, that the old value of `wordTokenEmbeddings` will get orphaned and Python will clean it up and we will then be left with a single tensor and it is going to be used twice in a forward pass..\n",
    "\n",
    "And another good reason to use it is because of memory efficiency too, because this single tensor is a ton of parameters (`768 * 50257 = 38,597,376 ≈ 40M`) and this is a `124M` parameter model which means around `30%` of the parameters, which we are being efficient with. And we also expect our model to work slightly better, because of this scheme..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Proper Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'd like to follow the way `GPT-2` initializes it's weights...\n",
    "\n",
    "Unfortunately, the `GPT-2` and `GPT-3` papers are not explicit about their initializations and we kind of have to read between the lines, and instead of going through the paper which is quite vague, there's a bit of information in the <a href=\"https://github.com/openai/gpt-2/blob/master/src/model.py\">`model.py` code</a> that OpenAI released...\n",
    "\n",
    "And once we check the code we see:\n",
    "```python\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n",
    "        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "...\n",
    "def model(hparams, X, past=None, scope='model', reuse=False):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results\n",
    "```\n",
    "And we see that they initialized their weights with a `random_normal_initializer()` from `TensorFlow` which is intuitively the **normal distribution** of specified **standard deviation(s)** for the weights(`0.01`&`0.02`), and for the bias they initialized it with all `0`'s using the `constant_initializer()`...\n",
    "\n",
    "So, let's follow how they initialized the weights here and implement it in our code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a **private function** called `_initializeParameters()` using the `pre-underscore syntax` in Python in our `GPTModel` class and implement the proper initialization of parameters...\n",
    "\n",
    "And since the standard deviation of `0.01` and `0.02` is about the same, we will stick with `0.02` to decrease the complexity of the code and initialize our model faster...\n",
    "\n",
    "And we can use the <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.apply_.html\">PyTorch's `apply()`</a> method to iterate over the sub-modules of a specified module, at the end of our initialization...\n",
    "\n",
    "And we come up with the following code:\n",
    "```python\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        ...\n",
    "        # Initialize Correct Parameters\n",
    "        self.apply(self._initializeParameters)\n",
    "\n",
    "    def forward(self, indeces, labels=None):\n",
    "        ...\n",
    "\n",
    "    def _initializeParameters(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, torch.nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And note that the PyTorch's **default bias initialization** is the **uniform distribution**, which here we are setting to `0` for all the items in bias...\n",
    "\n",
    "And the only other layer that requires initialization is the `LayerNorm`(s) inside the `Block` module. And PyTorch set's the scale of the initialization to be `1` and the off-set of the initialization to be `0`, which is what we want, and will leave the code as is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we also notice the `Linear`'s default initialization, we will see that it is $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$, where $k = \\frac{1}{\\text{fan-in}}$, which is fairly in the vicinity of `0.02`...\n",
    "\n",
    "And if we look at the sizes of $d_{model}$ we see:\n",
    "| $d_{model}$ | Initialization ($\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$) |\n",
    "|-------------|-----------------------------------------------------|\n",
    "| 768         | $\\sqrt{\\frac{1}{768}} \\approx 0.03$                 |\n",
    "| 1024        | $\\sqrt{\\frac{1}{1024}} \\approx 0.03$                |\n",
    "| 1280        | $\\sqrt{\\frac{1}{1280}} \\approx 0.02$                |\n",
    "| 1600        | $\\sqrt{\\frac{1}{1600}} \\approx 0.02$                |\n",
    "\n",
    "Hinting us that we are still in the vicinity of what we already implemented earlier...\n",
    "\n",
    "But we are still not done with the initialization because there is one more caveat here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we swing back to the <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">`GPT-2` paper</a>, and scroll down to the section `2.2. Input Representation`, we will find this line:\n",
    "> A modified initialization which accounts for the accumulation on the residual path with model depth\n",
    "is used. We scale the weights of residual layers at initialization by a factor of $1/\\sqrt{N}$ where $N$ is the number of residual layers.\n",
    "\n",
    "And we have not implemented that yet, and we can do so now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
