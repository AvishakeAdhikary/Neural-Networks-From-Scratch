{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to OpenAI GPT v2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is the continuation of <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch</a> and <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20Tokenizer.ipynb\">GPT Tokenizer</a> notebooks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to reproduce the <a href=\"https://github.com/openai/gpt-2\">OpenAI's GPT 2</a> model, the (`124M`) version of it.\n",
    "\n",
    "Now, when OpenAI released GPT 2, they released it with this <a href=\"https://openai.com/index/better-language-models/\">blog post</a> and this <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">paper</a>. And on top of that, they released this <a href=\"https://github.com/openai/gpt-2\">code</a> on GitHub..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, when reproducing GPT 2, we have to be careful, because we are going to be reproducing the `124M` parameter model. And the thing to be careful with it is there's always a sub-series of models of different sizes when these model releases are made and usually the biggest model is called the **\"GPT\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the charts that we have in the paper for a second: \\\n",
    "![OpenAIGPT2 Graphs](ExplanationMedia/Images/OpenAIGPT2Graphs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the reason we have multiple models is because, according to the above graphs we see that we consider the `Number of parameters in the Language Model` in the `x-axis` and the `y-axis` we put a lot of *downstream metrics* that we are interested in like (\"Translation\", \"Summerization\", \"Question Answering\") and so on and we can chart out the *downstream metrics* as the model size increases.\n",
    "\n",
    "And in the paper we see a table like this:\n",
    "\n",
    "| Parameters | Layers | $d_{model}$ |\n",
    "|------------|--------|-----------|\n",
    "| 117M       | 12     | 768       |\n",
    "| 345M       | 24     | 1024      |\n",
    "| 762M       | 36     | 1280      |\n",
    "| 1542M      | 48     | 1600      |\n",
    "\n",
    "And we see `4` models in the `GPT-2` sub series, starting at `124M` all the way up to `1558M`...\n",
    "\n",
    "But you might be thinking that I might have made a mistake because, in the table the numbers are different and the numbers I spoke of are different. And the reason my numbers disagree with this table is because this entire table is wrong and if we go to their <a href=\"https://github.com/openai/gpt-2\">GitHub repository</a> we see a note that says:\n",
    "> * *Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper). Thus you may have seen small referred to as 117M and medium referred to as 345M.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the `124M` parameter model, we see that they used `12 Layers` in the Transformer and `768 Channel Dimensions` in the Transformer.\n",
    "\n",
    "And by the end of this notebook we will try to beat the original `GPT-2 124M` model and will be looking at loss graphs to see our model perform better.\n",
    "\n",
    "The thing to note here is, this paper is more than 5 years old now and the computation was very low at the time, but today we can reproduce the same model's performance in roughly an hour or so and it will cost us around $10 (if we want to do this on a cloud compute, or in other words, a computer that we can all rent). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
