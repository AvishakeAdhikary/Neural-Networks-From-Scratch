{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to OpenAI GPT v2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is the continuation of <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch</a> and <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20Tokenizer.ipynb\">GPT Tokenizer</a> notebooks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to reproduce the <a href=\"https://github.com/openai/gpt-2\">OpenAI's GPT 2</a> model, the (`124M`) version of it.\n",
    "\n",
    "Now, when OpenAI released GPT 2, they released it with this <a href=\"https://openai.com/index/better-language-models/\">blog post</a> and this <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">paper</a>. And on top of that, they released this <a href=\"https://github.com/openai/gpt-2\">code</a> on GitHub..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, when reproducing GPT 2, we have to be careful, because we are going to be reproducing the `124M` parameter model. And the thing to be careful with it is there's always a sub-series of models of different sizes when these model releases are made and usually the biggest model is called the **\"GPT\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the charts that we have in the paper for a second: \\\n",
    "![OpenAIGPT2 Graphs](ExplanationMedia/Images/OpenAIGPT2Graphs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the reason we have multiple models is because, according to the above graphs we see that we consider the `Number of parameters in the Language Model` in the `x-axis` and the `y-axis` we put a lot of *downstream metrics* that we are interested in like (\"Translation\", \"Summerization\", \"Question Answering\") and so on and we can chart out the *downstream metrics* as the model size increases.\n",
    "\n",
    "And in the paper we see a table like this:\n",
    "\n",
    "| Parameters | Layers | $d_{model}$ |\n",
    "|------------|--------|-----------|\n",
    "| 117M       | 12     | 768       |\n",
    "| 345M       | 24     | 1024      |\n",
    "| 762M       | 36     | 1280      |\n",
    "| 1542M      | 48     | 1600      |\n",
    "\n",
    "And we see `4` models in the `GPT-2` sub series, starting at `124M` all the way up to `1558M`...\n",
    "\n",
    "But you might be thinking that I might have made a mistake because, in the table the numbers are different and the numbers I spoke of are different. And the reason my numbers disagree with this table is because this entire table is wrong and if we go to their <a href=\"https://github.com/openai/gpt-2\">GitHub repository</a> we see a note that says:\n",
    "> * *Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper). Thus you may have seen small referred to as 117M and medium referred to as 345M.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the `124M` parameter model, we see that they used `12 Layers` in the Transformer and `768 Channel Dimensions` in the Transformer.\n",
    "\n",
    "And by the end of this notebook we will try to beat the original `GPT-2 124M` model and will be looking at loss graphs to see our model perform better.\n",
    "\n",
    "The thing to note here is, this paper is more than 5 years old now and it was probably a very complicated optimization at the time and the computation was very low at the time, but today we can reproduce the same model's performance in roughly an hour or so and it will cost us around $10 (if we want to do this on a cloud compute, or in other words, a computer that we can all rent). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one more thing to mention is, OpenAI did release it's model's weights and it is available on it's GitHub repository, but it's paper is not good with all of it's details with the training.\n",
    "\n",
    "So, in addition to the GPT-2 paper, we will also be referring to the <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3 paper</a>, which is a lot more concrete and a lot of the hyper-parameters and optimization settings and so on, which is not a huge departure from the architecture of GPT-2 version of the model.\n",
    "\n",
    "\n",
    "So, let's do this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Hugging Face Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first thing we'd like to do is start at the very end. Or in other words, we'll load the `GPT-2 124M` model as it was released by OpenAI and take it for a spin and sample some `tokens` from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the issue is...\n",
    "\n",
    "When we look at the code base and look for the <a href=\"\">`model.py`</a> we see these imports:\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.training import HParams\n",
    "```\n",
    "\n",
    "And we realise that the code is written in <a href=\"https://www.tensorflow.org/\">TensorFlow</a> (another alternative for creating and training deep learning models offered by Google). Meaning that the original `GPT-2` code was written in TensorFlow and is not used anymore...\n",
    "\n",
    "And as per our previous notebooks, we'd like to use PyTorch. And it will be a lot easier if we'd be able to work with the old explanations.\n",
    "\n",
    "But the problem with that is that the initial code is in TensorFlow and we'd like to use PyTorch. So, in order to get the targets we'd like to use the <a href=\"https://huggingface.co/docs/transformers/en/index\">`Hugging Face Transformers Library`</a> released at PyPi. We can use this <a href=\"https://huggingface.co/docs/transformers/en/installation\">installation documentaiton</a> to walk through the steps to install the library in our system..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check out Hugging Face's implementation of that transformer in their <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\">`modeling_gpt2.py`</a>. Which did a lot of work to convert all those TensorFlow code to PyTorch such that it becomes easier to load and work with.\n",
    "\n",
    "So in particular we can look at the <a href=\"https://huggingface.co/openai-community/gpt2\">Hugging Face GPT-2</a> model and load it using the Hugging Face transformers...\n",
    "\n",
    "\n",
    "So this is what the code looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "huggingface_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "huggingfaceStateDictionary = huggingface_model.state_dict()\n",
    "\n",
    "for key, value in huggingfaceStateDictionary.items():\n",
    "    print(key, value.shape)\n",
    "```\n",
    "Which gives us the result:\n",
    "```python\n",
    "transformer.wte.weight torch.Size([50257, 768])\n",
    "transformer.wpe.weight torch.Size([1024, 768])\n",
    "transformer.h.0.ln_1.weight torch.Size([768])\n",
    "transformer.h.0.ln_1.bias torch.Size([768])\n",
    "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.0.ln_2.weight torch.Size([768])\n",
    "transformer.h.0.ln_2.bias torch.Size([768])\n",
    "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.1.ln_1.weight torch.Size([768])\n",
    "transformer.h.1.ln_1.bias torch.Size([768])\n",
    "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.1.ln_2.weight torch.Size([768])\n",
    "transformer.h.1.ln_2.bias torch.Size([768])\n",
    "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
    "...\n",
    "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.ln_f.weight torch.Size([768])\n",
    "transformer.ln_f.bias torch.Size([768])\n",
    "lm_head.weight torch.Size([50257, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One awkward thing about this is, when we say `gpt2` it actually loads the `124M` parameter model and if we want the actual `GPT-2` model we'd specify it as `gpt2-xl`...\n",
    "\n",
    "Now when we actually get this `GPT-2` initialized, we want to get the **state dictionary** which is the **raw tensors loaded with values** and we can get those using the `.state_dict()` method. and we can print the `key` (which are the tensors) and the `value` (which are the tensor values) and we can look at the shapes of the `value` tensors to get an idea of the shapes of the states in the model...\n",
    "\n",
    "So, we can now look at the different parameters inside the `GPT-2` model and their shapes...\n",
    "\n",
    "And we can see that there are a lot of short forms of the terms that we already know of, so let's recall that:\n",
    "1. **wte**: Word Token Embeddings\n",
    "2. **wpe**: Word Position Embeddings\n",
    "3. **ln**: Layer Normalization\n",
    "4. **attn**: Attention\n",
    "5. **c_attn**: Cross Attention (awkward because `GPT-2` is a decoder only architecture and should be named **self attention**)\n",
    "6. **c_proj**: Projection layer within attention or MLP\n",
    "7. **mlp**: Multi-Layer Perceptron\n",
    "8. **lm_head**: Language Model Head (output layer)\n",
    "9. **c_fc**: Current/Common Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially can recall the very first key-value pair `transformer.wte.weight torch.Size([50257, 768])` as the `Word Token Embeddings` having a shape of `[50257, 768]` and it comes from the `50257` vocabulary of tokens (which is exactly the number of tokens we spoke about in our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20Tokenizer.ipynb\">Tokenizer Notebook</a>) having `768` feature space (or embedding vector space, or `768 dimensional embedding`)...\n",
    "\n",
    "We can also look at the second key-value pair `transformer.wpe.weight torch.Size([1024, 768])`, we can recall them as `Word Positional Embeddings` having a shape of `[1024, 768]`. So, because `GPT-2` has a maximum sequence length of `1024` we have upto `1024` positions that each token can attend to in the past. And every one of those positions in `GPT-2` has a fixed vector of `768` that is learnt by optimization.\n",
    "\n",
    "And everything else is just the other weights and biases of this transformer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now for example, if we take just the `Positional Embeddings` and we flatten it out (we get a `[1, 768]` vector) and take just the first `20` elements of the `768` embeddings we can see that we get the proper weights as an output for this code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "huggingfaceStateDictionary['transformer.wpe.weight'].view(-1)[:20]\n",
    "```\n",
    "We get:\n",
    "```python\n",
    "tensor([-0.0188, -0.1974,  0.0040,  0.0113,  0.0638, -0.1050,  0.0369, -0.1680,\n",
    "        -0.0491, -0.0565, -0.0025,  0.0135, -0.0042,  0.0151,  0.0166, -0.1381,\n",
    "        -0.0063, -0.0461,  0.0267, -0.2042])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot these weights and try to see what they represent like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(huggingfaceStateDictionary['transformer.wpe.weight'], cmap='gray')\n",
    "```\n",
    "\n",
    "![GPT-2.transformer.wpe.weight](ExplanationMedia/Images/GPT-2.transformer.wpe.weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that this has structure, because these positional embeddings end up learning these **sinusoids** and **cosines** to represent each of these positions and each row here stands in for that position and is processed by the transformer to recover all the relative positions and realise which token is where and attend to them depending on their position not just their content...\n",
    "\n",
    "So now if we look at the individual columns of these we see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "plt.plot(huggingfaceStateDictionary['transformer.wpe.weight'][:, 150])\n",
    "plt.plot(huggingfaceStateDictionary['transformer.wpe.weight'][:, 200])\n",
    "plt.plot(huggingfaceStateDictionary['transformer.wpe.weight'][:, 250])\n",
    "```\n",
    "![GPT-2Graphs.transformer.wpe.weight](ExplanationMedia/Images/GPT-2Graphs.transformer.wpe.weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we still don't know what these embeddings are doing and why they are the way they are.\n",
    "\n",
    "But we can still see that the lines are a little noisy and jittery and that is because this model was not fully trained, and the more trained this model becomes the more we'd expect these graphs to smooth out, which also tells us that the original `GPT-2` is an **under-trained** model.\n",
    "\n",
    "If I remember correctly, in the original \"Attention-Is-All-You-Need\" paper, the `positional embeddings` are actually initialized and fixed to sinusoids and cosines of different frequencies, but in `GPT-2` these are trained from scratch and they seem to recover these features during the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the `Hugging Face Transformers` we can not just get all the raw weights but also get something called `pipeline` and sample from it...\n",
    "\n",
    "Here is the sample code snippet for `5` different generations of the same context window of tokens `\"Hello, I'm a language model,\"`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generator(\"Hello, I'm a language model,\", max_length=50, num_return_sequences=5)\n",
    "```\n",
    "For which we get:\n",
    "```python\n",
    "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are the ones I like the most. To do your research, please contact me, this isn't your\"},\n",
    " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\\nBut my job as a C programmer is to sort through every single line of the script so I\"},\n",
    " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend from college a bit earlier, and in the context of the current language model I think it's important\"},\n",
    " {'generated_text': 'Hello, I\\'m a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string-replace \"\\\\r\" ))) {\\n\\nconsole. log\\n\\n}\\n\\nthat\\'s'},\n",
    " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example. I'm making an API for a game where I want a character to play a little bit of a\"}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, even though we are setting a seed we get different generations from both the code and the official <a href=\"https://huggingface.co/openai-community/gpt2\">Hugging Face GPT-2 Hosted Inference API</a>.\n",
    "\n",
    "But at this stage what is important is, we are getting coherent text and we were successfully able to load the model and look at all of it's parameters and the keys tell us, where in the model these come from...\n",
    "\n",
    "But we want to actually write our own `GPT-2` class so that we have a full understanding of what's happening there and we also don't want to work with something like the <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\">`modeling_gpt2.py`</a> because it's too complicated and we want to write this from scratch ourselves.\n",
    "\n",
    "So we are going to be implementing our `GPT-2` model in `GPT_v2.5.py` script inside our `GPT Scripts` directory in parallel...\n",
    "\n",
    "But first let's load the `GPT-2 124M` into our `GPT_v2.5.py` for the class that we are going to develop from scratch, which is going to give us confidence that we can load the OpenAI model and there's a setting of weights that exactly is the `124M` model and we will try to surpass our own created `GPT` class...\n",
    "\n",
    "So, we're going to get different weights and everything is going to look different and hopefully even better and we will have the confidence that we are in the same model family and same model class and we just have to re-discover a good setting of the weights from scratch... \n",
    "\n",
    "So let's now write the `GPT-2` model and let's load the weights and make sure that we can also generate text that looks coherent..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Pre-Trained Weight Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer - Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now swing over to the <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\" paper</a> that started everything and look at the Transformer architecture:\n",
    "\n",
    "![Transformer_Model_Architecture](ExplanationMedia/Images/Transformer_Model_Architecture.png)\n",
    "\n",
    "Now, once again, like the last notebook, we mentioned that this architecture has changed over the years and `GPT-2` is slightly modified than the original `Transformer`... \n",
    "\n",
    "In particular, we do **NOT** have the **Encoder**, and `GPT-2` is a **Decoder** only `Transformer` as we call it. In addition to that the **Cross-Attention** that is used by that **Encoder** is also **missing**. Everything else stays almost the same, but there are some differences that we are going to see next..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are two main differences: \\\n",
    "When we go to the `GPT-2` <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">paper</a>, under section `2.3 Model` we see that there's a re-shuffling of the layer-normalizations (they change place) and an additional layer normalization was added after the final self-attention block..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters (`GPTConfiguration`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the skeleton of the `nn.Module`(s) in our GPT Script and in particular we want to match up the schema that we got from `Hugging Face GPT-2`...\n",
    "\n",
    "And we will use a decorator called `@dataclass` which provides a decorator and functions for automatically adding generated special methods such as `__init__()` and `__repr__()` to user-defined classes...\n",
    "\n",
    "And we will use it to define all the hyper-parameters as a Class called `GPTConfiguration`...\n",
    "\n",
    "Now because we are going to be implementing the `124M GPT-2 Model`, when we go to the paper we see these hyper-parameters:\n",
    "1. block-size (context window) → 1024\n",
    "2. vocabulary-size (token vocabulary) → 50257\n",
    "3. n-layer (number of layers) → 12\n",
    "4. n-head (number of self-attention heads) → 12\n",
    "5. embedding-dimensions ($d_{model}$) → 768\n",
    "\n",
    "So let's implement this now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now our code looks like this:\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# GPT configuration hyper-parameters\n",
    "@dataclass\n",
    "class GPTConfiguration:\n",
    "    blockSize: int = 1024\n",
    "    vocabularySize: int = 50257\n",
    "    numberOfLayers: int = 12\n",
    "    numberOfHeads: int = 12\n",
    "    numberOfEmbeddingDimensions: int = 768\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GPTModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be able to use this configuration under the `GPTModel` class that we are going to write...\n",
    "\n",
    "For now our empty `GPTModel` class looks like this:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.configuration = configuration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to **copy** the schema from the **Hugging Face `GPT-2` model** by utilizing the `huggingfaceStateDictionary`...\n",
    "\n",
    "And here's what I came up with...\n",
    "\n",
    "We see that the container in the schema is called `transformer` which contains all the modules and we can create something like that using `torch.nn.ModuleDict` which is just a dictionary of torch `Module`(s) which let's us index into `Module`(s) using **keys**, just like a normal python dictionary...\n",
    "\n",
    "Within that we can create something called `wordTokenEmbeddings` which corresponds with `wte` and create something called `wordPositionalEmbeddings` which corresponds with `wpe`, and we can match the shapes and create our initial layers...\n",
    "\n",
    "Then in the **Hugging Face `GPT-2` model** we see that we have a long list of **hidden** layers represented by a `.h` and followed by a range of number `.0` to `.11` hinting us about the number of layers as `12`, so we can now utilize our `numberOfLayers` hyper-parameter to construct these long list of layers. And instead of a `torch.nn.ModuleDict` we can use a `torch.nn.ModuleList` instead, which is just a list of `Module`(s).\n",
    "\n",
    "The important thing to note is, in those hidden layers we see different kinds of layer **weights** and **biases** of different **layers** all having their own shapes and sizes, but we do see a pattern that they repeat themselves in terms of **layer number**, so for now we can just consider these **layers** as `Block`(s) and iterate them through a list and return itself to the list. Keep in mind that the `Block`'s defination has not been defined yet, and we will define it later, but we want all the `Block`(s) to take in the same `configuration` object and construct the layer objects through it, because we already have all the hyper-parameters set inside it...\n",
    "\n",
    "Now that we have our long list of **hidden layers** it is time to construct the final **layer normalization** layer according to the `GPT-2` paper, so we can create something like `finalLayerNorm` and match the shapes which corresponds to the `ln_f`...\n",
    "\n",
    "And lastly we can construct our **final classifier** (or the **language model head**) which is just a **Linear Layer** that projects all the **embeddings** to their respective **tokens**, having **no bias**. So, we can easily construct this **languageModelingHead** which corresponds to the `lm_head` and finish with our skeleton of the `GPT-2` model...\n",
    "\n",
    "Now we end up with a code like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# GPT configuration hyper-parameters\n",
    "@dataclass\n",
    "class GPTConfiguration:\n",
    "    blockSize: int = 1024\n",
    "    vocabularySize: int = 50257\n",
    "    numberOfLayers: int = 12\n",
    "    numberOfHeads: int = 12\n",
    "    numberOfEmbeddingDimensions: int = 768\n",
    "\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.configuration = configuration\n",
    "\n",
    "        self.transformer = torch.nn.ModuleDict(dict(\n",
    "            wordTokenEmbeddings = torch.nn.Embedding(configuration.vocabularySize, configuration.numberOfEmbeddingDimensions),\n",
    "            wordPositionalEmbeddings = torch.nn.Embedding(configuration.blockSize, configuration.numberOfEmbeddingDimensions),\n",
    "            hidden = torch.nn.ModuleList(Block(configuration) for _ in range(configuration.numberOfLayers)),\n",
    "            finalLayerNorm = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        ))\n",
    "\n",
    "        self.languageModelingHead = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, configuration.vocabularySize, bias=False)\n",
    "\n",
    "model = GPTModel(GPTConfiguration())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Block`(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the `Block` class which is currently undefined...\n",
    "\n",
    "Now, here the `Block` refers to the `Transformer Block` that gets repeated again and again as hidden layers...\n",
    "\n",
    "Now, according to our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch</a> notebook, we already defined this `Block`, and as we mentioned `GPT-2` also has a slightly modified `Transformer Block`...\n",
    "\n",
    "For now we are going to use this template:\n",
    "```python\n",
    "# Transformer Block\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For, now we understand that we are left with the following modules & properties:\n",
    "1. Add & Norm\n",
    "2. Attention\n",
    "3. Feed Forward Network\n",
    "4. Residual Pathways\n",
    "\n",
    "Let's start with **Addition and Normalization (Add & Norm)**.\n",
    "\n",
    "According to the diagram, the **Add & Norm** is there **AFTER** the **Attention & Feed Forward Network**, but in our case we will use them **BEFORE** the **Attention & Feed Forward Network**, making it a **Pre-Normalization (Pre-Norm)**...\n",
    "\n",
    "Then we have our **Attention** module, and for now we can relate it to the attention we built in our last <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch</a> notebook. Specifically, we built two modules (`MultiHeadAttention` and `Head`), but here we will implement both modules in a combined and mathematically optimized class called `CausalSelfAttention`...\n",
    "\n",
    "Then we have our **Feed Forward Network**, and we will call it our **MultiLayerPerceptron**.\n",
    "\n",
    "The thing to note is, once again two of the modules (`CausalSelfAttention` and `MultiLayerPerceptron`) remain undefined, and we are going to define them later...\n",
    "\n",
    "For now, we end up with a code like this:\n",
    "```python\n",
    "# Transformer Block\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.layerNormalization1 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.attention = CausalSelfAttention(configuration)\n",
    "        self.layerNormalization2 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.multiLayerPerceptron = MultiLayerPerceptron(configuration)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "```\n",
    "\n",
    "And lastly, we arrive at the **Residual Pathways**, and we see that the normalizations are **inside** the residual stream, or in other words, the residual pathway has normalizations **inside** them (which is not very good or desirable from an optimization perspective) and we actually prefer to have a single and clean residual stream all the way from **supervision** to all the way to the **inputs (or `tokens`)**, which is desirable because the gradients that flow from the top distributes the gradients equally because of additions, indicating that the gradients from the top flow straight to the inputs through the residual pathway (unchanged) but then addition to that, the gradient also flows through the blocks and the blocks contribute their own contribution over time when the optimization kicks in.\n",
    "\n",
    "Which means that we want to apply a **clean residual pathway**. And to do that we need the normalization to be applied to the residual pathway (result of layer normalization) **before** adding it back to the original **inputs**. This ensures that the residual connections are additive and do not interfere with the normalization process, facilitating better gradient flow and optimization stability.\n",
    "\n",
    "Therefore, we end up with a code like this:\n",
    "```python\n",
    "# Transformer Block\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.layerNormalization1 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.attention = CausalSelfAttention(configuration)\n",
    "        self.layerNormalization2 = torch.nn.LayerNorm(configuration.numberOfEmbeddingDimensions)\n",
    "        self.multiLayerPerceptron = MultiLayerPerceptron(configuration)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs + self.attention(self.layerNormalization1(inputs))\n",
    "        inputs = inputs + self.multiLayerPerceptron(self.layerNormalization1(inputs))\n",
    "```\n",
    "\n",
    "Here's how the residual pathways are structured:\n",
    "- **Attention Layer**: After applying `self.layerNormalization1`, the residual (inputs) are added to `attention_output`. This adheres to the clean residual pathway because the normalization (`layerNormalization1`) is applied **before** adding to inputs.\n",
    "- **MLP Layer**: Similarly, after applying `self.layerNormalization2`, the residual (inputs) is added to `mlp_output`. This also adheres to the clean residual pathway for the same reason as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one more thing that is interesting to note is that, **Attention** is a **communication operation**, and it is where all the tokens line-up in a sequence and this is where the tokens communicate and exchange information. And **Attention** is an **aggregation function**, it's a **pooling function**, it's a **weighted sum function**, it's a **reduce operation**.\n",
    "\n",
    "Whereas, **Multi Layer Perceptron** happens at every single token individually (**mapped**), and there is no information being exchanged or collected between the tokens.\n",
    "\n",
    "So, the **Attention** is the **reduce** and **Multi Layer Perceptron** is the **map**. And what we end up with is a repeated application of **Map-Reduce**. And this is where the `tokens` communicate and this is where they *think* individually about the information that they gathered. And every one of these blocks, iteratively refines the representation inside the residual stream...\n",
    "\n",
    "And now we can move on the to implementation of `CausalSelfAttention` and `MultiLayerPerceptron`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MultiLayerPerceptron`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move on to the `MultiLayerPerceptron (MLP)`, and I implemented the class as follows...\n",
    "\n",
    "```python\n",
    "# Multi Layer Perceptron (MLP)\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "```\n",
    "\n",
    "It is relatively straight forward. For now, we just have two `Linear` layers which wrap around a `GELU` non-linearity layer. So our block now becomes something like this:\n",
    "```python\n",
    "# Multi Layer Perceptron (MLP)\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        self.currentFullyConnected = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, 4 * configuration.numberOfEmbeddingDimensions)\n",
    "        self.gelu = torch.nn.GELU(approximate=\"tanh\")\n",
    "        self.currentProjection = torch.nn.Linear(4 * configuration.numberOfEmbeddingDimensions, configuration.numberOfEmbeddingDimensions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.currentFullyConnected(inputs)\n",
    "        inputs = self.gelu(inputs)\n",
    "        inputs = self.currentProjection(inputs)\n",
    "        return inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we swing over to the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\">GELU PyTorch Documentation</a>, we see **two** different `GELU`(s) being hinted there:\n",
    "1. Original GELU formulation (We will discuss this in a bit):\n",
    "   $$ \\text{GELU}(x) = x \\cdot \\Phi(x) $$\n",
    "2. Approximate GELU formulation:\n",
    "   $$ \\text{GELU}(x) = 0.5 \\cdot x \\cdot \\left(1 + \\tanh \\left( \\frac{2}{\\pi} \\cdot \\left(x + 0.044715 \\cdot x^3 \\right) \\right) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GELU](ExplanationMedia/Images/GELU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a preview, we can see that `GELU` is basically like a `ReLU`, except there's **no exactly flat tail at exactly `0`**. Otherwise, it just looks more like a slightly *smoother* `ReLU`. And it comes from this paper <a href=\"https://arxiv.org/abs/1606.08415\">\"Gaussian Error Linear Units (GELUs)\"</a> and there's a little bit of history here and I also invite you to step through the paper if you'd like. But for now, we will use the **approximate** version of the `GELU`, because that's what `GPT-2` in their model used..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one other reason of why we prefer to use `GELU` is that, in previous notebooks we have spoken about the **Dead-ReLU-Neuron-Problem** where, in the tail of a `ReLU`, where it's exactly flat at `0`, any activations that fall there will get exactly `0` gradient (meaning that there's no change, there's no adaptation, there's no development of the network), but `GELU` always contributes to a **local-gradient** and so there's always going to be a change and there's always going to be an adaptation in a *smoothed-out* way which empirically working better, as demonstrated in the paper.\n",
    "\n",
    "And we also followed the rule of *\"Position-wise Feed-Forward Networks\"* section of the original \"Attention-is-all-you-need\" paper, which is why we have the `4 * numberOfEmbeddingDimensions` in the shapes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can now move on to implement the `CausalSelfAttention` part of the code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CausalSelfAttention`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start implementing our `CausalSelfAttention` block which is none other than the combination of **Scaled Dot-Product Attention** and **Multi-Head Attention**...\n",
    "\n",
    "For now we have a skeleton like this:\n",
    "```python\n",
    "# Causal Self Attention (Scaled Dot-Product Attention + Multi-Head Attention)\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from our <a href=\"https://github.com/AvishakeAdhikary/Neural-Networks-From-Scratch/blob/main/GPT%20from%20Scratch.ipynb\">GPT from Scratch notebook</a>, that **Multi-Head Attention** is just multiple **Scaled Dot-Product Attention**(s) running in parellel and their outputs are just being concatenated and that becomes the output.\n",
    "\n",
    "Instead, we do a bunch of tensor *gymnastics* of mathematical operations of the same logic used behind both these **Multi-Head Attention** & **Scaled Dot-Product Attention** modules in a single block. But fundamentally, and algorithmically, nothing is different from what we implemented previously...\n",
    "\n",
    "And this is what we end up with:\n",
    "```python\n",
    "# Causal Self Attention (Scaled Dot-Product Attention + Multi-Head Attention)\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        assert configuration.numberOfEmbeddingDimensions % configuration.numberOfHeads == 0\n",
    "        self.causalAttention = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, 3 * configuration.numberOfEmbeddingDimensions)\n",
    "        self.causalProjection = torch.nn.Linear(configuration.numberOfEmbeddingDimensions, configuration.numberOfEmbeddingDimensions)\n",
    "        self.numberOfHeads = configuration.numberOfHeads\n",
    "        self.numberOfEmbeddingDimensions = configuration.numberOfEmbeddingDimensions\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(configuration.blockSize, configuration.blockSize)).view(1, 1, configuration.blockSize, configuration.blockSize))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        B, T, C = inputs.size()\n",
    "        query_key_value = self.causalAttention(inputs)\n",
    "        query, key, value = query_key_value.split(self.numberOfEmbeddingDimensions, dim=2)\n",
    "        query = query.view(B, T, self.numberOfHeads, C // self.numberOfHeads).transpose(1, 2)\n",
    "        key = key.view(B, T, self.numberOfHeads, C // self.numberOfHeads).transpose(1, 2)\n",
    "        value = value.view(B, T, self.numberOfHeads, C // self.numberOfHeads).transpose(1, 2)\n",
    "        attention = (query @ key.transpose(-2, -1)) * (1.0 / math.sqrt(key.size(-1)))\n",
    "        attention = attention.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        outputs = attention @ value\n",
    "        outputs = outputs.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        outputs = self.causalProjection(outputs)\n",
    "        return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Weights Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the skeleton ready, we can now move on to transfer the weights of the `Hugging Face GPT-2` to our `Custom GPT-2`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `from_pretrained()` - Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by understanding what we want to do first...\n",
    "\n",
    "We want to have a `from_pretrained()` method in our `GPTModel` class, that will transfer the weights for any kind of model we pass it (among the `4` models that are there in `GPT-2`), and copy the weights of each of those parameters and ensure their sizes and shapes match perfectly...\n",
    "\n",
    "And we also want our `from_pretrained()` method to be decorated by a `@classmethod` such that it could be accessed directly using the class reference and it is also able to modify the state of the class and return the appropriate model along with their appropriate parameters...\n",
    "\n",
    "So for now we can have a skeleton like this:\n",
    "```python\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        ... # Rest of the code\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained gpt: %s\" % modelType)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `from_pretrained()` - Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the separate configuartion arguements for all `4` `GPT-2` configurations...\n",
    "\n",
    "And our code will look like this:\n",
    "```python\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        ... # Rest of the code\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained gpt: %s\" % modelType)\n",
    "\n",
    "        # Creating separate configurations for separate GPT-2 models\n",
    "        configurationArguements = {\n",
    "            'gpt2':         dict(numberOfLayers=12, numberOfHeads=12, numberOfEmbeddingDimensions=768),  # 124M parameters\n",
    "            'gpt2-medium':  dict(numberOfLayers=24, numberOfHeads=16, numberOfEmbeddingDimensions=1024), # 350M parameters\n",
    "            'gpt2-large':   dict(numberOfLayers=36, numberOfHeads=20, numberOfEmbeddingDimensions=1280), # 774M parameters\n",
    "            'gpt2-xl':      dict(numberOfLayers=48, numberOfHeads=25, numberOfEmbeddingDimensions=1600), # 1558M parameters\n",
    "        }[modelType]\n",
    "        configurationArguements['vocabularySize'] = 50257\n",
    "        configurationArguements['blockSize'] = 1024\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can unpack our configurations into a variable called `configuration` based on the `modelType` arguement we pass as a parameter. And then we can initialize our `GPTModel` based on the `configuration` that we initialize. And then we can also copy the state-dictionary containing all the layers in our model in a variable called `stateDictionary` with the method `state_dict()` and unpack it's keys using the `keys()` method into a variable called `stateDictionaryKeys` (We have to keep in mind that we discard all the buffers that are not a part of the parameters like **Attention Mask** and **Attention Bias**)...\n",
    "\n",
    "So, now we have a code like this:\n",
    "```python\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        ... # Rest of the code\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # Creating separate configurations for separate GPT-2 models\n",
    "        configurationArguements = {\n",
    "            'gpt2':         dict(numberOfLayers=12, numberOfHeads=12, numberOfEmbeddingDimensions=768),  # 124M parameters\n",
    "            'gpt2-medium':  dict(numberOfLayers=24, numberOfHeads=16, numberOfEmbeddingDimensions=1024), # 350M parameters\n",
    "            'gpt2-large':   dict(numberOfLayers=36, numberOfHeads=20, numberOfEmbeddingDimensions=1280), # 774M parameters\n",
    "            'gpt2-xl':      dict(numberOfLayers=48, numberOfHeads=25, numberOfEmbeddingDimensions=1600), # 1558M parameters\n",
    "        }[modelType]\n",
    "        configurationArguements['vocabularySize'] = 50257\n",
    "        configurationArguements['blockSize'] = 1024\n",
    "\n",
    "        configuration = GPTConfiguration(**configurationArguements)\n",
    "        model = GPTModel(configuration)\n",
    "\n",
    "        stateDictionary = model.state_dict()\n",
    "        stateDictionaryKeys = stateDictionary.keys()\n",
    "        stateDictionaryKeys = [key for key in stateDictionaryKeys if not key.endswith('.attention.bias')]\n",
    "\n",
    "        return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `from_pretrained()` - Hugging Face Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our own custom model completely initialized, we can go ahead and initialize the `Hugging Face GPT-2 Model` into a single variable and it's state dictionary and keys into other variables called `huggingfaceStateDictionary` and `huggingfaceStateDictionaryKeys`...\n",
    "\n",
    "And then we can start copying the weights after **ignoring the buffers**. But now, before copying we have to keep in mind that the original code for the `GPT-2` model was trained using the `TensorFlow` library and some of the weights are **transposed** in that architecture, so we will manually hard-code those weights and **copy them after transposing them to their original PyTorch form**...\n",
    "\n",
    "One last thing to be careful about is, in our model we are using **custom names for our variables** but the `Hugging Face GPT-2 Model` has an architecture of **short forms**, so it is better to have a `parameterKeyMapping` that **maps our custom keys with the Hugging Face GPT-2 keys of the state-dictionary** such that it becomes much easier to iterate through...\n",
    "\n",
    "So, now we have a final code like this:\n",
    "```python\n",
    "# GPT model architecture\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, configuration):\n",
    "        super().__init__()\n",
    "        ... # Rest of the code\n",
    "\n",
    "    # Method to transfer weights from Hugging Face GPT-2\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, modelType):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"Loading weights from pretrained gpt: %s\" % modelType)\n",
    "        \n",
    "        # Creating separate configurations for separate GPT-2 models\n",
    "        blockSize = 1024\n",
    "        vocabularySize = 50257\n",
    "        configurationArguements = {\n",
    "            'gpt2':         dict(numberOfLayers=12, numberOfHeads=12, numberOfEmbeddingDimensions=768),  # 124M parameters\n",
    "            'gpt2-medium':  dict(numberOfLayers=24, numberOfHeads=16, numberOfEmbeddingDimensions=1024), # 350M parameters\n",
    "            'gpt2-large':   dict(numberOfLayers=36, numberOfHeads=20, numberOfEmbeddingDimensions=1280), # 774M parameters\n",
    "            'gpt2-xl':      dict(numberOfLayers=48, numberOfHeads=25, numberOfEmbeddingDimensions=1600), # 1558M parameters\n",
    "        }[modelType]\n",
    "        configurationArguements['vocabularySize'] = 50257\n",
    "        configurationArguements['blockSize'] = 1024\n",
    "\n",
    "        configuration = GPTConfiguration(**configurationArguements)\n",
    "        model = GPTModel(configuration)\n",
    "        stateDictionary = model.state_dict()\n",
    "        stateDictionaryKeys = stateDictionary.keys()\n",
    "        stateDictionaryKeys = [key for key in stateDictionaryKeys if not key.endswith('.attention.bias')]\n",
    "\n",
    "        huggingfaceModel = GPT2LMHeadModel.from_pretrained(modelType)\n",
    "        huggingfaceStateDictionary = huggingfaceModel.state_dict()\n",
    "        huggingfaceStateDictionaryKeys = huggingfaceStateDictionary.keys()\n",
    "        huggingfaceStateDictionaryKeys = [key for key in huggingfaceStateDictionaryKeys if not key.endswith('.attn.masked_bias')]\n",
    "        huggingfaceStateDictionaryKeys = [key for key in huggingfaceStateDictionaryKeys if not key.endswith('.attn.bias')]\n",
    "        transposedParameters = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        assert len(huggingfaceStateDictionaryKeys) == len(stateDictionaryKeys), f\"Mismatched Keys: {len(huggingfaceStateDictionaryKeys)} != {len(stateDictionaryKeys)}\"\n",
    "\n",
    "        parameterKeyMapping = {\n",
    "            customKey: huggingfaceKey\n",
    "            for customKey, huggingfaceKey in zip(stateDictionaryKeys, huggingfaceStateDictionaryKeys)\n",
    "            }\n",
    "\n",
    "        for customKey, huggingfaceKey in parameterKeyMapping.items():\n",
    "            if (huggingfaceStateDictionary[huggingfaceKey].shape != stateDictionary[customKey].shape):\n",
    "                # Special treatment for the Conv1D weights (Transposed Weights)\n",
    "                if (huggingfaceKey.endswith(word) for word in transposedParameters):\n",
    "                    assert huggingfaceStateDictionary[huggingfaceKey].shape[::-1] == stateDictionary[customKey].shape\n",
    "                    with torch.no_grad():\n",
    "                        stateDictionary[customKey].copy_(huggingfaceStateDictionary[huggingfaceKey].t())\n",
    "                # Vanilla copy for other parameters\n",
    "                else:\n",
    "                    assert huggingfaceStateDictionary[huggingfaceKey].shape == stateDictionary[customKey].shape\n",
    "                    with torch.no_grad():\n",
    "                        stateDictionary[customKey].copy_(huggingfaceStateDictionary[huggingfaceKey])\n",
    "        \n",
    "        return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as we do this, you will see that it will start downloading the model from Hugging Face like this:\n",
    "```bash\n",
    "Loading weights from pretrained gpt: gpt2\n",
    "config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 665/665 [00:00<?, ?B/s]\n",
    "model.safetensors:  34%|████████████████████████████████████████████████████▋                                                                                                    | 189M/548M [00:18<00:35, 10.1MB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done we can now move on to the generation phase of the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
